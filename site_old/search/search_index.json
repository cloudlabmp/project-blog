{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"about/","title":"Profile","text":""},{"location":"about/#hi-im-matthew-pollock","title":"Hi, I'm Matthew Pollock. \ud83d\udc4b","text":"<p>Lead Infrastructure Engineer | Cloud Engineer | IT Infrastructure Manager </p>"},{"location":"about/#about-me","title":"About Me","text":"<ul> <li>\u26a1 I like technology that actually works. Not hype, not complexity for the sake of it \u2013 just smart, practical solutions that solve real problems.</li> <li>\ud83c\udfaf My approach to IT is straightforward: learn what\u2019s useful, implement it properly, and make things better.</li> <li>\ud83d\ude80 I don\u2019t follow trends just because they\u2019re trends. If a new technology makes sense, I\u2019ll dig into it, test it, and figure out how to use it in a way that improves reliability, efficiency, and security. If it doesn\u2019t? I move on.</li> </ul>"},{"location":"about/#what-i-focus-on","title":"What I Focus On: \ud83d\udd0d","text":"<ul> <li>\u2705 Learning with purpose \u2013 staying ahead, but only where it counts</li> <li>\u2705 Making things work \u2013 cloud, automation, security, infrastructure</li> <li>\u2705 Cutting through the noise \u2013 useful tech over pointless buzzwords</li> <li>\u2705 Helping others \u2013 sharing knowledge when it actually benefits someone</li> </ul> <p>\ud83d\udca1 IT should be effective, not overcomplicated. If something needs fixing, I\u2019ll fix it. If it can be improved, I\u2019ll improve it. And if it\u2019s just a waste of time? I\u2019ll call it what it is.</p>"},{"location":"about/#contact-me","title":"Contact Me \ud83d\udcec","text":"<ul> <li>\ud83d\udcde Mobile: +44 7751720891</li> <li>\ud83d\udce7 Email: matt.pollock@outlook.com</li> </ul> <p>\ud83d\udce5 Download CV</p>"},{"location":"about/#resume","title":"Resume \ud83c\udfc6","text":""},{"location":"about/#career","title":"Career \ud83d\udcbc","text":""},{"location":"about/#ax-ltd-cloud-it-infrastructure-lead","title":"AX Ltd - Cloud &amp; IT Infrastructure Lead","text":"<p>\ud83d\udcc5 December 2024 - Present</p> <ul> <li>\u2601\ufe0f Cloud Operations: Manage and optimize Azure and AWS environments, ensuring secure, scalable, and cost-efficient infrastructure to support business operations.</li> <li>\ud83d\udd10 Identity &amp; Security: Maintain secure identity systems with Azure AD, including MFA, Conditional Access Policies, and Privileged Identity Management.</li> <li>\ud83d\udee1\ufe0f Governance &amp; Compliance: Establish and enforce governance policies with Azure Policy, Group Policy, and Defender for Cloud.</li> <li>\ud83d\udd0d Monitoring &amp; Performance: Use tools such as Azure Monitor, AWS CloudWatch, and PRTG Monitoring.</li> <li>\ud83c\udfaf Strategic Planning: Drive innovation and cloud optimization.</li> <li>\ud83e\udd1d Collaboration &amp; Mentorship: Provide mentorship to enhance team expertise and innovation.</li> </ul>"},{"location":"about/#ax-ltd-it-infrastructure-manager","title":"AX Ltd - IT Infrastructure Manager","text":"<p>\ud83d\udcc5 January 2017 - December 2024</p> <ul> <li>\ud83d\ude80 Led full-scale cloud migration to Microsoft Azure.</li> <li>\ud83d\udc65 Managed and mentored a team of five infrastructure engineers.</li> <li>\ud83c\udfd7\ufe0f Designed and implemented infrastructure for remote depot sites.</li> <li>\ud83d\udcb0 Optimized vendor relationships and licensing agreements.</li> </ul>"},{"location":"about/#ax-ltd-senior-it-infrastructure-engineer","title":"AX Ltd - Senior IT Infrastructure Engineer","text":"<p>\ud83d\udcc5 November 2011 - December 2016</p> <ul> <li>\ud83d\udca1 Designed and executed multiple Microsoft Exchange migrations and domain controller refreshes.</li> <li>\ud83d\udd10 Implemented and optimized Azure AD services.</li> <li>\ud83d\udee0\ufe0f Deployed Microsoft Intune and the Defender suite.</li> <li>\ud83d\ude80 Led security enhancements, including email hygiene solutions and Active Directory Group Policies.</li> </ul>"},{"location":"about/#certifications","title":"Certifications \ud83c\udf93","text":""},{"location":"about/#cloud-technologies-2021-present","title":"Cloud Technologies (2021 - Present) \u2601\ufe0f","text":"<ul> <li>\ud83c\udfc6 Microsoft Certified: Azure DevOps Engineer Expert (AZ-400)</li> <li>\ud83c\udfc6 Microsoft Certified: Azure Security Engineer Associate (AZ-500)</li> <li>\ud83c\udfc6 Microsoft Certified: Azure Solutions Architect Expert (AZ-305)</li> <li>\ud83c\udfc6 Microsoft Certified: Azure Administrator Associate (AZ-104)</li> <li>\ud83c\udfc6 AWS Certified: Solutions Architect Associate (SAA-C02)</li> <li>\ud83c\udfc6 AWS Certified: Cloud Practitioner (CLF-C01)</li> <li>\ud83c\udfc6 Microsoft Certified: Azure Fundamentals (AZ-900)</li> </ul>"},{"location":"about/#microsoft-server-technologies-2001-2019","title":"Microsoft Server Technologies (2001 - 2019) \ud83d\udda5\ufe0f","text":"<ul> <li>\ud83c\udfc5 MCP, MCSA, MCSE, MCTS, MCITP</li> <li>\ud83d\udda5\ufe0f Microsoft Windows Server 2003/2008/2012</li> <li>\ud83d\udce7 Microsoft Exchange 2010/2013</li> </ul>"},{"location":"about/#skills","title":"Skills \ud83d\udee0\ufe0f","text":"<ul> <li>\u2601\ufe0f Cloud Technologies: Azure, AWS, hybrid cloud solutions</li> <li>\ud83d\udd27 Systems Administration: Windows Server, Active Directory, Exchange, M365</li> <li>\u2699\ufe0f Scripting &amp; Automation: PowerShell, Azure CLI, Python</li> <li>\ud83d\udee1\ufe0f Security &amp; Compliance: Defender for Endpoint, Intune, Zero Trust security models</li> <li>\ud83c\udf0d Infrastructure Optimization: Networking, DNS, firewalls, SD-WAN, VPNs</li> <li>\ud83d\udd04 DevOps &amp; IaC: Terraform, Bicep, GitHub Actions, Azure DevOps</li> </ul>"},{"location":"learn/","title":"Learn \ud83d\udcd6","text":"<p>Welcome to my learning page! This is where I document my hands-on training, technical experiments, and personal projects\u2014both work-related and those I explore in my own time. I\u2019m a firm believer in lifelong learning, constantly refining my skills and tackling new challenges across Azure, AWS, DevOps, and beyond. This space is a reflection of that journey\u2014practical, focused, and always evolving. Take a look around!</p>"},{"location":"learn/#recent-updates","title":"\ud83c\udf89 Recent Updates \ud83c\udf89","text":"<ul> <li>\ud83d\udd25\u2705 Feb 25 - PASSED - Microsoft DevOps Engineer Expert (AZ-400) - MS EXAM</li> <li>\ud83d\udd25\u2705 Jan 25 - PASSED - GenAI Essentials - ExamPro EXAM</li> <li>\ud83d\udd25\u2705 Dec 24 - PASSED - Microsoft Security Engineer Associate (AZ-500) - MS EXAM</li> <li>\ud83d\udd25\u2705 Oct 24 - PASSED - Microsoft Azure Solutions Architect (AZ-305) - MS EXAM</li> </ul>"},{"location":"learn/#finops-courses","title":"\ud83d\udcb0 FinOps Courses","text":"<ul> <li>Introduction to FinOps</li> </ul>"},{"location":"learn/#github-courses","title":"\ud83d\udc19 GitHub Courses","text":"<ul> <li>Exam Pro - GitHub Actions</li> </ul>"},{"location":"learn/#ai-courses","title":"\ud83e\udd16 AI Courses","text":"<ul> <li>GenAI Essentials - ExamPro</li> </ul>"},{"location":"learn/#devops-courses","title":"\ud83d\udcda DevOps Courses","text":"<ul> <li>Microsoft DevOps Engineer (AZ-400) - James Lee</li> <li>Learn DevOps: Docker, Kubernetes, Terraform, and Azure DevOps</li> <li>AWS Certified DevOps Engineer - Professional - Adrian Cantrill</li> <li>AWS Certified DevOps Engineer - Professional 2023 - DOP-C02 - St\u00e9phane Maarek</li> <li>AWS Certified Developer - Associate - Adrian Cantrill</li> </ul>"},{"location":"learn/#docker-courses","title":"\ud83d\udc33 Docker Courses","text":"<ul> <li>Derek Morgan - More Than Certified In Docker</li> <li>Docker Fundamentals - Adrian Cantrill</li> </ul>"},{"location":"learn/#python-courses","title":"\ud83d\udc0d Python Courses","text":"<ul> <li>100 Days of Code: The Complete Python Pro Bootcamp</li> </ul>"},{"location":"learn/#iot-courses","title":"\ud83d\udce1 IoT Courses","text":"<ul> <li>AWS IoT Immersion Day Workshop (25 Hands-on Labs)</li> <li>IoT Microcontrollers: Onboarding a Raspberry Pi using Raspbian</li> <li>IoT Edge Computing: Introduction to Amazon FreeRTOS</li> <li>Introduction to AWS IoT Device Management</li> <li>Getting Started with AWS IoT SiteWise</li> <li>Amazon Kinesis Video Streams - Getting Started</li> <li>Introduction to IoT Device Defender</li> <li>AWS IoT: Visual Walkthrough</li> <li>Introduction to AWS IoT Analytics</li> <li>Getting Started with AWS IoT Greengrass</li> <li>Introduction to AWS IoT</li> <li>Handling AWS IoT Device Data and States</li> <li>Securely Connecting AWS IoT Devices to the Cloud</li> <li>Managing AWS IoT Devices at Scale</li> <li>Analyzing, Visualizing, and Gaining Insights from IoT Devices</li> <li>AWS IoT Learning Plan: Foundations - 5 courses</li> <li>Easy Raspberry Pi IoT Server</li> <li>Beginners Masterclass into Internet of Things - Amit Rana</li> <li>IoT Foundations - AWS &amp; NodeRed - Derek Morgan</li> </ul>"},{"location":"learn/#personal-projects","title":"\ud83d\ude80 Personal Projects","text":""},{"location":"learn/#build-a-homelab-server-on-raspberry-pi5-with-openmediavault-docker","title":"\ud83c\udfe0 Build a HomeLab Server on Raspberry Pi5 with OpenMediaVault &amp; Docker","text":"<ul> <li>OpenMediaVault7</li> <li>Portainer</li> <li>Docker Compose</li> <li>Synology NAS</li> <li>Cloudflare Tunnels</li> </ul>"},{"location":"learn/#cloud-resume-challenge-aws-azure","title":"\ud83c\udf10 Cloud Resume Challenge (AWS &amp; Azure)","text":"<ul> <li>Terraform IaC (AWS &amp; Azure)</li> <li>GitHub Actions (AWS &amp; Azure)</li> <li>Route53 (AWS &amp; Azure)</li> <li>Lambda &amp; Azure Functions (AWS &amp; Azure)</li> <li>DynamoDB &amp; CosmosDB (AWS &amp; Azure)</li> <li>Azure FrontDoor &amp; CDN (Azure)</li> <li>CloudFront &amp; ACM (AWS)</li> <li>API Gateway &amp; CloudWatch (AWS)</li> <li>S3 &amp; Azure Storage Accounts (AWS &amp; Azure)</li> <li>Python, JavaScript, Html (AWS &amp; Azure)</li> <li>More details here</li> </ul>"},{"location":"learn/#build-a-3-tier-bird-sightings-app-in-aws","title":"\ud83d\udc26 Build a 3-tier Bird Sightings App in AWS","text":"<ul> <li>ChatGPT (web content creation)</li> <li>AWS S3 (static website hosting)</li> <li>AWS DynamoDB (database backend)</li> <li>AWS Lambda (data capture)</li> <li>AWS Application Gateway</li> </ul>"},{"location":"learn/#azure-courses","title":"\u2601\ufe0f Azure Courses","text":"<ul> <li>Microsoft Azure Solutions Architect (AZ-305) - James Lee</li> <li>Microsoft DevOps Engineer (AZ-400) - James Lee</li> <li>Microsoft Azure Security Engineer (AZ-500) - James Lee</li> <li>Microsoft Azure Administrator (AZ-104) - James Lee</li> </ul>"},{"location":"learn/#terraform-courses","title":"\ud83c\udf0d Terraform Courses","text":"<ul> <li>Build a Dev Environment with Terraform and Azure - Derek Morgan</li> <li>More Than Certified In Terraform - Derek Morgan</li> <li>Terraform Course - Automate your AWS cloud infrastructure</li> </ul>"},{"location":"learn/#aws-certification-courses","title":"\ud83c\udf93 AWS Certification Courses","text":"<ul> <li>AWS Certified SysOps Administrator - Associate - Adrian Cantrill</li> <li>AWS Certified Developer - Associate - Adrian Cantrill</li> <li>AWS Certified DevOps Engineer - Professional - Adrian Cantrill</li> <li>AWS Certified Solutions Architect - Associate (SAA-C03) - Adrian Cantrill</li> <li>AWS Certified DevOps Engineer Professional 2023 - DOP-C02 - St\u00e9phane Maarek</li> </ul>"},{"location":"learn/#other-courses-resources","title":"\ud83c\udfa5 Other Courses &amp; Resources","text":"<ul> <li>Data Engineering Course for Beginners - freeCodeCamp.org</li> <li>Microsoft 365 Cloud Security Administration - Hands On</li> <li>How to Build a 3 Tier Architecture in AWS</li> <li>Extract Email Attachment using AWS</li> <li>5 Mini Python Projects - For Beginners</li> </ul> <p>\ud83d\udee0 Built with MKDocs</p>"},{"location":"work/","title":"Experience","text":""},{"location":"work/#cloud-it-infrastructure-lead-ax-ltd-dec-2024-present","title":"Cloud &amp; IT Infrastructure Lead: AX Ltd (Dec 2024 - Present)","text":"<ul> <li>Cloud Operations: Manage and optimize Azure and AWS environments, ensuring secure, scalable, and cost-efficient infrastructure to support business operations.</li> <li>Identity &amp; Security: Maintain secure identity systems with Azure AD, including MFA, Conditional Access Policies, and Privileged Identity Management, while enhancing endpoint protection using Microsoft Defender for Endpoint and Intune.</li> <li>Governance &amp; Compliance: Establish and enforce governance policies with Azure Policy, Group Policy, and Defender for Cloud, ensuring regulatory compliance and secure configurations.</li> <li>Infrastructure Oversight: Oversee critical systems like Active Directory, DNS, and SSL administration, ensuring high availability and operational reliability.</li> <li>Monitoring &amp; Performance: Use tools such as Azure Monitor, AWS CloudWatch, and PRTG Monitoring for proactive performance management and issue resolution.</li> <li>Strategic Planning: Drive innovation by identifying opportunities for technology adoption, cloud optimization, and application modernization. Support organizational efficiency by integrating DevOps practices, including CI/CD pipelines using Azure DevOps and GitHub Actions, to streamline deployments and improve workflows.</li> <li>Collaboration &amp; Mentorship: Partner with teams across the organization to align IT strategy with business goals and provide mentorship to enhance team expertise and innovation.</li> </ul>"},{"location":"work/#it-infrastructure-manager-ax-ltd-jan-2017-dec-2024","title":"IT Infrastructure Manager: AX Ltd (Jan 2017 - Dec 2024)","text":"<ul> <li>Directed a full-scale cloud migration to Microsoft Azure, decommissioning primary and secondary data centres, significantly reducing physical IT infrastructure, and lowering operational costs.</li> <li>Managed and mentored a team of five infrastructure engineers, delivering key projects, including SD-WAN deployment, network modernization, virtualization platform refreshes, and datacentre relocations, improving cost efficiency, resilience, and performance.</li> <li>Designed and implemented infrastructure for new remote depot sites, reducing deployment times while ensuring seamless integration with existing systems.</li> <li>Optimized vendor relationships and licensing agreements, reducing annual costs while maintaining compliance and service quality.</li> </ul>"},{"location":"work/#senior-it-infrastructure-engineer-ax-ltd-nov-2011-dec-2024","title":"Senior IT Infrastructure Engineer: AX Ltd (Nov 2011 - Dec 2024)","text":"<ul> <li>Designed and executed multiple Microsoft Exchange migrations and domain controller refreshes, achieving improved performance, reliability, and a reduction in hardware requirements.</li> <li>Implemented and optimized Azure AD services, including MFA, Conditional Access policies, and Privileged Identity Management, ensuring secure identity management across hybrid environments.</li> <li>Deployed Microsoft Intune and the Defender suite, enhancing endpoint security and streamlining device management to support a mobile-first workforce.</li> <li>Spearheaded security enhancements, introducing email hygiene solutions, refining Active Directory Group Policies, and improving governance to strengthen the organization's overall security posture.</li> <li>Delivered advanced infrastructure monitoring and troubleshooting processes, leveraging automation to reduce incident resolution times and improving system reliability.</li> </ul>"},{"location":"work/#skills-competencies","title":"Skills &amp; Competencies","text":"<ul> <li>Microsoft Azure: Azure AD, Intune, PIM, Security Center, Sentinel, Monitor, Automation, Backup, ASR, Key Vault, Policy, Arc, Bastion.</li> <li>Azure Applications: Application Gateway, Load Balancer, WAF, App Service, Application Proxy.</li> <li>AWS: IAM, EC2, S3, Route53, VPCs, VPNs, App Gateway, DynamoDB, Lambda, CloudFormation, CloudTrail.</li> <li>Collaboration Platforms: Microsoft 365, Exchange Online, Teams, SharePoint Online.</li> <li>Windows Server: Administration of Windows Server 2003-2022.</li> <li>Microsoft Exchange: On-premises and hybrid migrations, server optimization.</li> <li>Mimecast: Email security and compliance.</li> <li>Windows IIS: Configuration and management.</li> <li>Microsoft Security Suite: Defender for Endpoint, Identity, Cloud, email hygiene.</li> <li>IAM &amp; PAM: Azure AD (Entra ID), MFA, SSPR, Group Policy, Conditional Access, AWS IAM.</li> <li>Networking: DNS, firewalls, VPNs, Azure WAF, Load Balancer, Cato SD-WAN, network peering.</li> <li>Infrastructure Optimization: AD streamlining, domain controller refreshes, DHCP, SSL administration.</li> <li>IoT Platforms: AWS IoT, Azure IoT.</li> <li>Infrastructure as Code: Terraform, CloudFormation, Bicep, ARM templates.</li> <li>Scripting &amp; Automation: PowerShell, Azure CLI, Python.</li> <li>DevOps Practices: Azure DevOps, AWS CodePipeline, GitHub, Docker, Kubernetes, AKS.</li> <li>Monitoring &amp; Troubleshooting: Azure Monitor, AWS CloudWatch, Log Analytics, PRTG Monitoring.</li> <li>Documentation &amp; Training: Technical documentation, best practices, security protocols.</li> </ul>"},{"location":"work/#contact","title":"Contact","text":"<ul> <li>Email</li> <li>LinkedIn</li> </ul>"},{"location":"blog/welcome-to-my-blog/","title":"Welcome to My Blog","text":"<p>Hello and welcome to my blog! \ud83c\udf89</p> <p>This blog is a place where I'll be documenting my projects, sharing insights, and keeping track of useful information related to IT, cloud infrastructure, automation, and more. If you're here, chances are you're interested in similar topics, so I hope you find this content helpful!</p>","tags":["technology","learning"]},{"location":"blog/welcome-to-my-blog/#why-this-blog","title":"Why This Blog?","text":"<p>I've started this blog as a centralized knowledge base for my work and learning experiences. Rather than storing documentation across different platforms, I'll be using MkDocs to create my content, and GitHub to keep everything structured and easily accessible.</p>","tags":["technology","learning"]},{"location":"blog/welcome-to-my-blog/#what-to-expect","title":"What to Expect","text":"<p>Here are some of the topics I'll be covering:</p> <ul> <li>IT Infrastructure &amp; Cloud Technologies \u2013 Insights on Azure, AWS, and hybrid environments.</li> <li>Automation &amp; Scripting \u2013 Useful scripts, automation techniques, and efficiency tips.</li> <li>Cybersecurity &amp; Compliance \u2013 Best practices and lessons learned.</li> <li>Troubleshooting &amp; Solutions \u2013 Step-by-step guides to solving technical challenges.</li> <li>Personal Learning &amp; Development \u2013 Certifications, learning resources, and productivity tips.</li> </ul>","tags":["technology","learning"]},{"location":"blog/welcome-to-my-blog/#next-steps","title":"Next Steps","text":"<p>I'll be updating this blog regularly with useful content, so stay tuned! If you have any suggestions or topics you'd like me to cover, feel free to reach out.</p> <p>Thanks for stopping by, and I hope you find this blog useful! \ud83d\ude80</p>","tags":["technology","learning"]},{"location":"blog/first-steps-into-ai-automation-my-journey-from-trial-to-self-hosted-chaos/","title":"\ud83e\udd16 First Steps into AI Automation: My Journey from Trial to Self-Hosted Chaos","text":"<p>What started as 'let me just automate some emails' somehow turned into a comprehensive exploration of every AI automation platform and deployment method known to mankind...</p> <p>After months of reading about AI automation tools and watching everyone else's productivity skyrocket with clever workflows, I finally decided to stop being a spectator and dive in myself. What started as a simple \"let's automate job alert emails\" experiment quickly became a week-long journey through cloud trials, self-hosted deployments, OAuth authentication battles, and enough Docker containers to power a small data centre.</p> <p>In this post, you'll discover:</p> <ul> <li>Real costs of AI automation experimentation ($10-50 range)</li> <li>Why self-hosted OAuth2 is significantly harder than cloud versions</li> <li>Performance differences: Pi 5 vs. desktop hardware for local AI</li> <li>When to choose local vs. cloud AI models</li> <li>Time investment reality: ~10 hours over 1 week for this project</li> </ul> <p>Here's how my first real foray into AI automation unfolded \u2014 spoiler alert: it involved more container migrations than I initially planned.</p> <p>Hardware baseline for this project:</p> <p>\ud83d\udcbb Development Environment</p> <ul> <li>Primary machine: AMD Ryzen 7 5800X, 32GB DDR4, NVMe SSD</li> <li>Pi 5 setup: 8GB RAM, microSD storage  </li> <li>Network: Standard home broadband (important for cloud API performance)</li> </ul>","tags":["n8n","Ollama","OpenWebUI","Azure OpenAI","Docker","Email Automation","LLM","Self-Hosting"]},{"location":"blog/first-steps-into-ai-automation-my-journey-from-trial-to-self-hosted-chaos/#the-mission-taming-job-alert-email-chaos","title":"\ud83c\udfaf The Mission: Taming Job Alert Email Chaos","text":"<p>Let's set the scene. If you're drowning in recruitment emails like I was, spending 30+ minutes daily parsing through multiple job listings scattered across different emails, you'll understand the frustration. Each recruitment platform has its own format, some emails contain 5-10 different opportunities, and manually extracting the relevant URLs was becoming a productivity killer.</p> <p>The vision: Create an automated workflow that would:</p> <ul> <li>Scrape job-related emails from my Outlook.com inbox</li> <li>Extract and clean the job data using AI</li> <li>Generate a neat summary email with all the job URLs in one place</li> <li>Send it back to me in a digestible format</li> </ul> <p>Simple enough, right? Famous last words.</p>","tags":["n8n","Ollama","OpenWebUI","Azure OpenAI","Docker","Email Automation","LLM","Self-Hosting"]},{"location":"blog/first-steps-into-ai-automation-my-journey-from-trial-to-self-hosted-chaos/#phase-1-the-n8n-cloud-trial-adventure","title":"\ud83d\udd04 Phase 1: The n8n Cloud Trial Adventure","text":"<p>My research pointed to n8n as the go-to tool for this kind of automation workflow. Being sensible, I started with their 14-day cloud trial rather than jumping straight into self-hosting complexities.</p>","tags":["n8n","Ollama","OpenWebUI","Azure OpenAI","Docker","Email Automation","LLM","Self-Hosting"]},{"location":"blog/first-steps-into-ai-automation-my-journey-from-trial-to-self-hosted-chaos/#initial-setup-first-success","title":"\u2699\ufe0f Initial Setup &amp; First Success","text":"<p>The n8n cloud interface is genuinely impressive \u2014 drag-and-drop workflow building with a proper visual editor that actually makes sense. Within a couple of hours, I had:</p> <p>\u2705 Connected to Outlook.com via their built-in connector \u2705 Set up email filtering to grab job-related messages \u2705 Configured basic data processing to extract text content \u2705 Integrated OpenAI API for intelligent job URL extraction  </p> <p></p>","tags":["n8n","Ollama","OpenWebUI","Azure OpenAI","Docker","Email Automation","LLM","Self-Hosting"]},{"location":"blog/first-steps-into-ai-automation-my-journey-from-trial-to-self-hosted-chaos/#the-ai-integration-challenge","title":"\ud83e\udd16 The AI Integration Challenge","text":"<p>This is where things got interesting. Initially, I connected the workflow to my OpenAI API account, using GPT-4 to parse email content and extract job URLs. The AI component worked brilliantly \u2014 almost too well, since I managed to burn through my $10 worth of token credits in just two days of testing.</p> <p>The cost reality: Those \"just testing a few prompts\" sessions add up fast. A single complex email with multiple job listings processed through GPT-4 was costing around $0.15-0.30 per API call. When you're iterating on prompts and testing edge cases, those costs compound quickly.</p> <p>Lesson learned: Test with smaller models first, then scale up. GPT-4 is excellent but not cheap for experimental workflows.</p>","tags":["n8n","Ollama","OpenWebUI","Azure OpenAI","Docker","Email Automation","LLM","Self-Hosting"]},{"location":"blog/first-steps-into-ai-automation-my-journey-from-trial-to-self-hosted-chaos/#partial-success-the-classic-it-story","title":"\ud83c\udfaf Partial Success (The Classic IT Story)","text":"<p>The workflow was partially successful \u2014 and in true IT fashion, \"partially\" is doing some heavy lifting here. While the automation successfully processed emails and generated summaries, it had one glaring limitation: it only extracted one job URL per email, when most recruitment emails contain multiple opportunities.</p> <p>What this actually meant: A typical recruitment email might contain 5-7 job listings with individual URLs, but my workflow would only capture the first one it encountered. This wasn't a parsing issue \u2014 the AI was correctly identifying all the URLs in its response, but the n8n workflow was only processing the first result from the AI output.</p> <p>Why this limitation exists: The issue stemmed from how I'd configured the data processing nodes in n8n. The workflow was treating the AI response as a single data item rather than an array of multiple URLs. This is a common beginner mistake when working with structured data outputs.</p> <p>This became the recurring theme of my experimentation week: everything works, just not quite how you want it to.</p>","tags":["n8n","Ollama","OpenWebUI","Azure OpenAI","Docker","Email Automation","LLM","Self-Hosting"]},{"location":"blog/first-steps-into-ai-automation-my-journey-from-trial-to-self-hosted-chaos/#enter-azure-openai","title":"\ud83d\udca1 Enter Azure OpenAI","text":"<p>Rather than continue burning through OpenAI credits, I pivoted to Azure OpenAI. This turned out to be a smart move for several reasons:</p> <ul> <li>Cost control: Better integration with my existing Azure credits</li> <li>Familiar environment: Already comfortable with Azure resource management  </li> <li>Testing flexibility: My Visual Studio Developer subscription gives me \u00a3120 monthly credits</li> </ul> <p>I deployed a GPT-4 Mini model in my test lab Azure tenant \u2014 perfect for experimentation without breaking the bank.</p> <p></p> <p>The Azure OpenAI integration worked seamlessly with n8n, and I successfully redirected my workflow to use the new endpoint. Finally, something that worked first time.</p> <p></p>","tags":["n8n","Ollama","OpenWebUI","Azure OpenAI","Docker","Email Automation","LLM","Self-Hosting"]},{"location":"blog/first-steps-into-ai-automation-my-journey-from-trial-to-self-hosted-chaos/#phase-2-self-hosting-ambitions-container-edition-1","title":"\ud83d\udc33 Phase 2: Self-Hosting Ambitions (Container Edition #1)","text":"<p>With the n8n cloud trial clocking ticking down, I faced the classic build-vs-buy decision. The cloud version was excellent, but I wanted full control and the ability to experiment without subscription constraints. The monthly $20 cost wasn't prohibitive, but the learning opportunity of self-hosting was too appealing to pass up.</p> <p>Enter self-hosting with Docker containers \u2014 specifically, targeting my Raspberry Pi 5 setup.</p>","tags":["n8n","Ollama","OpenWebUI","Azure OpenAI","Docker","Email Automation","LLM","Self-Hosting"]},{"location":"blog/first-steps-into-ai-automation-my-journey-from-trial-to-self-hosted-chaos/#the-openmediavault-experiment","title":"\ud83c\udfe0 The OpenMediaVault Experiment","text":"<p>My first attempt involved deploying n8n as a self-hosted Docker container on my OpenMediaVault (OMV) setup. For those unfamiliar, OMV is a network-attached storage (NAS) solution built on Debian, perfect for home lab environments where you want proper storage management with container capabilities.</p> <p>Why the Pi 5 + OMV route:</p> <ul> <li>Always-on availability: Unlike my main PC, the Pi runs 24/7</li> <li>Low power consumption: Perfect for continuous automation workflows</li> <li>Storage integration: OMV provides excellent Docker volume management</li> <li>Learning opportunity: Understanding self-hosted deployment challenges</li> </ul> <p>The setup:</p> <ul> <li>Host: Raspberry Pi 5 running OpenMediaVault</li> <li>Backend storage: NAS device for persistent data</li> <li>Database: PostgreSQL container for n8n's backend</li> <li>Edition: n8n Community Edition (self-hosted)</li> </ul> <p></p>","tags":["n8n","Ollama","OpenWebUI","Azure OpenAI","Docker","Email Automation","LLM","Self-Hosting"]},{"location":"blog/first-steps-into-ai-automation-my-journey-from-trial-to-self-hosted-chaos/#the-great-oauth-authentication-battle","title":"\ud83d\ude24 The Great OAuth Authentication Battle","text":"<p>This is where my self-hosting dreams met reality with a resounding thud.</p> <p>I quickly discovered that replicating my cloud workflow wasn't going to be straightforward. The self-hosted community edition has functionality restrictions compared to the cloud version, but more frustratingly, I couldn't get OAuth2 authentication working properly.</p> <p>Why OAuth2 is trickier with self-hosted setups:</p> <ul> <li>Redirect URI complexity: Cloud services handle callback URLs automatically, but self-hosted instances need manually configured redirect URIs</li> <li>App registration headaches: Azure app registrations expect specific callback patterns that don't align well with dynamic self-hosted URLs</li> <li>Token management: Cloud versions handle OAuth token refresh automatically; self-hosted requires manual configuration</li> <li>Security certificate requirements: Many OAuth providers now require HTTPS callbacks, adding SSL certificate management complexity</li> </ul> <p>The specific challenges I hit:</p> <ul> <li>Outlook.com authentication: Couldn't configure OAuth2 credentials using an app registration from my test lab Azure tenant</li> <li>Exchange Online integration: Also failed to connect via app registration \u2014 kept getting \"invalid redirect URI\" errors</li> <li>Documentation gaps: Self-hosting authentication setup felt less polished than the cloud version</li> </ul> <p>After several hours over two days debugging OAuth flows and Azure app registrations, I admitted defeat on the email integration front. Sometimes retreat is the better part of valour.</p>","tags":["n8n","Ollama","OpenWebUI","Azure OpenAI","Docker","Email Automation","LLM","Self-Hosting"]},{"location":"blog/first-steps-into-ai-automation-my-journey-from-trial-to-self-hosted-chaos/#simple-success-weather-api-workflow","title":"\ud83c\udf24\ufe0f Simple Success: Weather API Workflow","text":"<p>Rather than abandon the entire self-hosting experiment, I pivoted to a simpler proof-of-concept. I created a basic workflow using:</p> <ul> <li>OpenWeatherMap API for weather data</li> <li>Gmail integration with app passwords (much simpler than OAuth2)</li> <li>Basic data processing and email generation</li> </ul> <p>This worked perfectly and proved that the self-hosted n8n environment was functional \u2014 the issue was specifically with the more complex authentication requirements of my original workflow.</p> <p></p>","tags":["n8n","Ollama","OpenWebUI","Azure OpenAI","Docker","Email Automation","LLM","Self-Hosting"]},{"location":"blog/first-steps-into-ai-automation-my-journey-from-trial-to-self-hosted-chaos/#phase-3-the-wsl-migration-container-migration-2","title":"\ud83d\udc33 Phase 3: The WSL Migration (Container Migration #2)","text":"<p>While the Pi 5 setup was working fine for simple workflows, I started feeling the hardware limitations when testing more complex operations. Loading even smaller AI models was painfully slow, and memory constraints meant I couldn't experiment with anything approaching production-scale workflows.</p> <p>Time for Container Migration #2.</p>","tags":["n8n","Ollama","OpenWebUI","Azure OpenAI","Docker","Email Automation","LLM","Self-Hosting"]},{"location":"blog/first-steps-into-ai-automation-my-journey-from-trial-to-self-hosted-chaos/#moving-to-wsl-docker-desktop","title":"\ud83d\udda5\ufe0f Moving to WSL + Docker Desktop","text":"<p>With the Pi 5 hitting performance limits, I decided to experiment with local AI models using Ollama (a local LLM hosting platform) and OpenWebUI (a web interface for interacting with AI models). This required more computational resources than the Pi could provide, so I deployed these tools using Docker Compose inside Ubuntu running on Windows WSL (Windows Subsystem for Linux).</p> <p>This setup offered several advantages:</p> <p>Why WSL over the Pi 5:</p> <ul> <li>Better hardware resources: Access to my Windows PC's 32GB RAM and 8-core CPU vs. Pi 5's 8GB RAM limitation</li> <li>Docker Desktop integration: Visual container management through familiar interface</li> <li>Development flexibility: Easier to iterate and debug workflows with full IDE access</li> <li>Performance reality: Local LLM model loading went from 1+ minutes on Pi 5 to under 30 seconds</li> </ul> <p>My development machine specs:</p> <ul> <li>CPU: AMD Ryzen 7 5800H with Radeon Graphics</li> <li>RAM: 32GB DDR4</li> <li>Storage: NVMe SSD for fast model loading</li> <li>GPU: None (pure CPU inference)</li> </ul> <p>Time Investment Reality:</p> <ul> <li>n8n cloud setup: 2-3 hours (including initial workflow creation)</li> <li>OAuth2 debugging: 3+ hours over 2 days (ongoing challenge)</li> <li>Pi 5 container setup: 2+ hours</li> <li>Docker Desktop container set up: 2+ hours</li> <li>Total project time: ~10 hours over 1 week</li> </ul> <p>The new stack:</p> <ul> <li>Host: Ubuntu in WSL2 on Windows</li> <li>Container orchestration: Docker Compose</li> <li>Management: Docker Desktop for Windows</li> <li>Models: Ollama for local LLM hosting</li> <li>Interface: OpenWebUI for model interaction</li> </ul> <p></p>","tags":["n8n","Ollama","OpenWebUI","Azure OpenAI","Docker","Email Automation","LLM","Self-Hosting"]},{"location":"blog/first-steps-into-ai-automation-my-journey-from-trial-to-self-hosted-chaos/#local-llm-experimentation","title":"\ud83e\udde0 Local LLM Experimentation","text":"<p>This is where the project took an interesting turn. Rather than continuing to rely on cloud APIs, I started experimenting with local language models through Ollama.</p> <p>Why local LLMs?</p> <ul> <li>Cost control: No per-token charges for experimentation</li> <li>Privacy: Sensitive data stays on local infrastructure</li> <li>Learning opportunity: Understanding how different models perform</li> </ul> <p>The Docker Compose setup made it trivial to spin up different model combinations and test their performance on my email processing use case.</p>","tags":["n8n","Ollama","OpenWebUI","Azure OpenAI","Docker","Email Automation","LLM","Self-Hosting"]},{"location":"blog/first-steps-into-ai-automation-my-journey-from-trial-to-self-hosted-chaos/#reality-check-local-vs-cloud-performance","title":"\u26a0\ufe0f Reality Check: Local vs. Cloud Performance","text":"<p>Let's be honest here \u2014 using an LLM locally is never going to be a fully featured replacement for the likes of ChatGPT or Claude. This became apparent pretty quickly during my testing.</p> <p>Performance realities:</p> <ul> <li>Speed: Unless you're running some serious hardware, the performance will be a lot slower than the online AI counterparts</li> <li>Model capabilities: Local models (especially smaller ones that run on consumer hardware) lack the sophisticated reasoning of GPT-4 or Claude</li> <li>Resource constraints: My standard PC setup meant I was limited to smaller model variants</li> <li>Response quality: Noticeably less nuanced and accurate responses compared to cloud services</li> </ul> <p>Where local LLMs do shine:</p> <ul> <li>Privacy-sensitive tasks: When you can't send data to external APIs</li> <li>Development and testing: Iterating on prompts without burning through API credits</li> <li>Learning and experimentation: Understanding how different model architectures behave</li> <li>Offline scenarios: When internet connectivity is unreliable</li> </ul> <p>The key insight: local LLMs are a complement to cloud services, not a replacement. Use them when privacy, cost, or learning are the primary concerns, but stick with cloud APIs when you need reliable, high-quality results.</p>","tags":["n8n","Ollama","OpenWebUI","Azure OpenAI","Docker","Email Automation","LLM","Self-Hosting"]},{"location":"blog/first-steps-into-ai-automation-my-journey-from-trial-to-self-hosted-chaos/#hybrid-approach-best-of-both-worlds","title":"\ud83d\udd17 Hybrid Approach: Best of Both Worlds","text":"<p>The final configuration became a hybrid approach:</p> <ul> <li>OpenWebUI connected to Azure OpenAI for production-quality responses</li> <li>Local Ollama models for development and privacy-sensitive testing</li> <li>Docker containers exposed through Docker Desktop for easy management</li> </ul> <p>This gave me the flexibility to choose the right tool for each task \u2014 cloud APIs when I need reliability and performance, local models when I want to experiment or maintain privacy.</p> <p></p>","tags":["n8n","Ollama","OpenWebUI","Azure OpenAI","Docker","Email Automation","LLM","Self-Hosting"]},{"location":"blog/first-steps-into-ai-automation-my-journey-from-trial-to-self-hosted-chaos/#cost-reality-check","title":"\ud83d\udcb0 Cost Reality Check","text":"<p>After a week of experimentation, here's how the costs actually broke down:</p> Service Trial Period Monthly Cost My Usage Notes n8n Cloud 14 days free \u20ac20/month 2 weeks testing Full OAuth2 features OpenAI API Pay-per-use Variable $10 in 2 days Expensive for testing Azure OpenAI Free credits \u00a3120/month budget ~\u00a315 used Better for experimentation Self-hosted Free Hardware + time 2 days setup OAuth2 complexity <p>Key insight: The \"free\" self-hosted option came with a significant time cost \u2014 debugging authentication issues for hours vs. having things work immediately in the cloud version.</p>","tags":["n8n","Ollama","OpenWebUI","Azure OpenAI","Docker","Email Automation","LLM","Self-Hosting"]},{"location":"blog/first-steps-into-ai-automation-my-journey-from-trial-to-self-hosted-chaos/#current-state-lessons-learned-next-steps","title":"\ud83d\udcca Current State: Lessons Learned &amp; Next Steps","text":"<p>After a week of container deployments, OAuth battles, and API integrations, here's where I've landed:</p>","tags":["n8n","Ollama","OpenWebUI","Azure OpenAI","Docker","Email Automation","LLM","Self-Hosting"]},{"location":"blog/first-steps-into-ai-automation-my-journey-from-trial-to-self-hosted-chaos/#whats-working-well","title":"\u2705 What's Working Well","text":"<p>Technical Stack:</p> <ul> <li>n8n self-hosted: Currently running 2 active workflows (weather alerts, basic data processing)</li> <li>Azure OpenAI integration: Reliable and cost-effective for AI processing \u2014 saving ~\u00a325/month vs. direct OpenAI API</li> <li>Docker containerisation: Easy deployment and management across different environments</li> <li>WSL environment: 10x performance improvement over Pi 5 for local AI model loading</li> </ul> <p>Process Improvements:</p> <ul> <li>Iterative approach: Start simple, add complexity gradually \u2014 this saved significant debugging time</li> <li>Hybrid cloud/local strategy: Use the right tool for each requirement rather than forcing one solution</li> <li>Container flexibility: Easy to migrate and scale across different hosts when hardware constraints appear</li> </ul> <p>Daily productivity impact: While the original job email automation isn't fully solved, the weather automation saves ~10 minutes daily, and the learning has already paid dividends in other automation projects.</p>","tags":["n8n","Ollama","OpenWebUI","Azure OpenAI","Docker","Email Automation","LLM","Self-Hosting"]},{"location":"blog/first-steps-into-ai-automation-my-journey-from-trial-to-self-hosted-chaos/#ongoing-challenges-the-work-in-progress-list","title":"\u26a0\ufe0f Ongoing Challenges (The Work-in-Progress List)","text":"<p>Authentication Issues:</p> <ul> <li>OAuth2 integration with Outlook.com/Exchange Online still unresolved</li> <li>Need to explore alternative authentication methods or different email providers</li> <li>May require diving deeper into Azure app registration configurations</li> </ul> <p>Workflow Limitations:</p> <ul> <li>Original job email processing goal partially achieved but needs refinement</li> <li>Multiple job URL extraction per email still needs work</li> <li>Error handling and retry logic need improvement</li> </ul> <p>Infrastructure Decisions:</p> <ul> <li>Balancing local vs. cloud resources for different use cases</li> <li>Determining optimal Docker deployment strategy for production workflows</li> <li>Managing costs across multiple AI service providers</li> </ul> <p>Decision-making process during failures: When something doesn't work, I typically: (1) Troubleshoot the exact error using ChatGPT or Anthropic Claude, (2) Search for similar issues in community forums, (3) Try a simpler alternative approach, (4) If still stuck after 2-3 hours, pivot to a different method rather than continuing to debug indefinitely.</p>","tags":["n8n","Ollama","OpenWebUI","Azure OpenAI","Docker","Email Automation","LLM","Self-Hosting"]},{"location":"blog/first-steps-into-ai-automation-my-journey-from-trial-to-self-hosted-chaos/#next-steps-future-experiments","title":"\ud83d\ude80 Next Steps &amp; Future Experiments","text":"<p>Short-term goals (next 2-4 weeks):</p> <ol> <li>Resolve OAuth2 authentication for proper email integration</li> <li>Improve job URL extraction accuracy \u2014 tackle the multiple URLs per email challenge</li> <li>Add error handling and logging to existing workflows</li> <li>Explore alternative email providers if Outlook.com integration remains problematic</li> </ol> <p>Medium-term exploration (next 2-3 months):</p> <ol> <li>Local LLM performance tuning for specific use cases</li> <li>Workflow templates for common automation patterns</li> <li>Integration with other productivity tools (calendar, task management)</li> <li>Monitoring and alerting for automated workflows</li> </ol>","tags":["n8n","Ollama","OpenWebUI","Azure OpenAI","Docker","Email Automation","LLM","Self-Hosting"]},{"location":"blog/first-steps-into-ai-automation-my-journey-from-trial-to-self-hosted-chaos/#quick-wins-for-beginners","title":"\ud83d\udee0\ufe0f Quick Wins for Beginners","text":"<p>If you're just starting your AI automation journey, here are the lessons learned that could save you time:</p> <p>\ud83c\udfaf Start Simple First</p> <ul> <li>Begin with n8n cloud trial to understand the platform without authentication headaches</li> <li>Use simple APIs (weather, RSS feeds) before tackling complex ones (email OAuth2)</li> <li>Test with smaller AI models before jumping to GPT-4</li> </ul> <p>\ud83d\udca1 Budget for Experimentation</p> <ul> <li>Set aside $20-50 for API testing \u2014 it goes faster than you think</li> <li>Azure OpenAI credits can be more cost-effective than direct OpenAI API for learning</li> <li>Factor in time costs when choosing self-hosted vs. cloud solutions</li> </ul> <p>\ud83d\udd27 Have Fallback Options Ready</p> <ul> <li>Plan alternative authentication methods (app passwords vs. OAuth2)</li> <li>Keep both cloud and local AI options available</li> <li>Document what works and what doesn't for future reference</li> </ul>","tags":["n8n","Ollama","OpenWebUI","Azure OpenAI","Docker","Email Automation","LLM","Self-Hosting"]},{"location":"blog/first-steps-into-ai-automation-my-journey-from-trial-to-self-hosted-chaos/#technical-resources-documentation","title":"\ud83d\udd27 Technical Resources &amp; Documentation","text":"<p>For anyone inspired to start their own AI automation journey, here are the key resources that proved invaluable:</p>","tags":["n8n","Ollama","OpenWebUI","Azure OpenAI","Docker","Email Automation","LLM","Self-Hosting"]},{"location":"blog/first-steps-into-ai-automation-my-journey-from-trial-to-self-hosted-chaos/#core-tools-platforms","title":"\ud83d\udee0\ufe0f Core Tools &amp; Platforms","text":"<ul> <li>n8n \u2014 Visual workflow automation platform</li> <li>Docker \u2014 Containerisation platform</li> <li>Docker Compose \u2014 Multi-container orchestration tool</li> <li>OpenMediaVault \u2014 NAS/storage management solution</li> </ul>","tags":["n8n","Ollama","OpenWebUI","Azure OpenAI","Docker","Email Automation","LLM","Self-Hosting"]},{"location":"blog/first-steps-into-ai-automation-my-journey-from-trial-to-self-hosted-chaos/#ai-llm-resources","title":"\ud83e\udd16 AI &amp; LLM Resources","text":"<ul> <li>Ollama \u2014 Local LLM hosting platform</li> <li>OpenWebUI \u2014 Web interface for local LLM interaction</li> <li>Azure OpenAI Service \u2014 Enterprise AI API platform</li> <li>OpenAI API \u2014 Direct API access to GPT models</li> </ul>","tags":["n8n","Ollama","OpenWebUI","Azure OpenAI","Docker","Email Automation","LLM","Self-Hosting"]},{"location":"blog/first-steps-into-ai-automation-my-journey-from-trial-to-self-hosted-chaos/#setup-guides-documentation","title":"\ud83d\udcda Setup Guides &amp; Documentation","text":"<ul> <li>n8n Self-Hosting Guide</li> <li>Docker Installation Documentation</li> <li>Azure OpenAI Quickstart \u2014 Getting started with Azure OpenAI</li> <li>WSL Installation Guide \u2014 Windows Subsystem for Linux setup</li> </ul>","tags":["n8n","Ollama","OpenWebUI","Azure OpenAI","Docker","Email Automation","LLM","Self-Hosting"]},{"location":"blog/first-steps-into-ai-automation-my-journey-from-trial-to-self-hosted-chaos/#troubleshooting-common-issues","title":"\ud83d\udd27 Troubleshooting Common Issues","text":"<p>Based on my week of trial and error, here are the most common problems you'll likely encounter:</p> <p>\ud83d\udd10 OAuth2 Authentication Failures</p> <p>Symptoms: \"Invalid redirect URI\" or \"Authentication failed\" errors when connecting to email services.</p> <p>Likely causes:</p> <ul> <li>Redirect URI mismatch between app registration and n8n configuration</li> <li>Self-hosted instance not using HTTPS for callbacks</li> <li>App registration missing required API permissions</li> </ul> <p>Solutions to try:</p> <ul> <li>Use app passwords instead of OAuth2 where possible (Gmail, Outlook.com) \u2014 Note: App passwords are simpler username/password credentials that bypass OAuth2 complexity but offer less security</li> <li>Ensure your n8n instance is accessible via HTTPS with valid SSL certificate</li> <li>Double-check app registration redirect URIs match exactly (including trailing slashes)</li> <li>Start with cloud trial to verify workflow logic before self-hosting</li> </ul> <p>\ud83d\udc33 Container Performance Issues</p> <p>Symptoms: Slow model loading, container crashes, high memory usage.</p> <p>Likely causes:</p> <ul> <li>Insufficient RAM allocation to Docker</li> <li>CPU-intensive models running on inadequate hardware</li> <li>Competing containers for limited resources</li> </ul> <p>Solutions to try:</p> <ul> <li>Increase Docker memory limits in Docker Desktop settings</li> <li>Use smaller model variants (7B instead of 13B+ parameters)</li> <li>Monitor resource usage with <code>docker stats</code> command</li> <li>Consider migrating from Pi to x86 hardware for better performance</li> </ul> <p>\ud83d\udcb8 API Rate Limiting and Costs</p> <p>Symptoms: API calls failing, unexpected high costs, token limits exceeded.</p> <p>Likely causes:</p> <ul> <li>Testing with expensive models (GPT-4) instead of cheaper alternatives</li> <li>No rate limiting in workflow configurations</li> <li>Inefficient prompt design causing high token usage</li> </ul> <p>Solutions to try:</p> <ul> <li>Start testing with GPT-3.5-turbo or GPT-4-mini models</li> <li>Implement workflow rate limiting and retry logic</li> <li>Optimize prompts to reduce token consumption</li> <li>Set API spending alerts in provider dashboards</li> </ul> <p>\ud83d\udcbb Resource Requirements Summary</p> <p>Minimum Requirements for Recreation:</p> <ul> <li>Cloud approach: n8n trial account + $20-50 API experimentation budget</li> <li>Self-hosted approach: 8GB+ RAM, Docker knowledge, 2-3 days setup time</li> <li>Local AI experimentation: 16GB+ RAM recommended, considerable patience, NVMe storage preferred</li> <li>Network: Stable broadband connection for cloud API performance</li> </ul>","tags":["n8n","Ollama","OpenWebUI","Azure OpenAI","Docker","Email Automation","LLM","Self-Hosting"]},{"location":"blog/first-steps-into-ai-automation-my-journey-from-trial-to-self-hosted-chaos/#final-thoughts-the-joy-of-controlled-chaos","title":"\ud83d\udcad Final Thoughts: The Joy of Controlled Chaos","text":"<p>What started as a simple email automation project became a comprehensive exploration of modern AI automation tools. While I didn't achieve my original goal completely (yet), the journey provided invaluable hands-on experience with:</p> <ul> <li>Container orchestration across different environments</li> <li>AI service integration patterns and best practices  </li> <li>Authentication complexity in self-hosted vs. cloud environments</li> <li>Hybrid deployment strategies for flexibility and cost control</li> </ul> <p>The beauty of this approach is that each \"failed\" experiment taught me something valuable about the tools and processes involved. The OAuth2 authentication issues, while frustrating, highlighted the importance of proper authentication design. The container migrations demonstrated the flexibility of modern deployment approaches.</p> <p>Most importantly: I now have a functional foundation for AI automation experiments, with both cloud and local capabilities at my disposal.</p> <p>Is it overengineered for a simple email processing task? Absolutely. Was it worth the learning experience? Without question.</p> <p>Have you tackled similar AI automation projects? I'd particularly love to hear from anyone who's solved the OAuth2 self-hosting puzzle or found creative workarounds for email processing limitations. Drop me a line if you've found better approaches to any of these challenges.</p>","tags":["n8n","Ollama","OpenWebUI","Azure OpenAI","Docker","Email Automation","LLM","Self-Hosting"]},{"location":"blog/first-steps-into-ai-automation-my-journey-from-trial-to-self-hosted-chaos/#image-requirements-summary","title":"\ud83d\udcf8 Image Requirements Summary","text":"<p>For anyone recreating this setup, here are the key screenshots included in this post:</p> <ol> <li>n8n-jobs-workflow-openai.png \u2014 Original workflow using direct OpenAI API (the expensive version that burned through $10 in 2 days)</li> <li>azure-openai-deployment.png \u2014 Azure OpenAI Studio showing GPT-4 Mini deployment configuration  </li> <li>n8n-jobs-workflow-azure.png \u2014 Improved workflow using Azure OpenAI integration (the cost-effective version)</li> <li>omv-docker-n8n-containers.png \u2014 OpenMediaVault interface showing Docker container management on Pi 5</li> <li>n8n-weather-workflow.png \u2014 Simple weather API to Gmail workflow demonstrating successful self-hosted setup</li> <li>docker-desktop-ollama.png \u2014 Docker Desktop showing Ollama and OpenWebUI containers running on WSL</li> <li>openwebui-local.png \u2014 OpenWebUI interface showing both Azure OpenAI and local model selection options</li> </ol> <p>Each image demonstrates the practical implementation rather than theoretical concepts, helping readers visualize the actual tools and interfaces involved in the automation journey.</p> <p>Share on  Share on </p>","tags":["n8n","Ollama","OpenWebUI","Azure OpenAI","Docker","Email Automation","LLM","Self-Hosting"]},{"location":"blog/building-ai-powered-infrastructure-monitoring-from-home-lab-to-cloud-production/","title":"Building AI-Powered Infrastructure Monitoring: From Home Lab to Cloud Production","text":"<p>After successfully diving into AI automation with n8n (and surviving the OAuth battles), I decided to tackle a more ambitious learning project: exploring how to integrate AI into infrastructure monitoring systems. The goal was to understand how AI can transform traditional monitoring from simple threshold alerts into intelligent analysis that provides actionable insights\u2014all while experimenting in a safe home lab environment before applying these concepts to production cloud infrastructure.</p> <p>What you'll discover in this post:</p> <ul> <li>Complete monitoring stack deployment using Docker Compose</li> <li>Prometheus and Grafana setup for metrics collection</li> <li>n8n workflow automation for data processing and AI analysis</li> <li>Azure OpenAI integration for intelligent infrastructure insights</li> <li>Professional email reporting with HTML templates</li> <li>Lessons learned for transitioning to production cloud environments</li> <li>Practical skills for integrating AI into traditional monitoring workflows</li> </ul> <p>Here's how I built a home lab monitoring system to explore AI integration patterns that can be applied to production cloud infrastructure.</p> <p>Full disclosure: I'm using a Visual Studio Enterprise subscription which provides \u00a3120 monthly Azure credits. This makes Azure OpenAI experimentation cost-effective for learning purposes. I found direct OpenAI API connections too expensive for extensive experimentation.</p>","tags":["AI","Infrastructure Monitoring","Prometheus","Grafana","n8n","Azure OpenAI","DevOps","Home Lab","Cloud Infrastructure","Workflow Automation"]},{"location":"blog/building-ai-powered-infrastructure-monitoring-from-home-lab-to-cloud-production/#prerequisites-planning","title":"\ud83c\udfaf Prerequisites &amp; Planning","text":"<p>Before diving into the implementation, let's establish what you'll need and the realistic time investment required for this learning project.</p>","tags":["AI","Infrastructure Monitoring","Prometheus","Grafana","n8n","Azure OpenAI","DevOps","Home Lab","Cloud Infrastructure","Workflow Automation"]},{"location":"blog/building-ai-powered-infrastructure-monitoring-from-home-lab-to-cloud-production/#realistic-learning-prerequisites","title":"Realistic Learning Prerequisites","text":"<p>Essential Background Knowledge:</p> <p>Docker &amp; Containerization:</p> <ul> <li>Can deploy multi-container applications with Docker Compose</li> <li>Understand container networking and volume management</li> <li>Can debug why containers can't communicate with each other</li> <li>Familiar with basic Docker commands (logs, exec, inspect)</li> <li>Learning Resource: Docker Official Tutorial - Comprehensive introduction to containerization</li> </ul> <p>API Integration:</p> <ul> <li>Comfortable making HTTP requests with authentication headers</li> <li>Can read and debug JSON responses</li> <li>Understand REST API concepts and error handling</li> <li>Experience with tools like curl or Postman for API testing</li> <li>Learning Resource: REST API Tutorial - Complete guide to RESTful services</li> </ul> <p>Infrastructure Monitoring Concepts:</p> <ul> <li>Know what CPU, memory, and disk metrics actually represent</li> <li>Understand the difference between metrics, logs, and traces</li> <li>Familiar with the concept of time-series data</li> <li>Basic understanding of what constitutes \"normal\" vs \"problematic\" system behavior</li> <li>Learning Resource: Prometheus Documentation - Monitoring fundamentals and concepts</li> </ul> <p>Skills You'll Develop During This Project:</p> <ul> <li>AI prompt engineering for infrastructure analysis</li> <li>Workflow automation with complex orchestration</li> <li>Integration of traditional monitoring with modern AI services</li> <li>Business communication of technical metrics</li> <li>Cost-conscious AI service usage and optimization</li> </ul> <p>Community Learning Resources:</p> <ul> <li>n8n Community: community.n8n.io - Workflow automation support and examples</li> <li>Prometheus Community: prometheus.io/community - Monitoring best practices and troubleshooting</li> <li>Azure OpenAI Documentation: Azure AI Services - Official API documentation and examples</li> <li>Docker Learning: Docker Labs - Hands-on container tutorials</li> </ul>","tags":["AI","Infrastructure Monitoring","Prometheus","Grafana","n8n","Azure OpenAI","DevOps","Home Lab","Cloud Infrastructure","Workflow Automation"]},{"location":"blog/building-ai-powered-infrastructure-monitoring-from-home-lab-to-cloud-production/#honest-time-investment-expectations","title":"Honest Time Investment Expectations","text":"<p>If you have all prerequisites: 1-2 weeks for complete implementation</p> <ul> <li>Basic setup: 2-3 hours</li> <li>AI integration: 4-6 hours</li> <li>Customization and optimization: 6-8 hours</li> <li>Cloud transition planning: 4-6 hours</li> </ul> <p>If missing Docker skills: Add 2-3 weeks for learning fundamentals</p> <ul> <li>Docker basics course: 1-2 weeks</li> <li>Hands-on container practice: 1 week</li> <li>Recommended Learning: Docker Official Tutorial and Play with Docker</li> <li>Then proceed with main project</li> </ul> <p>If new to monitoring: Add 1-2 weeks for infrastructure concepts</p> <ul> <li>Prometheus/Grafana tutorials: 1 week</li> <li>Understanding metrics and alerting: 1 week</li> <li>Recommended Learning: Prometheus Getting Started and Grafana Fundamentals</li> <li>Then integrate AI capabilities</li> </ul> <p>If unfamiliar with APIs: Add 1 week for HTTP/JSON basics</p> <ul> <li>REST API fundamentals: 3-4 days</li> <li>JSON manipulation practice: 2-3 days</li> <li>Authentication concepts: 1-2 days</li> <li>Recommended Learning: HTTP/REST API Tutorial and hands-on practice with JSONPlaceholder</li> </ul>","tags":["AI","Infrastructure Monitoring","Prometheus","Grafana","n8n","Azure OpenAI","DevOps","Home Lab","Cloud Infrastructure","Workflow Automation"]},{"location":"blog/building-ai-powered-infrastructure-monitoring-from-home-lab-to-cloud-production/#hardware-service-requirements","title":"Hardware &amp; Service Requirements","text":"<p>Minimum Configuration:</p> <ul> <li>Hardware: 8GB+ RAM (Pi 5 8GB or standard x86 machine)</li> <li>Storage: 100GB+ available space (containers + metrics retention)</li> <li>Network: Stable internet connection with static IP preferred</li> <li>Software: Docker 20.10+, Docker Compose 2.0+</li> </ul> <p>Service Account Setup for Learning:</p> <ul> <li>Azure OpenAI: Azure subscription with OpenAI access (Visual Studio Enterprise subscription provides excellent experimentation credits)</li> <li>Email Provider: Gmail App Password works perfectly for testing</li> <li>Cloud Account: AWS/Azure free tier for eventual cloud transition</li> <li>Monitoring Tools: All open-source options used in this project</li> </ul> <p>Learning Environment Costs:</p> <ul> <li>Azure OpenAI: Covered by Visual Studio Enterprise subscription credits</li> <li>Infrastructure: Minimal electricity costs for Pi 5 operation</li> <li>Email: \u00a30 (using Gmail App Password)</li> <li>Total out-of-pocket: Essentially \u00a30 for extensive experimentation</li> </ul> <p>Note: Without subscription benefits, AI analysis costs should be carefully monitored as they can accumulate with frequent polling.</p>","tags":["AI","Infrastructure Monitoring","Prometheus","Grafana","n8n","Azure OpenAI","DevOps","Home Lab","Cloud Infrastructure","Workflow Automation"]},{"location":"blog/building-ai-powered-infrastructure-monitoring-from-home-lab-to-cloud-production/#my-learning-setup","title":"My Learning Setup","text":"<p>\ud83d\udda5\ufe0f Home Lab Environment</p> <ul> <li>Primary system: Raspberry Pi 5, 8GB RAM (24/7 learning host)</li> <li>Development approach: Iterative experimentation with immediate feedback</li> <li>Network setup: Standard home lab environment</li> <li>Learning support: Anthropic Claude for debugging and optimization</li> </ul>","tags":["AI","Infrastructure Monitoring","Prometheus","Grafana","n8n","Azure OpenAI","DevOps","Home Lab","Cloud Infrastructure","Workflow Automation"]},{"location":"blog/building-ai-powered-infrastructure-monitoring-from-home-lab-to-cloud-production/#phase-1-foundation-the-monitoring-stack","title":"\ud83c\udfd7\ufe0f Phase 1: Foundation - The Monitoring Stack","text":"<p>Building any intelligent monitoring system starts with having something intelligent to monitor. Enter the classic Prometheus + Grafana combo, containerized for easy deployment and scalability.</p> <p>The foundation phase establishes reliable metric collection before adding intelligence layers. This approach ensures we have clean, consistent data to feed into AI analysis rather than trying to retrofit intelligence into poorly designed monitoring systems.</p>","tags":["AI","Infrastructure Monitoring","Prometheus","Grafana","n8n","Azure OpenAI","DevOps","Home Lab","Cloud Infrastructure","Workflow Automation"]},{"location":"blog/building-ai-powered-infrastructure-monitoring-from-home-lab-to-cloud-production/#learning-checkpoint-before-you-begin","title":"\u2705 Learning Checkpoint: Before You Begin","text":"<p>Before starting this project, verify you can:</p> <ul> <li>[ ] Deploy a multi-container application with Docker Compose</li> <li>[ ] Debug why a container can't reach another container  </li> <li>[ ] Make API calls with authentication headers using curl</li> <li>[ ] Read and understand JSON data structures</li> <li>[ ] Explain what CPU and memory metrics actually mean for system health</li> </ul> <p>Quick Test: Can you deploy a simple web application stack (nginx + database) using Docker Compose and troubleshoot networking issues? If not, spend time with Docker fundamentals first.</p> <p>Common Issue at This Stage: Container networking problems are the most frequent stumbling block. If containers can't communicate, review Docker Compose networking documentation and practice with simple multi-container applications before proceeding.</p>","tags":["AI","Infrastructure Monitoring","Prometheus","Grafana","n8n","Azure OpenAI","DevOps","Home Lab","Cloud Infrastructure","Workflow Automation"]},{"location":"blog/building-ai-powered-infrastructure-monitoring-from-home-lab-to-cloud-production/#docker-compose-infrastructure","title":"\ud83d\udc33 Docker Compose Infrastructure","text":"<p>The entire monitoring stack deploys through a single Docker Compose file. This approach ensures consistent environments from home lab development through cloud production.</p> <pre><code>version: '3.8'\n\nservices:\n  prometheus:\n    image: prom/prometheus:latest\n    container_name: prometheus\n    restart: unless-stopped\n    ports:\n      - \"9090:9090\"\n    volumes:\n      - prometheus_data:/prometheus\n      - ./config/prometheus.yml:/etc/prometheus/prometheus.yml:ro\n    command:\n      - '--config.file=/etc/prometheus/prometheus.yml'\n      - '--storage.tsdb.path=/prometheus'\n      - '--storage.tsdb.retention.time=7d'\n      - '--web.console.libraries=/etc/prometheus/console_libraries'\n      - '--web.console.templates=/etc/prometheus/consoles'\n      - '--web.enable-lifecycle'\n    networks:\n      - monitoring\n\n  grafana:\n    image: grafana/grafana-oss:latest\n    container_name: grafana\n    restart: unless-stopped\n    ports:\n      - \"3000:3000\"\n    environment:\n      - GF_SECURITY_ADMIN_USER=admin\n      - GF_SECURITY_ADMIN_PASSWORD=aimonitoring123\n      - GF_USERS_ALLOW_SIGN_UP=false\n    volumes:\n      - grafana_data:/var/lib/grafana\n      - ./config/grafana/provisioning:/etc/grafana/provisioning:ro\n    networks:\n      - monitoring\n\n  node-exporter:\n    image: prom/node-exporter:latest\n    container_name: node-exporter\n    restart: unless-stopped\n    ports:\n      - \"9100:9100\"\n    volumes:\n      - /proc:/host/proc:ro\n      - /sys:/host/sys:ro\n      - /:/rootfs:ro\n    command:\n      - '--path.procfs=/host/proc'\n      - '--path.rootfs=/rootfs'\n      - '--path.sysfs=/host/sys'\n      - '--collector.filesystem.mount-points-exclude=^/(sys|proc|dev|host|etc)($$|/)'\n    networks:\n      - monitoring\n\n  n8n:\n    image: n8nio/n8n:latest\n    container_name: n8n\n    restart: unless-stopped\n    ports:\n      - \"5678:5678\"\n    environment:\n      - N8N_BASIC_AUTH_ACTIVE=true\n      - N8N_BASIC_AUTH_USER=admin\n      - N8N_BASIC_AUTH_PASSWORD=aimonitoring123\n      - N8N_HOST=0.0.0.0\n      - N8N_PORT=5678\n      - N8N_PROTOCOL=http\n    volumes:\n      - n8n_data:/home/node/.n8n\n    networks:\n      - monitoring\n\nvolumes:\n  prometheus_data:\n  grafana_data:\n  n8n_data:\n\nnetworks:\n  monitoring:\n    driver: bridge\n</code></pre>","tags":["AI","Infrastructure Monitoring","Prometheus","Grafana","n8n","Azure OpenAI","DevOps","Home Lab","Cloud Infrastructure","Workflow Automation"]},{"location":"blog/building-ai-powered-infrastructure-monitoring-from-home-lab-to-cloud-production/#prometheus-configuration","title":"\u2699\ufe0f Prometheus Configuration","text":"<p>The heart of metric collection needs careful configuration to balance comprehensive monitoring with resource efficiency:</p> <pre><code># config/prometheus.yml\nglobal:\n  scrape_interval: 15s\n  evaluation_interval: 15s\n\nrule_files:\n  - \"rules/*.yml\"\n\nscrape_configs:\n  - job_name: 'prometheus'\n    static_configs:\n      - targets: ['localhost:9090']\n\n  - job_name: 'node-exporter'\n    static_configs:\n      - targets: ['node-exporter:9100']\n    scrape_interval: 5s\n    metrics_path: /metrics\n\n  - job_name: 'grafana'\n    static_configs:\n      - targets: ['grafana:3000']\n    scrape_interval: 30s\n\nalerting:\n  alertmanagers:\n    - static_configs:\n        - targets: []\n</code></pre>","tags":["AI","Infrastructure Monitoring","Prometheus","Grafana","n8n","Azure OpenAI","DevOps","Home Lab","Cloud Infrastructure","Workflow Automation"]},{"location":"blog/building-ai-powered-infrastructure-monitoring-from-home-lab-to-cloud-production/#deployment-and-initial-setup","title":"\ud83d\ude80 Deployment and Initial Setup","text":"<p>Launch your monitoring foundation with a single command:</p> <pre><code># Create project structure\nmkdir ai-monitoring-lab &amp;&amp; cd ai-monitoring-lab\nmkdir -p config/grafana/provisioning/{datasources,dashboards}\nmkdir -p data logs\n\n# Deploy the stack\ndocker-compose up -d\n\n# Verify deployment\ndocker-compose ps\ndocker-compose logs -f prometheus\n</code></pre> <ul> <li>Access your new monitoring stack:</li> <li>Prometheus: http://localhost:9090</li> <li>Grafana: http://localhost:3000 (admin/aimonitoring123)</li> <li>Node Exporter: http://localhost:9100/metrics</li> </ul> <p></p> <p>Within minutes, you'll have comprehensive system metrics flowing through Prometheus and visualized in Grafana. But pretty graphs are just the beginning\u2014the real transformation happens when we add AI analysis.</p>","tags":["AI","Infrastructure Monitoring","Prometheus","Grafana","n8n","Azure OpenAI","DevOps","Home Lab","Cloud Infrastructure","Workflow Automation"]},{"location":"blog/building-ai-powered-infrastructure-monitoring-from-home-lab-to-cloud-production/#learning-checkpoint-monitoring-foundation","title":"\u2705 Learning Checkpoint: Monitoring Foundation","text":"<p>Before proceeding to workflow automation, verify you can:</p> <ul> <li>[ ] Access Prometheus at localhost:9090 and see targets as \"UP\"</li> <li>[ ] View system metrics in Grafana dashboards</li> <li>[ ] Write basic PromQL queries (like <code>node_memory_MemAvailable_bytes</code>)</li> <li>[ ] Understand what the metrics represent in business terms</li> <li>[ ] Create a custom Grafana panel showing memory usage as a percentage</li> </ul> <p>Quick Test: Create a dashboard panel that shows \"Memory utilization is healthy/concerning\" based on percentage thresholds. If you can't do this easily, spend more time with Prometheus queries and Grafana visualization.</p> <p>Common Issues at This Stage:</p> <ul> <li>Prometheus targets showing as \"DOWN\" - Usually container networking or firewall issues</li> <li>Grafana showing \"No data\" - Often datasource URL configuration problems  </li> <li>PromQL query errors - Syntax issues with metric names or functions</li> </ul> <p></p>","tags":["AI","Infrastructure Monitoring","Prometheus","Grafana","n8n","Azure OpenAI","DevOps","Home Lab","Cloud Infrastructure","Workflow Automation"]},{"location":"blog/building-ai-powered-infrastructure-monitoring-from-home-lab-to-cloud-production/#bridging-to-intelligence-why-traditional-monitoring-isnt-enough","title":"\ud83d\udd17 Bridging to Intelligence: Why Traditional Monitoring Isn't Enough","text":"<p>Traditional monitoring tells you what happened (CPU is at 85%) but not why it matters or what you should do about it. Most alerts are just noise without context about whether that 85% CPU usage is normal for your workload or a sign of impending system failure.</p> <p>This is where workflow automation and AI analysis bridge the gap between raw metrics and actionable insights.</p> <p>What n8n brings to the solution:</p> <ul> <li>Orchestrates data collection from multiple sources beyond just Prometheus</li> <li>Transforms raw metrics into structured data suitable for AI analysis  </li> <li>Handles error scenarios and fallbacks gracefully without custom application development</li> <li>Enables complex logic through visual workflows rather than scripting</li> <li>Provides integration capabilities with email, chat systems, and other tools</li> </ul> <p>Why AI analysis matters:</p> <ul> <li>Adds context: \"85% CPU usage is normal for this workload during business hours\"</li> <li>Predicts trends: \"Memory usage trending upward, recommend capacity review in 2 weeks\"  </li> <li>Communicates impact: \"System operating efficiently with no immediate business impact\"</li> <li>Reduces noise: Only alert on situations that actually require attention</li> </ul> <p>The combination creates a monitoring system that doesn't just detect problems\u2014it explains them in business terms and recommends specific actions.</p>","tags":["AI","Infrastructure Monitoring","Prometheus","Grafana","n8n","Azure OpenAI","DevOps","Home Lab","Cloud Infrastructure","Workflow Automation"]},{"location":"blog/building-ai-powered-infrastructure-monitoring-from-home-lab-to-cloud-production/#phase-2-n8n-workflow-automation-the-intelligence-orchestrator","title":"\ud83e\udd16 Phase 2: n8n Workflow Automation - The Intelligence Orchestrator","text":"<p>n8n transforms our basic monitoring stack into an intelligent analysis system. Through visual workflow design, we can create complex logic without writing extensive custom code.</p> <p></p>","tags":["AI","Infrastructure Monitoring","Prometheus","Grafana","n8n","Azure OpenAI","DevOps","Home Lab","Cloud Infrastructure","Workflow Automation"]},{"location":"blog/building-ai-powered-infrastructure-monitoring-from-home-lab-to-cloud-production/#data-collection-the-foundation-nodes","title":"\u23f0 Data Collection: The Foundation Nodes","text":"<p>The workflow begins with intelligent data collection that fetches exactly the metrics needed for AI analysis:</p> <pre><code>// Schedule Trigger Node Configuration\n{\n  \"rule\": {\n    \"interval\": [\n      {\n        \"field\": \"cronExpression\",\n        \"value\": \"0 */1 * * *\"  // Every hour\n      }\n    ]\n  }\n}\n</code></pre> <p>Prometheus Query Node (HTTP Request):</p> <pre><code>{\n  \"url\": \"http://prometheus:9090/api/v1/query\",\n  \"method\": \"GET\",\n  \"qs\": {\n    \"query\": \"((node_memory_MemTotal_bytes - node_memory_MemAvailable_bytes) / node_memory_MemTotal_bytes) * 100\"\n  },\n  \"options\": {\n    \"timeout\": 10000\n  }\n}\n</code></pre>","tags":["AI","Infrastructure Monitoring","Prometheus","Grafana","n8n","Azure OpenAI","DevOps","Home Lab","Cloud Infrastructure","Workflow Automation"]},{"location":"blog/building-ai-powered-infrastructure-monitoring-from-home-lab-to-cloud-production/#process-metrics-the-data-transformation","title":"\ud83d\udd04 Process Metrics: The Data Transformation","text":"<p>The magic happens in the data processing node, which transforms Prometheus's JSON responses into clean, AI-friendly data structures. This step is crucial\u2014AI analysis is only as good as the data you feed it.</p> <pre><code>// Process Metrics Node - JavaScript Code\nconst input = $input.first();\n\ntry {\n  // Extract metric values from Prometheus response\n  const memoryData = input.json.data.result[0];\n  const memoryPercent = parseFloat(memoryData.value[1]).toFixed(2);\n\n  // Determine system health status\n  let alertLevel, alertStatus, systemHealth;\n\n  if (memoryPercent &lt; 60) {\n    alertLevel = 'LOW';\n    alertStatus = 'HEALTHY';\n    systemHealth = 'optimal';\n  } else if (memoryPercent &lt; 80) {\n    alertLevel = 'MEDIUM';\n    alertStatus = 'WATCH';\n    systemHealth = 'elevated but manageable';\n  } else {\n    alertLevel = 'HIGH';\n    alertStatus = 'CRITICAL';\n    systemHealth = 'requires immediate attention';\n  }\n\n  // Structure data for AI analysis\n  const processedData = {\n    timestamp: new Date().toISOString(),\n    memory_percent: parseFloat(memoryPercent),\n    alert_level: alertLevel,\n    alert_status: alertStatus,\n    system_health: systemHealth,\n    collection_source: 'prometheus',\n    analysis_ready: true\n  };\n\n  return { json: processedData };\n\n} catch (error) {\n  console.error('Metrics processing failed:', error);\n  return { \n    json: { \n      error: true, \n      message: 'Unable to process metrics data',\n      timestamp: new Date().toISOString()\n    } \n  };\n}\n</code></pre>","tags":["AI","Infrastructure Monitoring","Prometheus","Grafana","n8n","Azure OpenAI","DevOps","Home Lab","Cloud Infrastructure","Workflow Automation"]},{"location":"blog/building-ai-powered-infrastructure-monitoring-from-home-lab-to-cloud-production/#learning-checkpoint-n8n-workflow-fundamentals","title":"\u2705 Learning Checkpoint: n8n Workflow Fundamentals","text":"<p>Before adding AI analysis, verify you can:</p> <ul> <li>[ ] Create a basic n8n workflow that fetches Prometheus data</li> <li>[ ] Process the JSON response and extract specific metrics</li> <li>[ ] Send a test email with the processed data</li> <li>[ ] Handle basic error scenarios (API timeout, malformed response)</li> <li>[ ] Understand the data flow from Prometheus \u2192 n8n \u2192 Email</li> </ul> <p>Quick Test: Build a simple workflow that emails you the current memory percentage every hour. If this seems challenging, spend more time understanding n8n's HTTP request and JavaScript processing nodes.</p> <p>Common Issues at This Stage:</p> <ul> <li>n8n workflow execution failures - Usually authentication or API endpoint problems</li> <li>JavaScript node errors - Often due to missing error handling or incorrect data parsing</li> <li>Email delivery failures - SMTP configuration or authentication issues</li> </ul> <p>Debugging Tip: Use console.log() extensively in JavaScript nodes and check the execution logs for detailed error information.</p>","tags":["AI","Infrastructure Monitoring","Prometheus","Grafana","n8n","Azure OpenAI","DevOps","Home Lab","Cloud Infrastructure","Workflow Automation"]},{"location":"blog/building-ai-powered-infrastructure-monitoring-from-home-lab-to-cloud-production/#phase-3-azure-openai-integration-adding-intelligence","title":"\ud83e\udde0 Phase 3: Azure OpenAI Integration - Adding Intelligence","text":"<p>This is where the system evolves from \"automated alerting\" to \"intelligent analysis.\" Azure OpenAI takes our clean metrics and transforms them into actionable insights that even non-technical stakeholders can understand and act upon.</p> <p>The transition from raw monitoring data to business intelligence happens here\u2014transforming \"Memory usage is 76%\" into \"The system is operating efficiently with healthy resource utilization, indicating well-balanced workloads with adequate capacity for current business requirements.\"</p> <p>Why this transformation matters:</p> <ul> <li>Technical teams get context about whether metrics indicate real problems</li> <li>Business stakeholders understand impact without needing to interpret technical details</li> <li>Decision makers receive actionable recommendations rather than just status updates</li> </ul>","tags":["AI","Infrastructure Monitoring","Prometheus","Grafana","n8n","Azure OpenAI","DevOps","Home Lab","Cloud Infrastructure","Workflow Automation"]},{"location":"blog/building-ai-powered-infrastructure-monitoring-from-home-lab-to-cloud-production/#ai-analysis-configuration","title":"\ud83c\udfaf AI Analysis Configuration","text":"<p>The AI analysis node sends structured data to Azure OpenAI with carefully crafted prompts:</p> <pre><code>// Azure OpenAI Analysis Node - HTTP Request Configuration\n{\n  \"url\": \"https://YOUR_RESOURCE.openai.azure.com/openai/deployments/gpt-4o-mini/chat/completions?api-version=2024-02-15-preview\",\n  \"method\": \"POST\",\n  \"headers\": {\n    \"Content-Type\": \"application/json\",\n    \"api-key\": \"YOUR_AZURE_OPENAI_API_KEY\"\n  },\n  \"body\": {\n    \"messages\": [\n      {\n        \"role\": \"system\",\n        \"content\": \"You are an infrastructure monitoring AI assistant. Analyze system metrics and provide clear, actionable insights for both technical teams and business stakeholders. Focus on business impact, recommendations, and next steps.\"\n      },\n      {\n        \"role\": \"user\", \n        \"content\": \"Analyze this system data: Memory usage: {{$json.memory_percent}}%, Status: {{$json.alert_status}}, Health: {{$json.system_health}}. Provide business context, technical assessment, and specific recommendations.\"\n      }\n    ],\n    \"max_tokens\": 500,\n    \"temperature\": 0.3\n  }\n}\n</code></pre>","tags":["AI","Infrastructure Monitoring","Prometheus","Grafana","n8n","Azure OpenAI","DevOps","Home Lab","Cloud Infrastructure","Workflow Automation"]},{"location":"blog/building-ai-powered-infrastructure-monitoring-from-home-lab-to-cloud-production/#report-generation-and-formatting","title":"\ud83d\udcca Report Generation and Formatting","text":"<p>The AI response gets structured into professional reports suitable for email distribution:</p> <pre><code>// Create Report Node - JavaScript Code\nconst input = $input.first();\n\ntry {\n  // Handle potential API errors\n  if (!input.json.choices || input.json.choices.length === 0) {\n    throw new Error('No AI response received');\n  }\n\n  // Extract AI analysis from Azure OpenAI response\n  const aiAnalysis = input.json.choices[0].message.content;\n  const metricsData = $('Process Metrics').item.json;\n\n  // Calculate token usage for monitoring\n  const tokenUsage = input.json.usage ? input.json.usage.total_tokens : 0;\n\n  const report = {\n    report_id: `AI-MONITOR-${new Date().toISOString().slice(0,10)}-${Date.now()}`,\n    generated_at: new Date().toISOString(),\n    ai_insights: aiAnalysis,\n    system_metrics: {\n      memory_usage: `${metricsData.memory_percent}%`,\n      cpu_usage: `${metricsData.cpu_percent}%`,\n      alert_status: metricsData.alert_status,\n      system_health: metricsData.system_health\n    },\n    usage_tracking: {\n      tokens_used: tokenUsage,\n      model_used: input.json.model || 'gpt-4o-mini'\n    },\n    metadata: {\n      next_check: new Date(Date.now() + 5*60*1000).toISOString(),\n      report_type: metricsData.alert_level === 'LOW' ? 'routine' : 'alert',\n      confidence_score: 0.95 // Based on data quality\n    }\n  };\n\n  return { json: report };\n\n} catch (error) {\n  // Fallback report without AI analysis\n  const metricsData = $('Process Metrics').item.json;\n\n  return { \n    json: { \n      report_id: `AI-MONITOR-ERROR-${Date.now()}`,\n      generated_at: new Date().toISOString(),\n      ai_insights: `System analysis unavailable due to AI service error. Raw metrics: Memory ${metricsData.memory_percent}%, CPU ${metricsData.cpu_percent}%. Status: ${metricsData.alert_status}`,\n      error: true,\n      error_message: error.message\n    } \n  };\n}\n</code></pre> <p>This transforms the AI response into a structured report with tracking information, token usage monitoring, and timestamps\u2014everything needed for understanding resource utilization and system performance.</p>","tags":["AI","Infrastructure Monitoring","Prometheus","Grafana","n8n","Azure OpenAI","DevOps","Home Lab","Cloud Infrastructure","Workflow Automation"]},{"location":"blog/building-ai-powered-infrastructure-monitoring-from-home-lab-to-cloud-production/#learning-checkpoint-ai-integration","title":"\u2705 Learning Checkpoint: AI Integration","text":"<p>Before moving to production thinking, verify you can:</p> <ul> <li>[ ] Successfully call Azure OpenAI API with authentication</li> <li>[ ] Create prompts that generate useful infrastructure analysis</li> <li>[ ] Handle API errors and implement fallback behavior</li> <li>[ ] Monitor token usage to understand resource consumption</li> <li>[ ] Generate reports that are readable by non-technical stakeholders</li> </ul> <p>Quick Test: Can you send sample metrics to Azure OpenAI and get back analysis that your manager could understand and act upon? If the analysis feels generic or unhelpful, focus on prompt engineering improvement.</p> <p>Common Issues at This Stage:</p> <ul> <li>Azure OpenAI authentication failures - API key or endpoint URL problems</li> <li>Rate limiting errors (HTTP 429) - Too frequent API calls or quota exceeded</li> <li>Generic AI responses - Prompts lack specificity or context</li> <li>Token usage escalation - Inefficient prompts or too frequent analysis</li> </ul> <p></p>","tags":["AI","Infrastructure Monitoring","Prometheus","Grafana","n8n","Azure OpenAI","DevOps","Home Lab","Cloud Infrastructure","Workflow Automation"]},{"location":"blog/building-ai-powered-infrastructure-monitoring-from-home-lab-to-cloud-production/#phase-4-professional-email-reporting","title":"\ud83d\udce7 Phase 4: Professional Email Reporting","text":"<p>The final component transforms AI insights into professional stakeholder communications that drive business decisions.</p>","tags":["AI","Infrastructure Monitoring","Prometheus","Grafana","n8n","Azure OpenAI","DevOps","Home Lab","Cloud Infrastructure","Workflow Automation"]},{"location":"blog/building-ai-powered-infrastructure-monitoring-from-home-lab-to-cloud-production/#html-email-template-design","title":"\ud83c\udfa8 HTML Email Template Design","text":"<pre><code>&lt;!-- Email Template Node - HTML Content --&gt;\n&lt;!DOCTYPE html&gt;\n&lt;html&gt;\n&lt;head&gt;\n&lt;meta charset=\"UTF-8\"&gt;\n&lt;meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\"&gt;\n&lt;title&gt;Infrastructure Intelligence Report&lt;/title&gt;\n&lt;style&gt;\nbody { font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif; margin: 0; padding: 20px; background-color: #f5f5f5; }\n.container { max-width: 800px; margin: 0 auto; background-color: white; border-radius: 8px; box-shadow: 0 2px 10px rgba(0,0,0,0.1); }\n.header { background: linear-gradient(135deg, #667eea 0%, #764ba2 100%); color: white; padding: 30px; border-radius: 8px 8px 0 0; }\n.content { padding: 30px; }\n.metric-card { background-color: #f8f9fa; border-left: 4px solid #007bff; padding: 15px; margin: 15px 0; border-radius: 4px; }\n.ai-analysis { background-color: #e8f4fd; border: 1px solid #bee5eb; padding: 20px; border-radius: 6px; margin: 20px 0; }\n.footer { background-color: #f8f9fa; padding: 20px; text-align: center; border-radius: 0 0 8px 8px; font-size: 12px; color: #6c757d; }\n&lt;/style&gt;\n&lt;/head&gt;\n&lt;body&gt;\n&lt;div class=\"container\"&gt;\n&lt;div class=\"header\"&gt;\n&lt;h1&gt;\ud83e\udd16 AI Infrastructure Intelligence Report&lt;/h1&gt;\n&lt;p&gt;Automated analysis and recommendations \u2022 {{ $json.generated_at }}&lt;/p&gt;\n&lt;/div&gt;\n\n&lt;div class=\"content\"&gt;\n&lt;h2&gt;\ud83e\udde0 AI-Powered Analysis&lt;/h2&gt;\n&lt;div class=\"ai-analysis\"&gt;\n&lt;strong&gt;System Intelligence Summary:&lt;/strong&gt;&lt;br&gt;\n{{ $json.ai_insights }}\n&lt;/div&gt;\n\n&lt;h2&gt;\ud83d\udcca Current System Metrics&lt;/h2&gt;\n&lt;div class=\"metric-card\"&gt;\n&lt;strong&gt;Memory Utilization:&lt;/strong&gt; {{ $json.system_metrics.memory_usage }}&lt;br&gt;\n&lt;strong&gt;System Status:&lt;/strong&gt; {{ $json.system_metrics.alert_status }}&lt;br&gt;\n&lt;strong&gt;Health Assessment:&lt;/strong&gt; {{ $json.system_metrics.system_health }}\n&lt;/div&gt;\n\n&lt;h3&gt;\ud83d\udcb0 Token Usage Analysis&lt;/h3&gt;\n&lt;div style=\"background-color: #d1ecf1; padding: 10px; border-radius: 5px;\"&gt;\n&lt;ul&gt;\n&lt;li&gt;&lt;strong&gt;Tokens Used This Report:&lt;/strong&gt; {{ $json.usage_tracking.tokens_used }}&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;AI Model:&lt;/strong&gt; {{ $json.usage_tracking.model_used }}&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Analysis Frequency:&lt;/strong&gt; Configurable based on monitoring requirements&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Note:&lt;/strong&gt; Token usage varies based on metric complexity and prompt length&lt;/li&gt;\n&lt;/ul&gt;\n&lt;/div&gt;\n\n&lt;h3&gt;\u23f0 Report Metadata&lt;/h3&gt;\n&lt;ul&gt;\n&lt;li&gt;&lt;strong&gt;Report ID:&lt;/strong&gt; {{ $json.report_id }}&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Generated:&lt;/strong&gt; {{ $json.generated_at }}&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Next Check:&lt;/strong&gt; {{ $json.metadata.next_check }}&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Report Type:&lt;/strong&gt; {{ $json.metadata.report_type }}&lt;/li&gt;\n&lt;/ul&gt;\n&lt;/div&gt;\n\n&lt;div class=\"footer\"&gt;\n&lt;p&gt;Generated by AI-Powered Infrastructure Monitoring System&lt;br&gt;\nHome Lab Implementation \u2022 Learning Project for Cloud Production Application&lt;/p&gt;\n&lt;/div&gt;\n&lt;/div&gt;\n&lt;/body&gt;\n&lt;/html&gt;\n</code></pre>","tags":["AI","Infrastructure Monitoring","Prometheus","Grafana","n8n","Azure OpenAI","DevOps","Home Lab","Cloud Infrastructure","Workflow Automation"]},{"location":"blog/building-ai-powered-infrastructure-monitoring-from-home-lab-to-cloud-production/#email-delivery-configuration","title":"\ud83d\udcee Email Delivery Configuration","text":"<pre><code>// Email Delivery Node - SMTP Configuration\n{\n  \"host\": \"smtp.gmail.com\",\n  \"port\": 587,\n  \"secure\": false,\n  \"auth\": {\n    \"user\": \"your-email@gmail.com\",\n    \"pass\": \"your-app-password\"\n  },\n  \"from\": \"AI Infrastructure Monitor &lt;your-email@gmail.com&gt;\",\n  \"to\": \"stakeholders@company.com\",\n  \"subject\": \"\ud83e\udd16 Infrastructure Intelligence Report - {{ $json.system_metrics.alert_status }}\",\n  \"html\": \"{{ $('HTML Template').item.json.html_content }}\"\n}\n</code></pre> <p>The difference between this and traditional monitoring emails is remarkable\u2014instead of \"CPU is at 85%,\" stakeholders get \"The system is operating within optimal parameters with excellent resource efficiency, suggesting current workloads are well-balanced and no immediate action is required.\"</p> <p></p>","tags":["AI","Infrastructure Monitoring","Prometheus","Grafana","n8n","Azure OpenAI","DevOps","Home Lab","Cloud Infrastructure","Workflow Automation"]},{"location":"blog/building-ai-powered-infrastructure-monitoring-from-home-lab-to-cloud-production/#my-learning-journey-what-actually-happened","title":"\ud83c\udfaf My Learning Journey: What Actually Happened","text":"<p>Understanding the real progression of this project helps set realistic expectations for your own learning experience.</p> <p>Week 1: Foundation Building I started by getting the basic monitoring stack working. The Docker Compose approach made deployment straightforward, but understanding why each component was needed took time. I spent several days just exploring Prometheus queries and Grafana dashboards\u2014this foundational understanding proved essential for later AI integration.</p> <p>Week 2: Workflow Automation Discovery Adding n8n was where things got interesting. The visual workflow builder made complex logic manageable, but I quickly learned that proper error handling isn't optional\u2014it's essential. Using Anthropic Claude to debug JavaScript issues in workflows saved hours of frustration and accelerated my learning significantly.</p> <p>Week 3: AI Integration Breakthrough This is where the real magic happened. Seeing raw metrics transformed into business-relevant insights was genuinely exciting. The key insight: prompt engineering for infrastructure is fundamentally different from general AI use\u2014specificity about your environment and context matters enormously.</p> <p>Week 4: Production Thinking The final week focused on understanding how these patterns would apply to real cloud infrastructure. This home lab approach meant I could experiment safely and make mistakes without impact, while building knowledge directly applicable to production environments.</p>","tags":["AI","Infrastructure Monitoring","Prometheus","Grafana","n8n","Azure OpenAI","DevOps","Home Lab","Cloud Infrastructure","Workflow Automation"]},{"location":"blog/building-ai-powered-infrastructure-monitoring-from-home-lab-to-cloud-production/#home-lab-performance-observations","title":"\ud83d\udcca Home Lab Performance Observations","text":"<p>After running the system continuously in my home lab environment:</p> <p>System Reliability:</p> <ul> <li>Uptime: 99.2% (brief restarts for updates and one power outage)</li> <li>Data collection reliability: 99.8% (missed 3 collection cycles due to network issues)</li> <li>AI analysis success rate: 97.1% (some Azure throttling during peak hours)</li> <li>Email delivery: 100% (SMTP proved reliable for testing purposes)</li> </ul> <p>Resource Utilization on Pi 5:</p> <ul> <li>Memory usage: 68% peak, 45% average (acceptable for home lab testing)</li> <li>CPU usage: 15% peak, 8% average (monitoring has minimal impact)</li> <li>Storage growth: 120MB/week (Prometheus data with 7-day retention and compression)</li> </ul> <p>Response Times in Home Lab:</p> <ul> <li>Metric collection: 2.3 seconds average</li> <li>AI analysis response: 8.7 seconds average (Azure OpenAI)</li> <li>End-to-end report generation: 12.4 seconds</li> <li>Email delivery: 3.1 seconds average</li> </ul> <p>Token Usage Observations:</p> <ul> <li>Average tokens per analysis: ~507 tokens</li> <li>Analysis frequency: Hourly during active testing</li> <li>Model efficiency: GPT-4o-mini provided excellent analysis quality for infrastructure metrics</li> <li>Optimization: Prompt refinement reduced token usage by ~20% over time</li> </ul> <p>Note: These are home lab observations for learning purposes. Production cloud deployments would have different performance characteristics and scaling requirements.</p>","tags":["AI","Infrastructure Monitoring","Prometheus","Grafana","n8n","Azure OpenAI","DevOps","Home Lab","Cloud Infrastructure","Workflow Automation"]},{"location":"blog/building-ai-powered-infrastructure-monitoring-from-home-lab-to-cloud-production/#challenges-and-learning-points","title":"\u26a0\ufe0f Challenges and Learning Points","text":"<p>Prometheus query optimization is critical: Inefficient queries can overwhelm the Pi 5, especially during high-cardinality metric collection. Always validate queries against realistic datasets and implement appropriate rate limiting. Complex aggregation queries should be pre-computed where possible.</p> <p>n8n workflow complexity escalates quickly: What starts as simple data collection becomes complex orchestration with error handling, retries, and fallbacks. Start simple and add features incrementally. I found that using Anthropic Claude to help debug workflow issues significantly accelerated problem resolution.</p> <p>AI prompt engineering requires iteration: Generic prompts produce generic insights that add little value over traditional alerting. Tailoring prompts for specific infrastructure contexts, stakeholder audiences, and business objectives dramatically improves output quality and relevance.</p> <p>Network reliability affects everything: Since the system depends on multiple external APIs (Azure OpenAI, SMTP), network connectivity issues cascade through the entire workflow. Implementing proper timeout handling and offline modes is essential for production reliability.</p> <p>Token usage visibility drives optimization: Monitoring token consumption in real-time helped optimize prompt design and understand the resource implications of different analysis frequencies. This transparency enabled informed decisions about monitoring granularity versus AI resource usage.</p>","tags":["AI","Infrastructure Monitoring","Prometheus","Grafana","n8n","Azure OpenAI","DevOps","Home Lab","Cloud Infrastructure","Workflow Automation"]},{"location":"blog/building-ai-powered-infrastructure-monitoring-from-home-lab-to-cloud-production/#common-issues-and-solutions","title":"\ud83d\udee0\ufe0f Common Issues and Solutions","text":"<p>Based on practical experience running this system, here are the most frequent challenges and their resolutions:</p>","tags":["AI","Infrastructure Monitoring","Prometheus","Grafana","n8n","Azure OpenAI","DevOps","Home Lab","Cloud Infrastructure","Workflow Automation"]},{"location":"blog/building-ai-powered-infrastructure-monitoring-from-home-lab-to-cloud-production/#prometheus-connection-issues","title":"Prometheus Connection Issues","text":"<p>Symptom: Targets showing as \"DOWN\" in Prometheus interface</p> <pre><code># Check Prometheus targets status\ncurl http://localhost:9090/api/v1/targets\n\n# Verify container networking\ndocker network inspect ai-monitoring-lab_monitoring\n\n# Check if services can reach each other\ndocker exec prometheus ping node-exporter\n</code></pre>","tags":["AI","Infrastructure Monitoring","Prometheus","Grafana","n8n","Azure OpenAI","DevOps","Home Lab","Cloud Infrastructure","Workflow Automation"]},{"location":"blog/building-ai-powered-infrastructure-monitoring-from-home-lab-to-cloud-production/#n8n-workflow-execution-failures","title":"n8n Workflow Execution Failures","text":"<p>Symptom: HTTP 500 errors in workflow execution logs</p> <pre><code>// Add comprehensive error handling to JavaScript nodes\ntry {\n  const result = processMetrics(input);\n  return { json: result };\n} catch (error) {\n  console.error('Processing failed:', error);\n  return { \n    json: { \n      error: true, \n      message: error.message,\n      timestamp: new Date().toISOString()\n    } \n  };\n}\n</code></pre>","tags":["AI","Infrastructure Monitoring","Prometheus","Grafana","n8n","Azure OpenAI","DevOps","Home Lab","Cloud Infrastructure","Workflow Automation"]},{"location":"blog/building-ai-powered-infrastructure-monitoring-from-home-lab-to-cloud-production/#azure-openai-rate-limiting","title":"Azure OpenAI Rate Limiting","text":"<p>Symptom: Sporadic HTTP 429 errors</p> <pre><code>// Implement exponential backoff for API calls\nasync function retryWithBackoff(apiCall, maxRetries = 3) {\n  for (let attempt = 1; attempt &lt;= maxRetries; attempt++) {\n    try {\n      return await apiCall();\n    } catch (error) {\n      if (error.status === 429 || error.status &gt;= 500) {\n        const backoffDelay = Math.min(1000 * Math.pow(2, attempt), 30000);\n        await new Promise(resolve =&gt; setTimeout(resolve, backoffDelay));\n      } else {\n        throw error;\n      }\n    }\n  }\n  throw new Error(`API call failed after ${maxRetries} attempts`);\n}\n</code></pre>","tags":["AI","Infrastructure Monitoring","Prometheus","Grafana","n8n","Azure OpenAI","DevOps","Home Lab","Cloud Infrastructure","Workflow Automation"]},{"location":"blog/building-ai-powered-infrastructure-monitoring-from-home-lab-to-cloud-production/#memory-management-on-raspberry-pi","title":"Memory Management on Raspberry Pi","text":"<p>Symptom: System becomes unresponsive under load</p> <pre><code># Add memory limits to docker-compose.yml\nservices:\n  prometheus:\n    mem_limit: 1g\n    mem_reservation: 512m\n  grafana:\n    mem_limit: 512m\n    mem_reservation: 256m\n</code></pre>","tags":["AI","Infrastructure Monitoring","Prometheus","Grafana","n8n","Azure OpenAI","DevOps","Home Lab","Cloud Infrastructure","Workflow Automation"]},{"location":"blog/building-ai-powered-infrastructure-monitoring-from-home-lab-to-cloud-production/#whats-next-after-this-project","title":"\ud83c\udfaf What's Next After This Project?","text":"<p>Having successfully built an AI-powered monitoring system in your home lab, you've developed transferable skills for larger infrastructure projects:</p> <p>Immediate Next Steps:</p> <ul> <li>Apply these patterns to cloud infrastructure (AWS EC2, Azure VMs)</li> <li>Expand monitoring to cover application metrics, not just system metrics</li> <li>Explore other AI models and prompt engineering techniques</li> </ul> <p>Future Learning Projects:</p> <ul> <li>Security Event Analysis: Use similar AI integration patterns for log analysis</li> <li>Cost Optimization: Apply AI analysis to cloud billing and usage data</li> <li>Capacity Planning: Extend monitoring for predictive resource planning</li> </ul> <p>Skills You Can Now Confidently Apply:</p> <ul> <li>Integrating AI services with traditional monitoring tools</li> <li>Creating business-relevant reports from technical metrics</li> <li>Building automated workflows for infrastructure management</li> <li>Designing scalable monitoring architectures</li> </ul>","tags":["AI","Infrastructure Monitoring","Prometheus","Grafana","n8n","Azure OpenAI","DevOps","Home Lab","Cloud Infrastructure","Workflow Automation"]},{"location":"blog/building-ai-powered-infrastructure-monitoring-from-home-lab-to-cloud-production/#additional-resources","title":"\ud83d\udcda Additional Resources","text":"","tags":["AI","Infrastructure Monitoring","Prometheus","Grafana","n8n","Azure OpenAI","DevOps","Home Lab","Cloud Infrastructure","Workflow Automation"]},{"location":"blog/building-ai-powered-infrastructure-monitoring-from-home-lab-to-cloud-production/#official-documentation","title":"\ud83d\udee0\ufe0f Official Documentation","text":"<p>Container and Orchestration:</p> <ul> <li>Docker Compose File Reference \u2014 Complete YAML schema validation</li> <li>Docker Networking Guide \u2014 Container communication and troubleshooting</li> </ul> <p>Monitoring and Observability:</p> <ul> <li>Prometheus Configuration \u2014 Official configuration reference</li> <li>Node Exporter Metrics \u2014 Available system metrics</li> <li>Grafana Provisioning \u2014 Automated setup documentation</li> </ul> <p>Workflow Automation:</p> <ul> <li>n8n Node Documentation \u2014 Complete node reference</li> <li>n8n Workflow Examples \u2014 Official workflow patterns</li> </ul> <p>AI Integration:</p> <ul> <li>Azure OpenAI REST API \u2014 Complete API specification</li> <li>Azure OpenAI Quickstart \u2014 Getting started guide</li> </ul>","tags":["AI","Infrastructure Monitoring","Prometheus","Grafana","n8n","Azure OpenAI","DevOps","Home Lab","Cloud Infrastructure","Workflow Automation"]},{"location":"blog/building-ai-powered-infrastructure-monitoring-from-home-lab-to-cloud-production/#learning-resources","title":"\ud83c\udf93 Learning Resources","text":"<p>Docker and Containerization:</p> <ul> <li>Docker Getting Started \u2014 Official tutorial</li> <li>Play with Docker \u2014 Interactive learning environment</li> </ul> <p>Monitoring Fundamentals:</p> <ul> <li>Prometheus Getting Started \u2014 Official introduction</li> <li>Grafana Fundamentals \u2014 Hands-on tutorial</li> </ul> <p>AI and Prompt Engineering:</p> <ul> <li>Azure OpenAI Learning Path \u2014 Microsoft Learn modules</li> <li>Prompt Engineering Guide \u2014 Best practices and examples</li> </ul>","tags":["AI","Infrastructure Monitoring","Prometheus","Grafana","n8n","Azure OpenAI","DevOps","Home Lab","Cloud Infrastructure","Workflow Automation"]},{"location":"blog/building-ai-powered-infrastructure-monitoring-from-home-lab-to-cloud-production/#conclusion-ai-enhanced-monitoring-success","title":"\ud83c\udfaf Conclusion: AI-Enhanced Monitoring Success","text":"<p>This home lab project successfully demonstrates how AI can transform traditional infrastructure monitoring from simple threshold alerts into intelligent, actionable insights. The structured approach\u2014from Docker fundamentals through AI integration\u2014provides a practical learning path for developing production-ready skills in a cost-effective environment.</p> <p>Key Achievements:</p> <ul> <li>Technical Integration: Successfully combined Prometheus, Grafana, n8n, and Azure OpenAI into a cohesive monitoring system</li> <li>AI Prompt Engineering: Developed context-specific prompts that transform raw metrics into business-relevant insights</li> <li>Professional Communication: Created stakeholder-ready reports that bridge technical data and business impact</li> <li>Cost-Conscious Development: Leveraged subscription benefits for extensive AI experimentation</li> </ul> <p>Most Valuable Insights:</p> <ul> <li>AI analysis quality depends on data structure and prompt engineering - generic prompts produce generic insights</li> <li>Visual workflow tools dramatically reduce development complexity while maintaining flexibility</li> <li>Home lab experimentation provides a safe environment for expensive AI service optimization</li> <li>Business context and stakeholder communication are as important as technical implementation</li> </ul> <p>Professional Development Impact: The patterns learned in this project\u2014intelligent data collection, contextual analysis, and automated communication\u2014scale directly to enterprise monitoring requirements. For infrastructure professionals exploring AI integration, this home lab approach provides hands-on experience with real tools and challenges that translate to immediate career value.</p> <p>The investment in learning these integration patterns delivers improved monitoring effectiveness, reduced alert noise, and enhanced stakeholder communication\u2014essential skills for modern infrastructure teams working with AI-augmented systems.</p> <p>Share on  Share on </p>","tags":["AI","Infrastructure Monitoring","Prometheus","Grafana","n8n","Azure OpenAI","DevOps","Home Lab","Cloud Infrastructure","Workflow Automation"]},{"location":"blog/monitoring-an-iis-based-web-farm-with-azure-application-insights/","title":"\ud83d\udcca Monitoring an IIS-Based Web Farm with Azure Application Insights","text":"<p>In this guide, you'll learn how to:</p> <p>\u2705 Set up Application Insights on an IIS-based web farm. \u2705 Configure Log Analytics, Data Collection Rules, and Data Collection Endpoints. \u2705 Use PowerShell to install the Application Insights agent. \u2705 Monitor live metrics, failures, performance, and logs in real-time.  </p> <p>By the end, you'll have a fully monitored IIS-based web farm using Azure! \ud83c\udfaf</p>","tags":["azure","monitoring","iis","appinsights"]},{"location":"blog/monitoring-an-iis-based-web-farm-with-azure-application-insights/#step-1-enabling-application-insights-on-iis-servers","title":"\ud83c\udfd7\ufe0f Step 1: Enabling Application Insights on IIS Servers","text":"<p>To effectively monitor your IIS-based application, you need to configure Azure Application Insights and ensure all required components are installed on your Azure VMs.</p>","tags":["azure","monitoring","iis","appinsights"]},{"location":"blog/monitoring-an-iis-based-web-farm-with-azure-application-insights/#prerequisites","title":"\ud83d\udee0\ufe0f Prerequisites","text":"<p>Before proceeding, ensure you have:</p> <ul> <li>An active Azure Subscription with permissions to create and manage resources.</li> <li>A Log Analytics Workspace (LAW) to store collected telemetry data.</li> <li>Azure Monitor Agent (AMA) installed on your IIS VMs.</li> <li>Necessary permissions to create Data Collection Rules (DCRs) and Data Collection Endpoints (DCEs).</li> </ul>","tags":["azure","monitoring","iis","appinsights"]},{"location":"blog/monitoring-an-iis-based-web-farm-with-azure-application-insights/#create-a-log-analytics-workspace","title":"Create a Log Analytics Workspace","text":"<ol> <li>Go to Azure Portal \u2192 Search for \"Log Analytics Workspaces\" \u2192 Create.</li> <li>Provide the following details:</li> <li>Subscription: Select your Azure subscription.</li> <li>Resource Group: Choose or create a new one.</li> <li>Name: Enter a unique name (e.g., <code>log-corpapp-prod-uksouth</code>).</li> <li>Region: Same as your IIS VMs.</li> <li>Click \"Review + Create\" and deploy the workspace.</li> </ol> <p>\ud83d\udd17 Microsoft Learn: Log Analytics Workspace</p>","tags":["azure","monitoring","iis","appinsights"]},{"location":"blog/monitoring-an-iis-based-web-farm-with-azure-application-insights/#create-a-data-collection-endpoint-dce","title":"Create a Data Collection Endpoint (DCE)","text":"<ol> <li>Navigate to Monitor \u2192 Data Collection Endpoints.</li> <li>Click \"+ Create\" and provide:</li> <li>Name: e.g., <code>dce-corpapp-prod-uksouth</code>.</li> <li>Subscription &amp; Resource Group: Same as your IIS VMs.</li> <li>Region: Same as Log Analytics Workspace.</li> <li>Review &amp; create the endpoint.</li> </ol> <p>\ud83d\udd17 Microsoft Learn: Data Collection Endpoints</p>","tags":["azure","monitoring","iis","appinsights"]},{"location":"blog/monitoring-an-iis-based-web-farm-with-azure-application-insights/#create-a-data-collection-rule-dcr","title":"Create a Data Collection Rule (DCR)","text":"<ol> <li>Go to Monitor \u2192 Data Collection Rules \u2192 + Create.</li> <li>Configure:</li> <li>Name: <code>dcr-corpapp-iis-prod-uksouth</code></li> <li>Subscription &amp; Resource Group: Same as above.</li> <li>Region: Same as DCE &amp; LAW.</li> <li>Define data sources:</li> <li>Windows Event Logs: Add <code>System</code>, <code>Application</code>, etc.</li> <li>Log Levels: Select relevant levels (Error, Warning, Information).</li> <li>Set Destination:</li> <li>Choose \"Log Analytics Workspace\" \u2192 Select the previously created workspace.</li> <li>Associate with IIS VMs (<code>WEB01</code> - <code>WEB05</code>).</li> <li>Review &amp; Create the rule.</li> </ol> <p>\ud83d\udd17 Microsoft Learn: Data Collection Rules</p>","tags":["azure","monitoring","iis","appinsights"]},{"location":"blog/monitoring-an-iis-based-web-farm-with-azure-application-insights/#install-the-azure-monitor-agent-ama","title":"Install the Azure Monitor Agent (AMA)","text":"<ol> <li>Navigate to each IIS VM.</li> <li>Under \"Monitoring\", select \"Extensions\".</li> <li>Click \"+ Add\" \u2192 AzureMonitorWindowsAgent \u2192 Install.</li> <li>Repeat for all IIS VMs.</li> </ol> <p>\ud83d\udd17 Microsoft Learn: Azure Monitor Agent</p>","tags":["azure","monitoring","iis","appinsights"]},{"location":"blog/monitoring-an-iis-based-web-farm-with-azure-application-insights/#enable-application-insights","title":"Enable Application Insights","text":"<ol> <li>Navigate to Azure Portal \u2192 Search for \"Application Insights\".</li> <li>Click \"+ Create\" \u2192 Provide:</li> <li>Subscription &amp; Resource Group: Same as VMs.</li> <li>Name: <code>insights-corpapp-prod-uksouth-001</code>.</li> <li>Region: Same as your IIS VMs.</li> <li>Application Type: ASP.NET Web Application.</li> <li>Click \"Review + Create\" and deploy.</li> </ol> <p>\ud83d\udd17 Microsoft Learn: Enable Application Insights</p>","tags":["azure","monitoring","iis","appinsights"]},{"location":"blog/monitoring-an-iis-based-web-farm-with-azure-application-insights/#install-the-application-insights-agent","title":"Install the Application Insights Agent","text":"<p>Use the following PowerShell script to install the agent on all of the IIS servers:</p> <pre><code># Install the Application Insights Agent\n$instrumentationKey = \"YOUR-INSTRUMENTATION-KEY\"\nInstall-PackageProvider -Name NuGet -Force\nInstall-Module -Name ApplicationInsightsWebTracking -Force\nEnable-ApplicationInsightsMonitoring -InstrumentationKey $instrumentationKey\nRestart-Service W3SVC\n</code></pre>","tags":["azure","monitoring","iis","appinsights"]},{"location":"blog/monitoring-an-iis-based-web-farm-with-azure-application-insights/#step-2-using-application-insights-for-monitoring","title":"\ud83d\udcca Step 2: Using Application Insights for Monitoring","text":"<p>With everything set up, it's time to monitor and analyze application performance! \ud83d\udd0d</p>","tags":["azure","monitoring","iis","appinsights"]},{"location":"blog/monitoring-an-iis-based-web-farm-with-azure-application-insights/#overview-dashboard","title":"\ud83d\udccc Overview Dashboard","text":"<ul> <li>Displays high-level health metrics, failed requests, and response times. \ud83d\udcf8 </li> </ul>","tags":["azure","monitoring","iis","appinsights"]},{"location":"blog/monitoring-an-iis-based-web-farm-with-azure-application-insights/#application-map","title":"\ud83d\udccc Application Map","text":"<ul> <li>Shows dependencies and interactions between components. \ud83d\udcf8 </li> </ul>","tags":["azure","monitoring","iis","appinsights"]},{"location":"blog/monitoring-an-iis-based-web-farm-with-azure-application-insights/#live-metrics","title":"\ud83d\udccc Live Metrics","text":"<ul> <li>Monitor real-time requests, server performance, and failures. \ud83d\udcf8 </li> </ul>","tags":["azure","monitoring","iis","appinsights"]},{"location":"blog/monitoring-an-iis-based-web-farm-with-azure-application-insights/#failures-exceptions","title":"\ud83d\udccc Failures &amp; Exceptions","text":"<ul> <li>Identify and diagnose failed requests &amp; top exceptions. \ud83d\udcf8 </li> </ul>","tags":["azure","monitoring","iis","appinsights"]},{"location":"blog/monitoring-an-iis-based-web-farm-with-azure-application-insights/#performance-monitoring","title":"\ud83d\udccc Performance Monitoring","text":"<ul> <li>Analyze response times, dependencies &amp; bottlenecks. \ud83d\udcf8 </li> </ul>","tags":["azure","monitoring","iis","appinsights"]},{"location":"blog/monitoring-an-iis-based-web-farm-with-azure-application-insights/#logs-queries","title":"\ud83d\udccc Logs &amp; Queries","text":"<ul> <li>Run Kusto Query Language (KQL) queries for deep insights.</li> </ul> <p>Example query to find failed requests:</p> <pre><code>requests\n| where timestamp &gt; ago(24h)\n| where success == false\n| project timestamp, name, resultCode, url\n| order by timestamp desc\n</code></pre> <p>\ud83d\udcf8 </p>","tags":["azure","monitoring","iis","appinsights"]},{"location":"blog/monitoring-an-iis-based-web-farm-with-azure-application-insights/#next-steps","title":"\u2705 Next Steps","text":"<p>\ud83c\udfaf Continue monitoring logs &amp; alerts for trends. \ud83c\udfaf Optimize Application Insights sampling to reduce telemetry costs. \ud83c\udfaf Automate reporting for key performance metrics.  </p> <p>By following this guide, you'll have a robust, real-time monitoring setup for your IIS web farm, ensuring optimal performance and quick issue resolution! \ud83d\ude80</p> <p>Share on  Share on </p>","tags":["azure","monitoring","iis","appinsights"]},{"location":"blog/azure-bcdr-review--turning-inherited-cloud-infrastructure-into-a-resilient-recovery-strategy/","title":"\u2699\ufe0f Azure BCDR Review \u2013 Turning Inherited Cloud Infrastructure into a Resilient Recovery Strategy","text":"<p>When we inherited our Azure estate from a previous MSP, some of the key technical components were already in place \u2014 ASR was configured for a number of workloads, and backups had been partially implemented across the environment.</p> <p>What we didn\u2019t inherit was a documented or validated BCDR strategy.</p> <p>There were no formal recovery plans defined in ASR, no clear failover sequences, and no evidence that a regional outage scenario had ever been modelled or tested. The building blocks were there \u2014 but there was no framework tying them together into a usable or supportable recovery posture.</p> <p>This post shares how I approached the challenge of assessing and strengthening our Azure BCDR readiness. It's not about starting from scratch \u2014 it's about applying structure, logic, and realism to an environment that had the right intentions but lacked operational clarity.</p> <p>Whether you're stepping into a similar setup or planning your first formal DR review, I hope this provides a practical and relatable blueprint.</p>","tags":["azure","bcdr","site recovery","disaster recovery","cloud strategy"]},{"location":"blog/azure-bcdr-review--turning-inherited-cloud-infrastructure-into-a-resilient-recovery-strategy/#where-we-started-technical-foundations-operational-gaps","title":"\ud83c\udfaf Where We Started: Technical Foundations, Operational Gaps","text":"<p>We weren\u2019t starting from zero \u2014 but we weren\u2019t in a position to confidently recover the environment either.</p> <p>What we found:</p> <ul> <li>\ud83d\udfe2 ASR replication was partially implemented  </li> <li>\ud83d\udfe1 VM backups were present but inconsistent  </li> <li>\u274c No Recovery Plans existed in ASR  </li> <li>\u274c No test failovers had ever been performed  </li> <li>\u26a0\ufe0f No documented RTO/RPO targets  </li> <li>\u2753 DNS and Private Endpoints weren\u2019t accounted for in DR  </li> <li>\ud83d\udd12 Networking had not been reviewed for failover scenarios  </li> <li>\ud83d\udeab No capacity reservations had been made</li> </ul> <p>This review was the first step in understanding whether our DR setup could work in practice \u2014 not just in theory.</p>","tags":["azure","bcdr","site recovery","disaster recovery","cloud strategy"]},{"location":"blog/azure-bcdr-review--turning-inherited-cloud-infrastructure-into-a-resilient-recovery-strategy/#1-workload-protection-whats-covered-whats-not","title":"\ud83d\udee1\ufe0f 1\ufe0f\u20e3 Workload Protection: What\u2019s Covered, What\u2019s Not","text":"<p>Some workloads were actively replicated via ASR. Others were only backed up. Some had both, a few had neither. There was no documented logic to explain why.</p> <p>Workload protection appeared to be driven by convenience or historical context \u2014 not by business impact or recovery priority.</p> <p>What we needed was a structured tiering model:</p> <ul> <li>\ud83e\udde0 Which workloads are mission-critical?</li> <li>\u23f1\ufe0f Which ones can tolerate extended recovery times?</li> <li>\ud83d\udcca What RTOs and RPOs are actually achievable?</li> </ul> <p>This became the foundation for everything else.</p>","tags":["azure","bcdr","site recovery","disaster recovery","cloud strategy"]},{"location":"blog/azure-bcdr-review--turning-inherited-cloud-infrastructure-into-a-resilient-recovery-strategy/#2-the-missing-operational-layer","title":"\ud83e\udde9 2\ufe0f\u20e3 The Missing Operational Layer","text":"<p>We had technical coverage \u2014 but no operational recovery strategy.</p> <p>There were no Recovery Plans in ASR. No sequencing, no post-failover validation, and no scripts or automation.</p> <p>In the absence of structure, recovery would be entirely manual \u2014 relying on individual knowledge, assumptions, and good luck during a critical event.</p> <p>Codifying dependencies, failover order, and recovery steps became a priority.</p>","tags":["azure","bcdr","site recovery","disaster recovery","cloud strategy"]},{"location":"blog/azure-bcdr-review--turning-inherited-cloud-infrastructure-into-a-resilient-recovery-strategy/#3-dns-identity-and-private-endpoint-blind-spots","title":"\ud83c\udf10 3\ufe0f\u20e3 DNS, Identity and Private Endpoint Blind Spots","text":"<p>DNS and authentication are easy to overlook \u2014 until they break.</p> <p>Our name resolution relied on internal DNS via AD-integrated zones, with no failover logic for internal record switching. No private DNS zones were in place.</p> <p>Private Endpoints were widely used, but all existed in the primary region. In a DR scenario, they would become unreachable.</p> <p>Identity was regionally redundant, but untested and not AZ-aware.</p> <p>We needed to promote DNS, identity, and PE routing to first-class DR concerns.</p>","tags":["azure","bcdr","site recovery","disaster recovery","cloud strategy"]},{"location":"blog/azure-bcdr-review--turning-inherited-cloud-infrastructure-into-a-resilient-recovery-strategy/#4-storage-and-data-access-risk","title":"\ud83d\udcbe 4\ufe0f\u20e3 Storage and Data Access Risk","text":"<p>Azure Storage backed a range of services \u2014 from SFTP and app data to file shares and diagnostics.</p> <p>Replication strategies varied (LRS, RA-GRS, ZRS) with no consistent logic or documentation. Critical storage accounts weren\u2019t aligned with workload tiering.</p> <p>Some workloads used Azure Files and Azure File Sync, but without defined mount procedures or recovery checks.</p> <p>In short: compute could come back, but data availability wasn\u2019t assured.</p>","tags":["azure","bcdr","site recovery","disaster recovery","cloud strategy"]},{"location":"blog/azure-bcdr-review--turning-inherited-cloud-infrastructure-into-a-resilient-recovery-strategy/#5-the-networking-piece-and-why-it-matters-more-than-people-think","title":"\ud83d\udd0c 5\ufe0f\u20e3 The Networking Piece (And Why It Matters More Than People Think)","text":"<p>NSGs, UDRs, custom routing, and SD-WAN all played a part in how traffic flowed.</p> <p>But in DR, assumptions break quickly.</p> <p>There was no documentation of network flow in the DR region, and no validation of inter-VM or service-to-service reachability post-failover.</p> <p>Some services \u2014 like App Gateways, Internal Load Balancers, and Private Endpoints \u2014 were region-bound and would require re-deployment or manual intervention.</p> <p>Networking wasn\u2019t the background layer \u2014 it was core to recoverability.</p>","tags":["azure","bcdr","site recovery","disaster recovery","cloud strategy"]},{"location":"blog/azure-bcdr-review--turning-inherited-cloud-infrastructure-into-a-resilient-recovery-strategy/#6-capacity-risk-when-dr-isnt-guaranteed","title":"\ud83d\udce6 6\ufe0f\u20e3 Capacity Risk: When DR Isn\u2019t Guaranteed","text":"<p>VM replication is only half the story. The other half is whether those VMs can actually start during a DR event.</p> <p>Azure doesn\u2019t guarantee regional capacity unless you've pre-purchased it.</p> <p>In our case, no capacity reservations had been made. That meant no assurance that our Tier 0 or Tier 1 workloads could even boot if demand spiked during a region-wide outage.</p> <p>This is a quiet but critical risk \u2014 and one worth addressing early.</p>","tags":["azure","bcdr","site recovery","disaster recovery","cloud strategy"]},{"location":"blog/azure-bcdr-review--turning-inherited-cloud-infrastructure-into-a-resilient-recovery-strategy/#conclusion-from-discovery-to-direction","title":"\u2705 Conclusion: From Discovery to Direction","text":"<p>This review wasn\u2019t about proving whether DR was in place \u2014 it was about understanding whether it would actually work.</p> <p>The tooling was present. The protection was partial. The process was missing.</p> <p>By mapping out what was covered, where the gaps were, and how recovery would actually unfold, we created a baseline that gave us clarity and control.</p>","tags":["azure","bcdr","site recovery","disaster recovery","cloud strategy"]},{"location":"blog/azure-bcdr-review--turning-inherited-cloud-infrastructure-into-a-resilient-recovery-strategy/#coming-next-documenting-the-plan","title":"\ud83d\udcd8 Coming Next: Documenting the Plan","text":"<p>In the next post, I\u2019ll walk through how I formalised the review into a structured BCDR posture document \u2014 including:</p> <ul> <li>\ud83e\uddf1 Mapping workloads by tier and impact  </li> <li>\u23f3 Defining current vs target RTO/RPO  </li> <li>\ud83d\udee0\ufe0f Highlighting gaps in automation, DNS, storage, and capacity  </li> <li>\ud83e\udded Building a recovery plan roadmap  </li> <li>\u2696\ufe0f Framing cost vs risk for stakeholder alignment  </li> </ul> <p>If you're facing a similar situation \u2014 whether you're inheriting someone else's cloud estate or building DR into a growing environment \u2014 I hope this series helps bring structure to the complexity.</p> <p>Let me know if you'd find it useful to share templates or walkthroughs in the next post.</p> <p>Share on  Share on </p>","tags":["azure","bcdr","site recovery","disaster recovery","cloud strategy"]},{"location":"blog/azure-bcdr--how-i-turned-a-dr-review-into-a-strategic-recovery-plan/","title":"\ud83e\uddfe Azure BCDR \u2013 How I Turned a DR Review into a Strategic Recovery Plan","text":"<p>In Part 1 of this series, I shared how we reviewed our Azure BCDR posture after inheriting a partially implemented cloud estate. The findings were clear: while the right tools were in place, the operational side of disaster recovery hadn\u2019t been addressed.</p> <p>There were no test failovers, no documented Recovery Plans, no automation, and several blind spots in DNS, storage, and private access.</p> <p>This post outlines how I took that review and turned it into a practical recovery strategy \u2014 one that we could share internally, align with our CTO, and use as a foundation for further work with our support partner.</p> <p>To provide context, our estate is deployed primarily in the UK South Azure region, with UK West serving as the designated DR target region.</p> <p>It\u2019s not a template \u2014 it\u2019s a repeatable, real-world approach to structuring a BCDR plan when you\u2019re starting from inherited infrastructure, not a clean slate.</p>","tags":["azure","bcdr","disaster recovery","site recovery","strategy"]},{"location":"blog/azure-bcdr--how-i-turned-a-dr-review-into-a-strategic-recovery-plan/#1-why-documenting-the-plan-matters","title":"\ud83e\udded 1. Why Documenting the Plan Matters","text":"<p>Most cloud teams can identify issues. Fewer take the time to formalise the findings in a way that supports action and alignment.</p> <p>Documenting our BCDR posture gave us three things:</p> <ul> <li>\ud83e\udde0 Clarity \u2014 a shared understanding of what\u2019s protected and what isn\u2019t  </li> <li>\ud83d\udd26 Visibility \u2014 a way to surface risk and prioritise fixes  </li> <li>\ud83c\udfaf Direction \u2014 a set of realistic, cost-aware next steps  </li> </ul> <p>We weren\u2019t trying to solve every problem at once. The goal was to define a usable plan we could act on, iterate, and eventually test \u2014 all while making sure that effort was focused on the right areas.</p>","tags":["azure","bcdr","disaster recovery","site recovery","strategy"]},{"location":"blog/azure-bcdr--how-i-turned-a-dr-review-into-a-strategic-recovery-plan/#2-starting-the-document","title":"\ud83e\uddf1 2. Starting the Document","text":"<p>I structured the document to speak to both technical stakeholders and senior leadership. It needed to balance operational context with strategic risk.</p>","tags":["azure","bcdr","disaster recovery","site recovery","strategy"]},{"location":"blog/azure-bcdr--how-i-turned-a-dr-review-into-a-strategic-recovery-plan/#core-sections-included","title":"\u270d\ufe0f Core sections included","text":"<ul> <li>Executive Summary \u2013 what the document is, why it matters  </li> <li>Maturity Snapshot \u2013 a simple traffic-light view of current vs target posture  </li> <li>Workload Overview \u2013 what\u2019s in scope and what\u2019s protected  </li> <li>Recovery Objectives \u2013 realistic RPO/RTO targets by tier  </li> <li>Gaps and Risks \u2013 the areas most likely to cause DR failure  </li> <li>Recommendations \u2013 prioritised, actionable, and cost-aware  </li> <li>Next Steps \u2013 what we can handle internally, and what goes to the MSP</li> </ul> <p>Each section followed the same principle: clear, honest, and focused on action. No fluff, no overstatements \u2014 just a straightforward review of where we stood and what needed doing.</p>","tags":["azure","bcdr","disaster recovery","site recovery","strategy"]},{"location":"blog/azure-bcdr--how-i-turned-a-dr-review-into-a-strategic-recovery-plan/#3-defining-the-current-state","title":"\ud83e\udde9 3. Defining the Current State","text":"<p>Before we could plan improvements, we had to document what actually existed. This wasn\u2019t about assumptions \u2014 it was about capturing the real configuration and coverage in Azure.</p>","tags":["azure","bcdr","disaster recovery","site recovery","strategy"]},{"location":"blog/azure-bcdr--how-i-turned-a-dr-review-into-a-strategic-recovery-plan/#workload-inventory","title":"\ud83d\uddc2\ufe0f Workload Inventory","text":"<p>We started by categorising all VMs and services:</p> <ul> <li>Domain controllers</li> <li>Application servers (web/API/backend)</li> <li>SQL Managed Instances</li> <li>Infrastructure services (file, render, schedulers)</li> <li>Management and monitoring VMs</li> </ul> <p>Each workload was mapped by criticality and recovery priority \u2014 not just by type.</p>","tags":["azure","bcdr","disaster recovery","site recovery","strategy"]},{"location":"blog/azure-bcdr--how-i-turned-a-dr-review-into-a-strategic-recovery-plan/#protection-levels","title":"\ud83d\udee1\ufe0f Protection Levels","text":"<p>For each workload, we recorded:</p> <ul> <li>\u2705 Whether it was protected by ASR</li> <li>\ud83d\udd01 Whether it was backed up only</li> <li>\ud83d\udeab Whether it had no protection (with justification)</li> </ul> <p>We also reviewed the geographic layout \u2014 e.g. which services were replicated into UK West, and which existed only in UK South.</p>","tags":["azure","bcdr","disaster recovery","site recovery","strategy"]},{"location":"blog/azure-bcdr--how-i-turned-a-dr-review-into-a-strategic-recovery-plan/#supporting-services","title":"\ud83e\udde0 Supporting Services","text":"<p>Beyond the VMs, we looked at:</p> <ul> <li>Identity services (AD, domain controllers, replication health)</li> <li>DNS architecture (AD-integrated zones, private DNS zones)</li> <li>Private Endpoints and their region-specific availability</li> <li>Storage account replication types (LRS, RA-GRS, ZRS)</li> <li>Network security and routing configurations in DR</li> </ul> <p>The aim wasn\u2019t to build a full asset inventory \u2014 just to gather enough visibility to start making risk-based decisions about what mattered, and what was missing.</p>","tags":["azure","bcdr","disaster recovery","site recovery","strategy"]},{"location":"blog/azure-bcdr--how-i-turned-a-dr-review-into-a-strategic-recovery-plan/#4-setting-recovery-objectives","title":"\u23f1\ufe0f 4. Setting Recovery Objectives","text":"<p>Once the current state was mapped, the next step was to define what \u201crecovery\u201d should actually look like \u2014 in terms that could be communicated, challenged, and agreed.</p> <p>We focused on two key metrics:</p> <ul> <li>RTO (Recovery Time Objective): How long can this system be offline before we see significant operational impact?</li> <li>RPO (Recovery Point Objective): How much data loss is acceptable in a worst-case failover?</li> </ul> <p>These weren\u2019t guessed or copied from a template. We worked with realistic assumptions based on our tooling, team capability, and criticality of the services.</p>","tags":["azure","bcdr","disaster recovery","site recovery","strategy"]},{"location":"blog/azure-bcdr--how-i-turned-a-dr-review-into-a-strategic-recovery-plan/#tiered-recovery-model","title":"\ud83d\udcca Tiered Recovery Model","text":"<p>Each workload was assigned to one of four tiers:</p> Tier Description Tier 0 Core infrastructure (identity, DNS, routing) Tier 1 Mission-critical production workloads Tier 2 Important, but not time-sensitive services SQL MI Treated separately due to its PaaS nature <p>We then applied RTO and RPO targets based on what we could achieve today vs what we aim to reach with improvements.</p>","tags":["azure","bcdr","disaster recovery","site recovery","strategy"]},{"location":"blog/azure-bcdr--how-i-turned-a-dr-review-into-a-strategic-recovery-plan/#heatmap-example","title":"\ud83d\udd25 Heatmap Example","text":"Workload Tier RPO (Current) RTO (Current) RPO (Optimised) RTO (Optimised) Tier 0 \u2013 Identity 5 min 60 min 5 min 30 min Tier 1 \u2013 Prod 5 min 360 min 5 min 60 min Tier 2 \u2013 Non-Crit 1440 min 1440 min 60 min 240 min SQL MI 0 min 60 min 0 min 30 min","tags":["azure","bcdr","disaster recovery","site recovery","strategy"]},{"location":"blog/azure-bcdr--how-i-turned-a-dr-review-into-a-strategic-recovery-plan/#5-highlighting-gaps-and-risks","title":"\ud83d\udea7 5. Highlighting Gaps and Risks","text":"<p>With recovery objectives defined, the gaps became much easier to identify \u2014 and to prioritise.</p> <p>We weren\u2019t trying to protect everything equally. The goal was to focus attention on the areas that introduced the highest risk to recovery if left unresolved.</p>","tags":["azure","bcdr","disaster recovery","site recovery","strategy"]},{"location":"blog/azure-bcdr--how-i-turned-a-dr-review-into-a-strategic-recovery-plan/#what-we-flagged","title":"\u26a0\ufe0f What We Flagged","text":"<ul> <li>\u274c No test failovers had ever been performed  </li> <li>\u274c No Recovery Plans existed  </li> <li>\ud83c\udf10 Public-facing infrastructure only existed in one region  </li> <li>\ud83d\udd12 Private Endpoints lacked DR equivalents  </li> <li>\ud83e\udded DNS failover was manual or undefined  </li> <li>\ud83d\udcbe Storage accounts had inconsistent replication logic  </li> <li>\ud83d\udeab No capacity reservations existed for critical VM SKUs  </li> </ul> <p>Each gap was documented with its impact, priority, and remediation options.</p>","tags":["azure","bcdr","disaster recovery","site recovery","strategy"]},{"location":"blog/azure-bcdr--how-i-turned-a-dr-review-into-a-strategic-recovery-plan/#6-strategic-recommendations","title":"\ud83d\udee0\ufe0f 6. Strategic Recommendations","text":"<p>We split our recommendations into what we could handle internally, and what would require input from our MSP or further investment.</p>","tags":["azure","bcdr","disaster recovery","site recovery","strategy"]},{"location":"blog/azure-bcdr--how-i-turned-a-dr-review-into-a-strategic-recovery-plan/#internal-actions","title":"\ud83d\udccc Internal Actions","text":"<ul> <li>Build and test Recovery Plans for Tier 0 and Tier 1 workloads  </li> <li>Improve DNS failover scripting  </li> <li>Review VM tags to reflect criticality and protection state  </li> <li>Create sequencing logic for application groups  </li> <li>Align NSGs and UDRs in DR with production  </li> </ul>","tags":["azure","bcdr","disaster recovery","site recovery","strategy"]},{"location":"blog/azure-bcdr--how-i-turned-a-dr-review-into-a-strategic-recovery-plan/#msp-led-or-partner-support","title":"\ud83e\udd1d MSP-Led or Partner Support","text":"<ul> <li>Duplicate App Gateways / ILBs in UK West  </li> <li>Implement Private DNS Zones  </li> <li>Review and implement capacity reservations  </li> <li>Test runbook-driven recovery automation  </li> <li>Conduct structured test failovers across service groups  </li> </ul>","tags":["azure","bcdr","disaster recovery","site recovery","strategy"]},{"location":"blog/azure-bcdr--how-i-turned-a-dr-review-into-a-strategic-recovery-plan/#7-making-it-actionable","title":"\ud83d\udcc5 7. Making It Actionable","text":"<p>A plan needs ownership and timelines. We assigned tasks by role and defined short-, medium-, and long-term priorities using a simple planning table.</p> <p>We treat the BCDR document as a living artefact \u2014 updated quarterly, tied to change control, and used to guide internal work and partner collaboration.</p>","tags":["azure","bcdr","disaster recovery","site recovery","strategy"]},{"location":"blog/azure-bcdr--how-i-turned-a-dr-review-into-a-strategic-recovery-plan/#8-closing-reflections","title":"\ud83d\udd1a 8. Closing Reflections","text":"<p>The original goal wasn\u2019t to build a perfect DR solution \u2014 it was to understand where we stood, make recovery realistic, and document a plan that would hold up when we needed it most.</p> <p>We inherited a functional technical foundation \u2014 but needed to formalise and validate it as part of a resilient DR posture.</p> <p>By documenting the estate, defining recovery objectives, and identifying where the real risks were, we turned a passive DR posture into something we could act on. We gave stakeholders clarity. We gave the support partner direction. And we gave ourselves a roadmap.</p>","tags":["azure","bcdr","disaster recovery","site recovery","strategy"]},{"location":"blog/azure-bcdr--how-i-turned-a-dr-review-into-a-strategic-recovery-plan/#whats-next","title":"\ud83d\udd1c What\u2019s Next","text":"<p>In the next part of this series, I\u2019ll walk through how we executed the plan:</p> <ul> <li>Building and testing our first Recovery Plan  </li> <li>Improving ASR coverage and validation  </li> <li>Running our first failover drill  </li> <li>Reviewing results and updating the heatmap  </li> </ul> <p>If you're stepping into an inherited cloud environment or starting your first structured DR review, I hope this gives you a practical view of what\u2019s involved \u2014 and what\u2019s achievable without overcomplicating the process.</p> <p>Let me know if you'd like to see templates or report structures from this process in a future post.</p> <p>Share on  Share on </p>","tags":["azure","bcdr","disaster recovery","site recovery","strategy"]},{"location":"blog/saving-azure-costs-with-scheduled-vm-startstop-using-custom-azure-automation-runbooks/","title":"\ud83d\udcb0 Saving Azure Costs with Scheduled VM Start/Stop using Custom Azure Automation Runbooks","text":"<p>As part of my ongoing commitment to FinOps practices, I've implemented several strategies to embed cost-efficiency into the way we manage cloud infrastructure. One proven tactic is scheduling virtual machines to shut down during idle periods, avoiding unnecessary spend.</p> <p>In this post, I\u2019ll share how I\u2019ve built out custom Azure Automation jobs to schedule VM start and stop operations. Rather than relying on Microsoft\u2019s pre-packaged solution, I\u2019ve developed a streamlined, purpose-built PowerShell implementation that provides maximum flexibility, transparency, and control.</p>","tags":["azure automation","vm schedule","cost optimisation","powershell","finops","azure vm"]},{"location":"blog/saving-azure-costs-with-scheduled-vm-startstop-using-custom-azure-automation-runbooks/#why-i-chose-custom-runbooks-over-the-prebuilt-solution","title":"\u270d\ufe0f Why I Chose Custom Runbooks Over the Prebuilt Solution","text":"<p>Microsoft provides a ready-made \u201cStart/Stop VMs during off-hours\u201d solution via the Automation gallery. While functional, it\u2019s:</p> <ul> <li>A bit over-engineered for simple needs,</li> <li>Relatively opaque under the hood, and</li> <li>Not ideal for environments where control and transparency are priorities.</li> </ul> <p>My custom jobs:</p> <ul> <li>Use native PowerShell modules within Azure Automation,</li> <li>Are scoped to exactly the VMs I want via tags,</li> <li>Provide clean logging and alerting, and</li> <li>Keep things simple, predictable, and auditable.</li> </ul>","tags":["azure automation","vm schedule","cost optimisation","powershell","finops","azure vm"]},{"location":"blog/saving-azure-costs-with-scheduled-vm-startstop-using-custom-azure-automation-runbooks/#step-1-set-up-the-azure-automation-account","title":"\ud83d\udee0\ufe0f Step 1: Set Up the Azure Automation Account","text":"<p>\ud83d\udd17 Official docs: Create and manage an Azure Automation Account</p> <ol> <li>Go to the Azure Portal and search for Automation Accounts.</li> <li>Click + Create.</li> <li>Fill out the basics:</li> <li>Name: e.g. <code>vm-scheduler</code></li> <li>Resource Group: Create new or select existing</li> <li>Region: Preferably where your VMs are located</li> <li>Enable System-Assigned Managed Identity</li> <li>Once created, go to the Automation Account and ensure the following modules are imported using the Modules blade in the Azure Portal:</li> <li><code>Az.Accounts</code></li> <li><code>Az.Compute</code></li> </ol> <p>\u2705 Tip: These modules can be added from the gallery in just a few clicks via the UI\u2014no scripting required.</p> <p>\ud83d\udca1 Prefer scripting? You can also install them using PowerShell:</p> <pre><code>Install-Module -Name Az.Accounts -Force\nInstall-Module -Name Az.Compute -Force\n</code></pre> <ol> <li>Assign the Virtual Machine Contributor role to the Automation Account's managed identity at the resource group or subscription level.</li> </ol>","tags":["azure automation","vm schedule","cost optimisation","powershell","finops","azure vm"]},{"location":"blog/saving-azure-costs-with-scheduled-vm-startstop-using-custom-azure-automation-runbooks/#cli-or-powershell-alternatives","title":"\u2699\ufe0f CLI or PowerShell alternatives","text":"<pre><code># Azure CLI example to create the automation account\naz automation account create \\\n  --name vm-scheduler \\\n  --resource-group MyResourceGroup \\\n  --location uksouth \\\n  --assign-identity\n</code></pre>","tags":["azure automation","vm schedule","cost optimisation","powershell","finops","azure vm"]},{"location":"blog/saving-azure-costs-with-scheduled-vm-startstop-using-custom-azure-automation-runbooks/#step-2-add-vm-tags-for-scheduling","title":"\ud83d\udcc5 Step 2: Add VM Tags for Scheduling","text":"<p>Apply consistent tags to any VM you want the runbooks to manage.</p> Key Value <code>AutoStartStop</code> <code>devserver</code> <p>You can use the Azure Portal or PowerShell to apply these tags.</p>","tags":["azure automation","vm schedule","cost optimisation","powershell","finops","azure vm"]},{"location":"blog/saving-azure-costs-with-scheduled-vm-startstop-using-custom-azure-automation-runbooks/#tag-vms-via-powershell","title":"\u2699\ufe0f Tag VMs via PowerShell","text":"<pre><code>$vm = Get-AzVM -ResourceGroupName \"MyRG\" -Name \"myVM\"\n$vm.Tags[\"AutoStartStop\"] = \"devserver\"\nUpdate-AzVM -VM $vm -ResourceGroupName \"MyRG\"\n</code></pre>","tags":["azure automation","vm schedule","cost optimisation","powershell","finops","azure vm"]},{"location":"blog/saving-azure-costs-with-scheduled-vm-startstop-using-custom-azure-automation-runbooks/#step-3-create-the-runbooks","title":"\ud83d\udcc2 Step 3: Create the Runbooks","text":"<p>\ud83d\udd17 Official docs: Create a runbook in Azure Automation</p>","tags":["azure automation","vm schedule","cost optimisation","powershell","finops","azure vm"]},{"location":"blog/saving-azure-costs-with-scheduled-vm-startstop-using-custom-azure-automation-runbooks/#create-a-new-runbook","title":"\u25b6\ufe0f Create a New Runbook","text":"<ol> <li>In your Automation Account, go to Process Automation &gt; Runbooks.</li> <li>Click + Create a runbook.</li> <li>Name it something like <code>Stop-TaggedVMs</code>.</li> <li>Choose PowerShell as the type.</li> <li>Paste in the code below (repeat this process for the start runbook later).</li> </ol>","tags":["azure automation","vm schedule","cost optimisation","powershell","finops","azure vm"]},{"location":"blog/saving-azure-costs-with-scheduled-vm-startstop-using-custom-azure-automation-runbooks/#runbook-code-auto-stop-based-on-tags","title":"\ud83d\udd39 Runbook Code: Auto-Stop Based on Tags","text":"<pre><code>Param\n(    \n    [Parameter(Mandatory=$false)][ValidateNotNullOrEmpty()]\n    [String]\n    $AzureVMName = \"All\",\n\n    [Parameter(Mandatory=$true)][ValidateNotNullOrEmpty()]\n    [String]\n    $AzureSubscriptionID = \"&lt;your-subscription-id&gt;\"\n)\n\ntry {\n    \"Logging in to Azure...\"\n    # Authenticate using the system-assigned managed identity of the Automation Account\n    Connect-AzAccount -Identity -AccountId \"&lt;managed-identity-client-id&gt;\"\n} catch {\n    Write-Error -Message $_.Exception\n    throw $_.Exception\n}\n\n$TagName  = \"AutoStartStop\"\n$TagValue = \"devserver\"\n\nSet-AzContext -Subscription $AzureSubscriptionID\n\nif ($AzureVMName -ne \"All\") {\n    $VMs = Get-AzResource -TagName $TagName -TagValue $TagValue | Where-Object {\n        $_.ResourceType -like 'Microsoft.Compute/virtualMachines' -and $_.Name -like $AzureVMName\n    }\n} else {\n    $VMs = Get-AzResource -TagName $TagName -TagValue $TagValue | Where-Object {\n        $_.ResourceType -like 'Microsoft.Compute/virtualMachines'\n    }\n}\n\nforeach ($VM in $VMs) {\n    Stop-AzVM -ResourceGroupName $VM.ResourceGroupName -Name $VM.Name -Verbose -Force\n}\n</code></pre> <p>\ud83d\udd17 Docs: Connect-AzAccount with Managed Identity</p>","tags":["azure automation","vm schedule","cost optimisation","powershell","finops","azure vm"]},{"location":"blog/saving-azure-costs-with-scheduled-vm-startstop-using-custom-azure-automation-runbooks/#create-the-start-runbook","title":"\ud83d\udd39 Create the Start Runbook","text":"<p>Duplicate the above, replacing <code>Stop-AzVM</code> with <code>Start-AzVM</code>.</p> <p>\ud83d\udd17 Docs: Start-AzVM</p>","tags":["azure automation","vm schedule","cost optimisation","powershell","finops","azure vm"]},{"location":"blog/saving-azure-costs-with-scheduled-vm-startstop-using-custom-azure-automation-runbooks/#step-4-create-and-link-schedules","title":"\u23f0 Step 4: Create and Link Schedules","text":"<p>\ud83d\udd17 Docs: Create schedules in Azure Automation</p> <ol> <li>Go to the Automation Account &gt; Schedules &gt; + Add a schedule.</li> <li>Create two schedules:</li> <li><code>DailyStartWeekdays</code> \u2014 Recurs every weekday at 07:30</li> <li><code>DailyStopWeekdays</code> \u2014 Recurs every weekday at 18:30</li> <li>Go to each runbook &gt; Link to schedule &gt; Choose the matching schedule.</li> </ol> <p>\ud83d\udcca You can get creative here: separate schedules for dev vs UAT, or different times for different departments.</p>","tags":["azure automation","vm schedule","cost optimisation","powershell","finops","azure vm"]},{"location":"blog/saving-azure-costs-with-scheduled-vm-startstop-using-custom-azure-automation-runbooks/#testing-your-runbooks","title":"\ud83e\uddea Testing Your Runbooks","text":"<p>You can test each runbook directly in the portal:</p> <ul> <li>Open the runbook</li> <li>Click Edit &gt; Test Pane</li> <li>Provide test parameters if needed</li> <li>Click Start and monitor output</li> </ul> <p>This is also a good time to validate:</p> <ul> <li>The identity has permission</li> <li>The tags are applied correctly</li> <li>The VMs are in a stopped or running state as expected</li> </ul>","tags":["azure automation","vm schedule","cost optimisation","powershell","finops","azure vm"]},{"location":"blog/saving-azure-costs-with-scheduled-vm-startstop-using-custom-azure-automation-runbooks/#the-results","title":"\ud83d\udcca The Results","text":"<p>Even this lightweight automation has produced major savings in our environment. Non-prod VMs are now automatically turned off outside office hours, resulting in monthly compute savings of up to 60% without sacrificing availability during working hours.</p>","tags":["azure automation","vm schedule","cost optimisation","powershell","finops","azure vm"]},{"location":"blog/saving-azure-costs-with-scheduled-vm-startstop-using-custom-azure-automation-runbooks/#ideas-for-further-enhancement","title":"\ud83e\udde0 Ideas for Further Enhancement","text":"<ul> <li>Pull tag values from a central config (e.g. Key Vault or Storage Table)</li> <li>Add logic to check for active RDP sessions or Azure Monitor heartbeats</li> <li>Alert via email or Teams on job success/failure</li> <li>Track savings over time and visualize them</li> </ul>","tags":["azure automation","vm schedule","cost optimisation","powershell","finops","azure vm"]},{"location":"blog/saving-azure-costs-with-scheduled-vm-startstop-using-custom-azure-automation-runbooks/#final-thoughts","title":"\ud83d\udcad Final Thoughts","text":"<p>If you\u2019re looking for a practical, immediate way to implement FinOps principles in Azure, VM scheduling is a great place to start. With minimal setup and maximum flexibility, custom runbooks give you control without the complexity of the canned solutions.</p> <p>Have you built something similar or extended this idea further? I\u2019d love to hear about it\u2014drop me a comment or reach out on LinkedIn.</p> <p>Stay tuned for more FinOps tips coming soon!</p> <p>Share on  Share on </p>","tags":["azure automation","vm schedule","cost optimisation","powershell","finops","azure vm"]},{"location":"blog/replacing-sql-credentials-with-user-assigned-managed-identity-uami-in-azure-sql-managed-instance/","title":"Replacing SQL Credentials with User Assigned Managed Identity (UAMI) in Azure SQL Managed Instance","text":"<p>Storing SQL usernames and passwords in application configuration files is still common practice \u2014 but it poses a significant security risk. As part of improving our cloud security posture, I recently completed a project to eliminate plain text credentials from our app connection strings by switching to Azure User Assigned Managed Identity (UAMI) authentication for our SQL Managed Instance.</p> <p>In this post, I\u2019ll walk through how to:</p> <ul> <li>Securely connect to Azure SQL Managed Instance without using usernames or passwords</li> <li>Use a User Assigned Managed Identity (UAMI) for authentication</li> <li>Test this connection using the new Go-based <code>sqlcmd</code> CLI</li> <li>Update real application code to remove SQL credentials</li> </ul>","tags":["azure","sql","managed identity","uami","security","identity"]},{"location":"blog/replacing-sql-credentials-with-user-assigned-managed-identity-uami-in-azure-sql-managed-instance/#why-replace-sql-credentials","title":"\ud83d\udd10 Why Replace SQL Credentials?","text":"<p>Hardcoded SQL credentials come with several downsides:</p> <ul> <li>Security risk: Stored secrets can be compromised if not properly secured</li> <li>Maintenance overhead: Rotating passwords across environments is cumbersome</li> <li>Audit concerns: Plain text credentials often trigger compliance red flags</li> </ul> <p>Azure Managed Identity solves this by providing a token-based, identity-first way to connect to services \u2014 no secrets required.</p>","tags":["azure","sql","managed identity","uami","security","identity"]},{"location":"blog/replacing-sql-credentials-with-user-assigned-managed-identity-uami-in-azure-sql-managed-instance/#what-is-a-user-assigned-managed-identity","title":"\u2699\ufe0f What is a User Assigned Managed Identity?","text":"<p>There are two types of Managed Identities in Azure:</p> <ul> <li>System-assigned: Tied to the lifecycle of a specific resource (like a VM or App Service)</li> <li>User-assigned: Standalone identity that can be attached to one or more resources</li> </ul> <p>For this project, we used a User Assigned Managed Identity (UAMI) to allow our applications to authenticate against SQL without managing secrets.</p>","tags":["azure","sql","managed identity","uami","security","identity"]},{"location":"blog/replacing-sql-credentials-with-user-assigned-managed-identity-uami-in-azure-sql-managed-instance/#project-objective","title":"\ud83c\udf1f Project Objective","text":"<p>Replace plain text SQL credentials in application connection strings with User Assigned Managed Identity (UAMI) for secure, best-practice authentication to Azure SQL Managed Instances.</p>","tags":["azure","sql","managed identity","uami","security","identity"]},{"location":"blog/replacing-sql-credentials-with-user-assigned-managed-identity-uami-in-azure-sql-managed-instance/#prerequisites","title":"\u2705 Prerequisites","text":"<p>To follow this guide, you\u2019ll need:</p> <ul> <li>An Azure SQL Managed Instance with Microsoft Entra (AAD) authentication enabled</li> <li>A User Assigned Managed Identity (UAMI)</li> <li>An Azure VM or App Service to host your app (or test client)</li> <li>The Go-based <code>sqlcmd</code> CLI installed   \u2192 Install guide</li> </ul>","tags":["azure","sql","managed identity","uami","security","identity"]},{"location":"blog/replacing-sql-credentials-with-user-assigned-managed-identity-uami-in-azure-sql-managed-instance/#setting-up-the-user-assigned-managed-identity-uami","title":"\ud83d\udd27 Setting Up the User Assigned Managed Identity (UAMI)","text":"<p>Before connecting to Azure SQL using UAMI, ensure the following steps are completed:</p> <ul> <li>Create the UAMI</li> <li>Assign the UAMI to the Virtual Machine(s)</li> <li>Configure Microsoft Entra authentication on the SQL Managed Instance</li> <li>Grant SQL access to the UAMI</li> </ul> <p>These steps can be completed via Azure CLI, PowerShell, or the Azure Portal.</p>","tags":["azure","sql","managed identity","uami","security","identity"]},{"location":"blog/replacing-sql-credentials-with-user-assigned-managed-identity-uami-in-azure-sql-managed-instance/#step-1-create-the-user-assigned-managed-identity-uami","title":"\ud83d\udee0\ufe0f Step 1: Create the User Assigned Managed Identity (UAMI)","text":"","tags":["azure","sql","managed identity","uami","security","identity"]},{"location":"blog/replacing-sql-credentials-with-user-assigned-managed-identity-uami-in-azure-sql-managed-instance/#cli","title":"\u2705 CLI","text":"<pre><code>az identity create \\\n  --name my-sql-uami \\\n  --resource-group my-rg \\\n  --location &lt;region&gt;\n</code></pre> <p>Save the Client ID and Object ID \u2014 you\u2019ll need them later.</p>","tags":["azure","sql","managed identity","uami","security","identity"]},{"location":"blog/replacing-sql-credentials-with-user-assigned-managed-identity-uami-in-azure-sql-managed-instance/#portal","title":"\u2705 Portal","text":"<ol> <li>Go to Azure Portal \u2192 Search Managed Identities</li> <li>Click + Create</li> <li>Choose Subscription, Resource Group, and Region</li> <li>Name the identity (e.g., <code>my-sql-uami</code>)</li> <li>Click Review + Create</li> </ol>","tags":["azure","sql","managed identity","uami","security","identity"]},{"location":"blog/replacing-sql-credentials-with-user-assigned-managed-identity-uami-in-azure-sql-managed-instance/#step-2-assign-the-uami-to-a-virtual-machine","title":"\ud83d\udd87\ufe0f Step 2: Assign the UAMI to a Virtual Machine","text":"<p>Attach the UAMI to:</p> <ul> <li>The VM(s) running your application code</li> <li>The VM used to test the connection</li> </ul>","tags":["azure","sql","managed identity","uami","security","identity"]},{"location":"blog/replacing-sql-credentials-with-user-assigned-managed-identity-uami-in-azure-sql-managed-instance/#cli_1","title":"\u2705 CLI","text":"<pre><code>az vm identity assign \\\n  --name my-vm-name \\\n  --resource-group my-rg \\\n  --identities my-sql-uami\n</code></pre>","tags":["azure","sql","managed identity","uami","security","identity"]},{"location":"blog/replacing-sql-credentials-with-user-assigned-managed-identity-uami-in-azure-sql-managed-instance/#portal_1","title":"\u2705 Portal","text":"<ol> <li>Go to Virtual Machines \u2192 Select your VM</li> <li>Click Identity under Settings</li> <li>Go to the User assigned tab</li> <li>Click + Add \u2192 Select the UAMI</li> <li>Click Add</li> </ol>","tags":["azure","sql","managed identity","uami","security","identity"]},{"location":"blog/replacing-sql-credentials-with-user-assigned-managed-identity-uami-in-azure-sql-managed-instance/#step-3-configure-sql-managed-instance-for-microsoft-entra-authentication","title":"\ud83d\udd11 Step 3: Configure SQL Managed Instance for Microsoft Entra Authentication","text":"<ol> <li>Set an Entra Admin:</li> <li>Go to your SQL MI \u2192 Azure AD admin blade</li> <li>Click Set admin and choose a user or group</li> <li> <p>Save changes</p> </li> <li> <p>Ensure Directory Reader permissions:</p> </li> <li>Your SQL MI\u2019s managed identity needs Directory Reader access</li> <li>You can assign this role via Entra ID &gt; Roles and administrators &gt; Directory Readers</li> </ol> <p>More details: Configure Entra authentication</p>","tags":["azure","sql","managed identity","uami","security","identity"]},{"location":"blog/replacing-sql-credentials-with-user-assigned-managed-identity-uami-in-azure-sql-managed-instance/#step-4-optional-assign-azure-role-to-the-uami","title":"\ud83d\udcdc Step 4: (Optional) Assign Azure Role to the UAMI","text":"<p>This may be needed if the identity needs to access Azure resource metadata or use Azure CLI from the VM.</p>","tags":["azure","sql","managed identity","uami","security","identity"]},{"location":"blog/replacing-sql-credentials-with-user-assigned-managed-identity-uami-in-azure-sql-managed-instance/#cli_2","title":"\u2705 CLI","text":"<pre><code>az role assignment create \\\n  --assignee-object-id &lt;uami-object-id&gt; \\\n  --role \"Reader\" \\\n  --scope /subscriptions/&lt;sub-id&gt;/resourceGroups/&lt;rg-name&gt;\n</code></pre>","tags":["azure","sql","managed identity","uami","security","identity"]},{"location":"blog/replacing-sql-credentials-with-user-assigned-managed-identity-uami-in-azure-sql-managed-instance/#portal_2","title":"\u2705 Portal","text":"<ol> <li>Go to the UAMI \u2192 Azure role assignments</li> <li>Click + Add role assignment</li> <li>Choose role (e.g., Reader)</li> <li>Set scope</li> <li>Click Save</li> </ol>","tags":["azure","sql","managed identity","uami","security","identity"]},{"location":"blog/replacing-sql-credentials-with-user-assigned-managed-identity-uami-in-azure-sql-managed-instance/#step-5-grant-sql-access-to-the-uami","title":"\ud83d\udd11 Step 5: Grant SQL Access to the UAMI","text":"<p>Once the UAMI is assigned to the VM and Entra auth is enabled on SQL MI, log in with an admin and run:</p> <pre><code>CREATE USER [&lt;client-id&gt;] FROM EXTERNAL PROVIDER;\nALTER ROLE db_datareader ADD MEMBER [&lt;client-id&gt;];\nALTER ROLE db_datawriter ADD MEMBER [&lt;client-id&gt;];\n</code></pre> <p>Or use a friendly name:</p> <pre><code>CREATE USER [my-app-identity] FROM EXTERNAL PROVIDER;\nALTER ROLE db_datareader ADD MEMBER [my-app-identity];\n</code></pre>","tags":["azure","sql","managed identity","uami","security","identity"]},{"location":"blog/replacing-sql-credentials-with-user-assigned-managed-identity-uami-in-azure-sql-managed-instance/#step-6-test-the-connection-using-sqlcmd","title":"\ud83e\uddea Step 6: Test the Connection Using <code>sqlcmd</code>","text":"<pre><code>sqlcmd \\\n  -S &lt;your-sql-mi&gt;.database.windows.net \\\n  -d &lt;database-name&gt; \\\n  --authentication-method ActiveDirectoryManagedIdentity \\\n  -U &lt;client-id-of-uami&gt;\n</code></pre> <p>If successful, you\u2019ll see the <code>1&gt;</code> prompt where you can execute SQL queries.</p>","tags":["azure","sql","managed identity","uami","security","identity"]},{"location":"blog/replacing-sql-credentials-with-user-assigned-managed-identity-uami-in-azure-sql-managed-instance/#step-7-update-application-code","title":"\ud83d\udcca Step 7: Update Application Code","text":"<p>Update your app to use the UAMI for authentication.</p> <p>Example connection string for UAMI in C#:</p> <pre><code>string connectionString = @\"Server=tcp:&lt;your-sql-mi&gt;.database.windows.net;\" +\n                          \"Authentication=Active Directory Managed Identity;\" +\n                          \"Encrypt=True;\" +\n                          \"User Id=&lt;your-uami-client-id&gt;;\" +\n                          \"Database=&lt;your-db-name&gt;;\";\n</code></pre> <p>Make sure your code uses <code>Microsoft.Data.SqlClient</code> with AAD token support.</p> <p>Or retrieve and assign the token programmatically:</p> <pre><code>var credential = new DefaultAzureCredential();\nvar token = await credential.GetTokenAsync(new TokenRequestContext(\n    new[] { \"https://database.windows.net/\" }));\n\nvar connection = new SqlConnection(\"Server=&lt;your-sql-mi&gt;; Database=&lt;your-db-name&gt;; Encrypt=True;\");\nconnection.AccessToken = token.Token;\n</code></pre>","tags":["azure","sql","managed identity","uami","security","identity"]},{"location":"blog/replacing-sql-credentials-with-user-assigned-managed-identity-uami-in-azure-sql-managed-instance/#security-benefits","title":"\ud83d\udd12 Security Benefits","text":"<ul> <li>\ud83d\udd10 No credentials stored</li> <li>\ud83d\udd01 No password rotation</li> <li>\ud83d\udee1\ufe0f Entra-integrated access control and auditing</li> </ul>","tags":["azure","sql","managed identity","uami","security","identity"]},{"location":"blog/replacing-sql-credentials-with-user-assigned-managed-identity-uami-in-azure-sql-managed-instance/#summary","title":"\u2705 Summary","text":"<p>By switching to User Assigned Managed Identity, we removed credentials from connection strings and aligned SQL access with best practices for cloud identity and security.</p> <p>Comments and feedback welcome!</p> <p>Share on  Share on </p>","tags":["azure","sql","managed identity","uami","security","identity"]},{"location":"blog/replacing-sas-tokens-with-user-assigned-managed-identity-uami-in-azcopy-for-blob-uploads/","title":"Replacing SAS Tokens with User Assigned Managed Identity (UAMI) in AzCopy for Blob Uploads","text":"<p>Using Shared Access Signature (SAS) tokens with <code>azcopy</code> is common \u2014 but rotating tokens and handling them securely can be a hassle. To improve security and simplify our automation, I recently replaced SAS-based authentication in our scheduled AzCopy jobs with Azure User Assigned Managed Identity (UAMI).</p> <p>In this post, I\u2019ll walk through how to:</p> <ul> <li>Replace AzCopy SAS tokens with managed identity authentication</li> <li>Assign the right roles to the UAMI</li> <li>Use <code>azcopy login</code> to authenticate non-interactively</li> <li>Automate the whole process in PowerShell</li> </ul>","tags":["azure","azcopy","blob storage","uami","identity","sas","powershell"]},{"location":"blog/replacing-sas-tokens-with-user-assigned-managed-identity-uami-in-azcopy-for-blob-uploads/#why-remove-sas-tokens","title":"\ud83d\udd0d Why Remove SAS Tokens?","text":"<p>SAS tokens are useful, but:</p> <ul> <li>\ud83d\udd11 They\u2019re still secrets \u2014 and secrets can be leaked</li> <li>\ud83d\udcc5 They expire \u2014 which breaks automation when not rotated</li> <li>\ud83d\udd10 They grant broad access \u2014 unless scoped very carefully</li> </ul> <p>Managed Identity is a much better approach when the copy job is running from within Azure (like an Azure VM or Automation account).</p>","tags":["azure","azcopy","blob storage","uami","identity","sas","powershell"]},{"location":"blog/replacing-sas-tokens-with-user-assigned-managed-identity-uami-in-azcopy-for-blob-uploads/#project-goal","title":"\ud83c\udf1f Project Goal","text":"<p>Replace the use of SAS tokens in an AzCopy job that uploads files from a local UNC share to Azure Blob Storage \u2014 by using a User Assigned Managed Identity.</p>","tags":["azure","azcopy","blob storage","uami","identity","sas","powershell"]},{"location":"blog/replacing-sas-tokens-with-user-assigned-managed-identity-uami-in-azcopy-for-blob-uploads/#prerequisites","title":"\u2705 Prerequisites","text":"<p>To follow along, you\u2019ll need:</p> <ul> <li>A User Assigned Managed Identity (UAMI)</li> <li>A Windows Server or Azure VM to run the copy job</li> <li>Access to a local source folder or UNC share (e.g., <code>\\\\fileserver\\\\data\\\\export\\\\</code>)</li> <li>AzCopy v10.7+ installed on the machine</li> <li>Azure RBAC permissions to assign roles</li> </ul> <p>\u2139\ufe0f Check AzCopy Version: Run <code>azcopy --version</code> to ensure you're using v10.7.0 or later, which is required for <code>--identity-client-id</code> support.</p>","tags":["azure","azcopy","blob storage","uami","identity","sas","powershell"]},{"location":"blog/replacing-sas-tokens-with-user-assigned-managed-identity-uami-in-azcopy-for-blob-uploads/#step-by-step-setup","title":"\ud83d\udd27 Step-by-Step Setup","text":"","tags":["azure","azcopy","blob storage","uami","identity","sas","powershell"]},{"location":"blog/replacing-sas-tokens-with-user-assigned-managed-identity-uami-in-azcopy-for-blob-uploads/#step-1-create-the-uami","title":"\ud83d\udee0\ufe0f Step 1: Create the UAMI","text":"","tags":["azure","azcopy","blob storage","uami","identity","sas","powershell"]},{"location":"blog/replacing-sas-tokens-with-user-assigned-managed-identity-uami-in-azcopy-for-blob-uploads/#cli","title":"\u2705 CLI","text":"<pre><code>az identity create \\\n  --name my-azcopy-uami \\\n  --resource-group my-resource-group \\\n  --location &lt;region&gt;\n</code></pre>","tags":["azure","azcopy","blob storage","uami","identity","sas","powershell"]},{"location":"blog/replacing-sas-tokens-with-user-assigned-managed-identity-uami-in-azcopy-for-blob-uploads/#portal","title":"\u2705 Portal","text":"<ol> <li>Go to Managed Identities in the Azure Portal</li> <li>Click + Create and follow the wizard</li> </ol>","tags":["azure","azcopy","blob storage","uami","identity","sas","powershell"]},{"location":"blog/replacing-sas-tokens-with-user-assigned-managed-identity-uami-in-azcopy-for-blob-uploads/#step-2-assign-the-uami-to-the-azure-vm","title":"\ud83d\udd87\ufe0f Step 2: Assign the UAMI to the Azure VM","text":"<p>AzCopy running on a VM must be able to assume the identity. Assign the UAMI to your VM:</p>","tags":["azure","azcopy","blob storage","uami","identity","sas","powershell"]},{"location":"blog/replacing-sas-tokens-with-user-assigned-managed-identity-uami-in-azcopy-for-blob-uploads/#cli_1","title":"\u2705 CLI","text":"<pre><code>az vm identity assign \\\n  --name my-vm-name \\\n  --resource-group my-resource-group \\\n  --identities my-azcopy-uami\n</code></pre>","tags":["azure","azcopy","blob storage","uami","identity","sas","powershell"]},{"location":"blog/replacing-sas-tokens-with-user-assigned-managed-identity-uami-in-azcopy-for-blob-uploads/#portal_1","title":"\u2705 Portal","text":"<ol> <li>Navigate to the Virtual Machines blade</li> <li>Select the VM running your AzCopy script</li> <li>Under Settings, click Identity</li> <li>Go to the User assigned tab</li> <li>Click + Add, select your UAMI, then click Add</li> </ol>","tags":["azure","azcopy","blob storage","uami","identity","sas","powershell"]},{"location":"blog/replacing-sas-tokens-with-user-assigned-managed-identity-uami-in-azcopy-for-blob-uploads/#step-3-assign-rbac-permissions-to-uami","title":"\ud83d\udd10 Step 3: Assign RBAC Permissions to UAMI","text":"<p>For AzCopy to function correctly with a UAMI, the following role assignments are recommended:</p> <ul> <li>Storage Blob Data Contributor: Required for read/write blob operations</li> <li>Storage Blob Data Reader: (Optional) For read-only scenarios or validation scripts</li> <li>Reader: (Optional) For browsing or metadata-only permissions on the storage account</li> </ul> <p>\u23f3 RBAC Tip: It may take up to 5 minutes for role assignments to propagate fully. If access fails initially, wait and retry.</p>","tags":["azure","azcopy","blob storage","uami","identity","sas","powershell"]},{"location":"blog/replacing-sas-tokens-with-user-assigned-managed-identity-uami-in-azcopy-for-blob-uploads/#cli_2","title":"\u2705 CLI","text":"<pre><code>az role assignment create \\\n  --assignee &lt;client-id-or-object-id&gt; \\\n  --role \"Storage Blob Data Contributor\" \\\n  --scope \"/subscriptions/&lt;sub-id&gt;/resourceGroups/&lt;rg&gt;/providers/Microsoft.Storage/storageAccounts/&lt;storage-account&gt;/blobServices/default/containers/&lt;container-name&gt;\"\n\naz role assignment create \\\n  --assignee &lt;client-id-or-object-id&gt; \\\n  --role \"Storage Blob Data Reader\" \\\n  --scope \"/subscriptions/&lt;sub-id&gt;/resourceGroups/&lt;rg&gt;/providers/Microsoft.Storage/storageAccounts/&lt;storage-account&gt;\"\n\naz role assignment create \\\n  --assignee &lt;client-id-or-object-id&gt; \\\n  --role \"Reader\" \\\n  --scope \"/subscriptions/&lt;sub-id&gt;/resourceGroups/&lt;rg&gt;/providers/Microsoft.Storage/storageAccounts/&lt;storage-account&gt;\"\n</code></pre>","tags":["azure","azcopy","blob storage","uami","identity","sas","powershell"]},{"location":"blog/replacing-sas-tokens-with-user-assigned-managed-identity-uami-in-azcopy-for-blob-uploads/#portal_2","title":"\u2705 Portal","text":"<ol> <li>Go to your Storage Account in the Azure Portal</li> <li>Click on the relevant container (or stay at the account level for broader scope)</li> <li>Open Access Control (IAM)</li> <li>Click + Add role assignment</li> <li>Repeat this for each role:</li> <li>Select Storage Blob Data Contributor, assign to your UAMI, and click Save</li> <li>Select Storage Blob Data Reader, assign to your UAMI, and click Save</li> <li>Select Reader, assign to your UAMI, and click Save</li> </ol>","tags":["azure","azcopy","blob storage","uami","identity","sas","powershell"]},{"location":"blog/replacing-sas-tokens-with-user-assigned-managed-identity-uami-in-azcopy-for-blob-uploads/#step-4-test-azcopy-login-using-uami","title":"\ud83e\uddea Step 4: Test AzCopy Login Using UAMI","text":"<pre><code>$clientId = \"&lt;your-uami-client-id&gt;\"\n&amp; \"C:\\azcopy\\azcopy.exe\" login --identity --identity-client-id $clientId\n</code></pre> <p>You should see a confirmation message that AzCopy has successfully logged in.</p> <p>\ud83d\udd0d To verify AzCopy is authenticated with the correct identity, you can run:</p> <pre><code>azcopy env\n</code></pre> <p>This will show the login type and confirm whether the token is being sourced from the Managed Identity.</p>","tags":["azure","azcopy","blob storage","uami","identity","sas","powershell"]},{"location":"blog/replacing-sas-tokens-with-user-assigned-managed-identity-uami-in-azcopy-for-blob-uploads/#step-5-upload-files-using-azcopy-uami","title":"\ud83d\udcc1 Step 5: Upload Files Using AzCopy + UAMI","text":"<p>Here's the PowerShell script that copies all files from a local share to the Blob container:</p> <pre><code>$clientId = \"&lt;your-uami-client-id&gt;\"\n\n# Login with Managed Identity\n&amp; \"C:\\azcopy\\azcopy.exe\" login --identity --identity-client-id $clientId\n\n# Run the copy job\n&amp; \"C:\\azcopy\\azcopy.exe\" copy \\\n  \"\\\\\\\\fileserver\\\\data\\\\export\\\\\" \\\n  \"https://&lt;your-storage-account&gt;.blob.core.windows.net/&lt;container-name&gt;\" \\\n  --overwrite=true \\\n  --from-to=LocalBlob \\\n  --blob-type=Detect \\\n  --put-md5 \\\n  --recursive \\\n  --log-level=INFO\n</code></pre> <p>\ud83d\udca1 UNC Note: Double backslashes are used in PowerShell to represent UNC paths properly.</p> <p>This script can be scheduled using Task Scheduler or run on demand.</p>","tags":["azure","azcopy","blob storage","uami","identity","sas","powershell"]},{"location":"blog/replacing-sas-tokens-with-user-assigned-managed-identity-uami-in-azcopy-for-blob-uploads/#automate-with-task-scheduler-optional","title":"\u23f1\ufe0f Automate with Task Scheduler (Optional)","text":"<p>To automate the job:</p> <ol> <li>Open Task Scheduler on your VM</li> <li>Create a New Task (not a Basic Task)</li> <li>Under General, select \"Run whether user is logged on or not\"</li> <li>Under Actions, add a new action to run <code>powershell.exe</code></li> <li>Set the arguments to point to your <code>.ps1</code> script</li> <li>Ensure the AzCopy path is hardcoded in your script</li> </ol>","tags":["azure","azcopy","blob storage","uami","identity","sas","powershell"]},{"location":"blog/replacing-sas-tokens-with-user-assigned-managed-identity-uami-in-azcopy-for-blob-uploads/#troubleshooting-common-errors","title":"\ud83d\ude91 Troubleshooting Common Errors","text":"","tags":["azure","azcopy","blob storage","uami","identity","sas","powershell"]},{"location":"blog/replacing-sas-tokens-with-user-assigned-managed-identity-uami-in-azcopy-for-blob-uploads/#403-authorizationpermissionmismatch","title":"\u274c 403 AuthorizationPermissionMismatch","text":"<ul> <li>Usually means the identity doesn\u2019t have the correct role or the role hasn\u2019t propagated yet</li> <li>Double-check:</li> <li>UAMI is assigned to the VM</li> <li>UAMI has <code>Storage Blob Data Contributor</code> on the correct container</li> <li>Wait 2\u20135 minutes and try again</li> </ul>","tags":["azure","azcopy","blob storage","uami","identity","sas","powershell"]},{"location":"blog/replacing-sas-tokens-with-user-assigned-managed-identity-uami-in-azcopy-for-blob-uploads/#azcopy-the-term-azcopy-is-not-recognized","title":"\u274c azcopy : The term 'azcopy' is not recognized","text":"<ul> <li>AzCopy is not in the system PATH</li> <li>Solution: Use the full path to <code>azcopy.exe</code>, like <code>C:\\azcopy\\azcopy.exe</code></li> </ul>","tags":["azure","azcopy","blob storage","uami","identity","sas","powershell"]},{"location":"blog/replacing-sas-tokens-with-user-assigned-managed-identity-uami-in-azcopy-for-blob-uploads/#benefits-of-switching-to-uami","title":"\ud83d\udee1\ufe0f Benefits of Switching to UAMI","text":"<ul> <li>\u2705 No secrets or keys stored on disk</li> <li>\u2705 No manual token expiry issues</li> <li>\u2705 Access controlled via Azure RBAC</li> <li>\u2705 Easily scoped and auditable</li> </ul>","tags":["azure","azcopy","blob storage","uami","identity","sas","powershell"]},{"location":"blog/replacing-sas-tokens-with-user-assigned-managed-identity-uami-in-azcopy-for-blob-uploads/#final-thoughts","title":"\ud83e\uddfc Final Thoughts","text":"<p>Replacing AzCopy SAS tokens with UAMI is one of those small wins that pays dividends over time. Once set up, it's secure, robust, and hands-off.</p> <p>Let me know if you'd like a variant of this that works from Azure Automation or a hybrid worker!</p> <p>Share on  Share on </p>","tags":["azure","azcopy","blob storage","uami","identity","sas","powershell"]},{"location":"blog/bringing-patch-management-in-house-migrating-from-msp-to-azure-update-manager/","title":"\ud83d\udd04 Bringing Patch Management In-House: Migrating from MSP to Azure Update Manager","text":"<p>It's all fun and games until the MSP contract expires and you realise 90 VMs still need their patching schedules sorted\u2026</p> <p>With our MSP contract winding down, the time had come to bring VM patching back in house. Our third-party provider had been handling it with their own tooling, which would no longer be used when the service contract expired.</p> <p>Enter Azure Update Manager \u2014 the modern, agentless way to manage patching schedules across your Azure VMs. Add a bit of PowerShell, sprinkle in some Azure Policy, and you've got yourself a scalable, policy-driven solution that's more visible, auditable, and way more maintainable.</p> <p>Here's how I made the switch \u2014 and managed to avoid a patching panic.</p>","tags":["Azure Update Manager","PowerShell","Azure Policy","VM Patching","FinOps"]},{"location":"blog/bringing-patch-management-in-house-migrating-from-msp-to-azure-update-manager/#prerequisites-permissions","title":"\u2699\ufe0f Prerequisites &amp; Permissions","text":"<p>Let's get the plumbing sorted before diving in.</p> <p>You'll need:</p> <ul> <li>The right PowerShell modules:</li> </ul> <pre><code>Install-Module Az -Scope CurrentUser -Force\nImport-Module Az.Maintenance, Az.Resources, Az.Compute\n</code></pre> <ul> <li>An account with Contributor permissions (or higher)</li> <li>Registered providers to avoid mysterious error messages:</li> </ul> <pre><code>Register-AzResourceProvider -ProviderNamespace Microsoft.Maintenance\nRegister-AzResourceProvider -ProviderNamespace Microsoft.GuestConfiguration\n</code></pre> <p>Why Resource Providers? Azure Update Manager needs these registered to create the necessary API endpoints and resource types in your subscription. Without them, you'll get cryptic \"resource type not found\" errors.</p> <p>Official documentation on Azure Update Manager prerequisites</p>","tags":["Azure Update Manager","PowerShell","Azure Policy","VM Patching","FinOps"]},{"location":"blog/bringing-patch-management-in-house-migrating-from-msp-to-azure-update-manager/#step-1-audit-the-current-setup","title":"\ud83d\udd75\ufe0f Step 1 \u2013 Audit the Current Setup","text":"<p>First order of business: collect the patching summary data from the MSP \u2014 which, helpfully, came in the form of multiple weekly CSV exports.</p> <p>I used GenAI to wrangle the mess into a structured format. The result was a clear categorisation of VMs based on the day and time they were typically patched \u2014 a solid foundation to work from.</p>","tags":["Azure Update Manager","PowerShell","Azure Policy","VM Patching","FinOps"]},{"location":"blog/bringing-patch-management-in-house-migrating-from-msp-to-azure-update-manager/#step-2-create-seven-new-maintenance-configurations","title":"\ud83e\uddf1 Step 2 \u2013 Create Seven New Maintenance Configurations","text":"<p>This is the foundation of Update Manager \u2014 define your recurring patch windows.</p> Click to expand: Create Maintenance Configurations (Sample Script) <pre><code># Azure Update Manager - Create Weekly Maintenance Configurations\n# Pure PowerShell syntax\n\n# Define parameters\n$resourceGroupName = \"rg-maintenance-uksouth-001\"\n$location = \"uksouth\"\n$timezone = \"GMT Standard Time\"\n$startDateTime = \"2024-06-01 21:00\"\n$duration = \"03:00\"  # 3 hours - meets minimum requirement\n\n# Day mapping for config naming (3-letter lowercase)\n$dayMap = @{\n    \"Monday\"    = \"mon\"\n    \"Tuesday\"   = \"tue\" \n    \"Wednesday\" = \"wed\"\n    \"Thursday\"  = \"thu\"\n    \"Friday\"    = \"fri\"\n    \"Saturday\"  = \"sat\"\n    \"Sunday\"    = \"sun\"\n}\n\n# Create maintenance configurations for each day\nforeach ($day in $dayMap.Keys) {\n    $shortDay = $dayMap[$day]\n    $configName = \"contoso-maintenance-config-vms-$shortDay\"\n\n    Write-Host \"Creating: $configName for $day...\" -ForegroundColor Yellow\n\n    try {\n        $result = New-AzMaintenanceConfiguration `\n            -ResourceGroupName $resourceGroupName `\n            -Name $configName `\n            -MaintenanceScope \"InGuestPatch\" `\n            -Location $location `\n            -StartDateTime $startDateTime `\n            -Timezone $timezone `\n            -Duration $duration `\n            -RecurEvery \"Week $day\" `\n            -InstallPatchRebootSetting \"IfRequired\" `\n            -ExtensionProperty @{\"InGuestPatchMode\" = \"User\"} `\n            -WindowParameterClassificationToInclude @(\"Critical\", \"Security\") `\n            -LinuxParameterClassificationToInclude @(\"Critical\", \"Security\") `\n            -Tag @{\n                \"Application\"  = \"Azure Update Manager\"\n                \"Owner\"        = \"Contoso\"\n                \"PatchWindow\"  = $shortDay\n            } `\n            -ErrorAction Stop\n\n        Write-Host \"\u2713 SUCCESS: $configName\" -ForegroundColor Green\n\n        # Quick validation\n        $createdConfig = Get-AzMaintenanceConfiguration -ResourceGroupName $resourceGroupName -Name $configName\n        Write-Host \"  Validated: $($createdConfig.RecurEvery) schedule confirmed\" -ForegroundColor Gray\n\n    } catch {\n        Write-Host \"\u2717 FAILED: $configName - $($_.Exception.Message)\" -ForegroundColor Red\n        continue\n    }\n}\n</code></pre> <p>\u26a0\ufe0f Don't forget: duration format is ISO 8601, not \"2 hours\" \u2014 and start time has to match the day it's tied to.</p> <p>Learn more about New-AzMaintenanceConfiguration</p>","tags":["Azure Update Manager","PowerShell","Azure Policy","VM Patching","FinOps"]},{"location":"blog/bringing-patch-management-in-house-migrating-from-msp-to-azure-update-manager/#step-3-tweak-the-maintenance-configs","title":"\ud83d\udee0\ufe0f Step 3 \u2013 Tweak the Maintenance Configs","text":"<p>Some patch windows felt too tight \u2014 and, just as importantly, I needed to avoid overlaps with existing backup jobs. Rather than let a large CU fail halfway through or run headlong into an Azure Backup job, I extended the duration on select configs and staggered them across the week:</p> <pre><code>$config = Get-AzMaintenanceConfiguration -ResourceGroupName \"rg-maintenance-uksouth-001\" -Name \"contoso-maintenance-config-vms-sun\"\n$config.Duration = \"04:00\"\nUpdate-AzMaintenanceConfiguration -ResourceGroupName \"rg-maintenance-uksouth-001\" -Name \"contoso-maintenance-config-vms-sun\" -Configuration $config\n\n# Verify the change\n$updatedConfig = Get-AzMaintenanceConfiguration -ResourceGroupName \"rg-maintenance-uksouth-001\" -Name \"contoso-maintenance-config-vms-sun\"\nWrite-Host \"Sunday window now: $($updatedConfig.Duration) duration\" -ForegroundColor Green\n</code></pre> <p>Learn more about Update-AzMaintenanceConfiguration</p>","tags":["Azure Update Manager","PowerShell","Azure Policy","VM Patching","FinOps"]},{"location":"blog/bringing-patch-management-in-house-migrating-from-msp-to-azure-update-manager/#step-4-use-ai-to-group-vms-by-patch-activity","title":"\ud83e\udd16 Step 4 \u2013 Use AI to Group VMs by Patch Activity","text":"<p>Armed with CSV exports of the latest patching summaries, I got AI to do the grunt work and make sense of the contents.</p> <p>What I did:</p> <ol> <li>Exported MSP data: Weekly CSV reports showing patch installation timestamps for each VM</li> <li> <p>Used Gen AI with various iterative prompts, starting the conversation with this:</p> <p>\"Attached is an export summary of the current patching activity from our incumbent MSP who currently look after the patching of the VM's in Azure I need you to review timestamps and work out which maintenance window each vm is currently in, and then match that to the appropriate maintenance config that we have just created. If there are mis matches in new and current schedule then we may need to tweak the settings of the new configs\"</p> </li> <li> <p>AI analysis revealed:</p> </li> <li>60% of VMs were patching on one weekday evening</li> <li>Several critical systems patching simultaneously</li> <li> <p>No consideration for application dependencies</p> </li> <li> <p>AI recommendation: Spread VMs across weekdays based on:</p> </li> <li>Criticality: Domain controllers on different days</li> <li>Function: Similar servers on different days (avoid single points of failure)  </li> <li>Dependencies: Database servers before application servers</li> </ol> <p>The result: A logical rebalancing that avoided \"all our eggs in Sunday 1AM\" basket and considered business impact.</p> <p>Why this matters: The current patching schedule was not optimized for business continuity. AI helped identify risks we hadn't considered.</p>","tags":["Azure Update Manager","PowerShell","Azure Policy","VM Patching","FinOps"]},{"location":"blog/bringing-patch-management-in-house-migrating-from-msp-to-azure-update-manager/#step-5-discover-all-vms-and-identify-gaps","title":"\ud83d\udd0d Step 5 \u2013 Discover All VMs and Identify Gaps","text":"<p>Before diving into bulk tagging, I needed to understand what we were working with across all subscriptions.</p> <p>First, let's see what VMs we have:</p> Click to expand: Discover Untagged VMs (Sample Script) <pre><code># Discover Untagged VMs Script for Azure Update Manager\n# This script identifies VMs that are missing Azure Update Manager tags\n\n$scriptStart = Get-Date\n\nWrite-Host \"=== Azure Update Manager - Discover Untagged VMs ===\" -ForegroundColor Cyan\nWrite-Host \"Scanning all accessible subscriptions for VMs missing maintenance tags...\" -ForegroundColor White\nWrite-Host \"\"\n\n# Function to check if VM has Azure Update Manager tags\nfunction Test-VMHasMaintenanceTags {\n    param($VM)\n\n    # Check for the three required tags\n    $hasOwnerTag = $VM.Tags -and $VM.Tags.ContainsKey(\"Owner\") -and $VM.Tags[\"Owner\"] -eq \"Contoso\"\n    $hasUpdatesTag = $VM.Tags -and $VM.Tags.ContainsKey(\"Updates\") -and $VM.Tags[\"Updates\"] -eq \"Azure Update Manager\"\n    $hasPatchWindowTag = $VM.Tags -and $VM.Tags.ContainsKey(\"PatchWindow\")\n\n    return $hasOwnerTag -and $hasUpdatesTag -and $hasPatchWindowTag\n}\n\n# Function to get VM details for reporting\nfunction Get-VMDetails {\n    param($VM, $SubscriptionName)\n\n    return [PSCustomObject]@{\n        Name = $VM.Name\n        ResourceGroup = $VM.ResourceGroupName\n        Location = $VM.Location\n        Subscription = $SubscriptionName\n        SubscriptionId = $VM.SubscriptionId\n        PowerState = $VM.PowerState\n        OsType = $VM.StorageProfile.OsDisk.OsType\n        VmSize = $VM.HardwareProfile.VmSize\n        Tags = if ($VM.Tags) { ($VM.Tags.Keys | ForEach-Object { \"$_=$($VM.Tags[$_])\" }) -join \"; \" } else { \"No tags\" }\n    }\n}\n\n# Initialize collections\n$taggedVMs = @()\n$untaggedVMs = @()\n$allVMs = @()\n$subscriptionSummary = @{}\n\nWrite-Host \"=== DISCOVERING VMs ACROSS ALL SUBSCRIPTIONS ===\" -ForegroundColor Cyan\n\n# Get all accessible subscriptions\n$subscriptions = Get-AzSubscription | Where-Object { $_.State -eq \"Enabled\" }\nWrite-Host \"Found $($subscriptions.Count) accessible subscriptions\" -ForegroundColor White\n\nforeach ($subscription in $subscriptions) {\n    try {\n        Write-Host \"`nScanning subscription: $($subscription.Name) ($($subscription.Id))\" -ForegroundColor Magenta\n        $null = Set-AzContext -SubscriptionId $subscription.Id -ErrorAction Stop\n\n        # Get all VMs in this subscription\n        Write-Host \"  Retrieving VMs...\" -ForegroundColor Gray\n        $vms = Get-AzVM -Status -ErrorAction Continue\n\n        $subTagged = 0\n        $subUntagged = 0\n        $subTotal = $vms.Count\n\n        Write-Host \"  Found $subTotal VMs in this subscription\" -ForegroundColor White\n\n        foreach ($vm in $vms) {\n            $vmDetails = Get-VMDetails -VM $vm -SubscriptionName $subscription.Name\n            $allVMs += $vmDetails\n\n            if (Test-VMHasMaintenanceTags -VM $vm) {\n                $taggedVMs += $vmDetails\n                $subTagged++\n                Write-Host \"    \u2713 Tagged: $($vm.Name)\" -ForegroundColor Green\n            } else {\n                $untaggedVMs += $vmDetails\n                $subUntagged++\n                Write-Host \"    \u26a0\ufe0f Untagged: $($vm.Name)\" -ForegroundColor Yellow\n            }\n        }\n\n        # Store subscription summary\n        $subscriptionSummary[$subscription.Name] = @{\n            Total = $subTotal\n            Tagged = $subTagged\n            Untagged = $subUntagged\n            SubscriptionId = $subscription.Id\n        }\n\n        Write-Host \"  Subscription Summary - Total: $subTotal | Tagged: $subTagged | Untagged: $subUntagged\" -ForegroundColor Gray\n\n    }\n    catch {\n        Write-Host \"  \u2717 Error scanning subscription $($subscription.Name): $($_.Exception.Message)\" -ForegroundColor Red\n        $subscriptionSummary[$subscription.Name] = @{\n            Total = 0\n            Tagged = 0\n            Untagged = 0\n            Error = $_.Exception.Message\n        }\n    }\n}\n\nWrite-Host \"\"\nWrite-Host \"=== OVERALL DISCOVERY SUMMARY ===\" -ForegroundColor Cyan\nWrite-Host \"Total VMs found: $($allVMs.Count)\" -ForegroundColor White\nWrite-Host \"VMs with maintenance tags: $($taggedVMs.Count)\" -ForegroundColor Green\nWrite-Host \"VMs missing maintenance tags: $($untaggedVMs.Count)\" -ForegroundColor Red\n\nif ($untaggedVMs.Count -eq 0) {\n    Write-Host \"\ufffd ALL VMs ARE ALREADY TAGGED! \ufffd\" -ForegroundColor Green\n    Write-Host \"No further action required.\" -ForegroundColor White\n    exit 0\n}\n\nWrite-Host \"\"\nWrite-Host \"=== SUBSCRIPTION BREAKDOWN ===\" -ForegroundColor Cyan\n$subscriptionSummary.GetEnumerator() | Sort-Object Name | ForEach-Object {\n    $sub = $_.Value\n    if ($sub.Error) {\n        Write-Host \"$($_.Key): ERROR - $($sub.Error)\" -ForegroundColor Red\n    } else {\n        $percentage = if ($sub.Total -gt 0) { [math]::Round(($sub.Tagged / $sub.Total) * 100, 1) } else { 0 }\n        Write-Host \"$($_.Key): $($sub.Tagged)/$($sub.Total) tagged ($percentage%)\" -ForegroundColor White\n    }\n}\n\nWrite-Host \"\"\nWrite-Host \"=== UNTAGGED VMs DETAILED LIST ===\" -ForegroundColor Red\nWrite-Host \"The following $($untaggedVMs.Count) VMs are missing Azure Update Manager maintenance tags:\" -ForegroundColor White\n\n# Group untagged VMs by subscription for easier reading\n$untaggedBySubscription = $untaggedVMs | Group-Object Subscription\n\nforeach ($group in $untaggedBySubscription | Sort-Object Name) {\n    Write-Host \"`n\ufffd Subscription: $($group.Name) ($($group.Count) untagged VMs)\" -ForegroundColor Magenta\n\n    $group.Group | Sort-Object Name | ForEach-Object {\n        Write-Host \"  \u2022 $($_.Name)\" -ForegroundColor Yellow\n        Write-Host \"    Resource Group: $($_.ResourceGroup)\" -ForegroundColor Gray\n        Write-Host \"    Location: $($_.Location)\" -ForegroundColor Gray\n        Write-Host \"    OS Type: $($_.OsType)\" -ForegroundColor Gray\n        Write-Host \"    VM Size: $($_.VmSize)\" -ForegroundColor Gray\n        Write-Host \"    Power State: $($_.PowerState)\" -ForegroundColor Gray\n        if ($_.Tags -ne \"No tags\") {\n            Write-Host \"    Existing Tags: $($_.Tags)\" -ForegroundColor DarkGray\n        }\n        Write-Host \"\"\n    }\n}\n\nWrite-Host \"=== ANALYSIS BY VM CHARACTERISTICS ===\" -ForegroundColor Cyan\n\n# Analyze by OS Type\n$untaggedByOS = $untaggedVMs | Group-Object OsType\nWrite-Host \"`n\ufffd Untagged VMs by OS Type:\" -ForegroundColor White\n$untaggedByOS | Sort-Object Name | ForEach-Object {\n    Write-Host \"  $($_.Name): $($_.Count) VMs\" -ForegroundColor White\n}\n\n# Analyze by Location\n$untaggedByLocation = $untaggedVMs | Group-Object Location\nWrite-Host \"`n\ufffd Untagged VMs by Location:\" -ForegroundColor White\n$untaggedByLocation | Sort-Object Count -Descending | ForEach-Object {\n    Write-Host \"  $($_.Name): $($_.Count) VMs\" -ForegroundColor White\n}\n\n# Analyze by VM Size (to understand workload types)\n$untaggedBySize = $untaggedVMs | Group-Object VmSize\nWrite-Host \"`n\ufffd Untagged VMs by Size:\" -ForegroundColor White\n$untaggedBySize | Sort-Object Count -Descending | Select-Object -First 10 | ForEach-Object {\n    Write-Host \"  $($_.Name): $($_.Count) VMs\" -ForegroundColor White\n}\n\n# Analyze by Resource Group (might indicate application/workload groupings)\n$untaggedByRG = $untaggedVMs | Group-Object ResourceGroup\nWrite-Host \"`n\ufffd Untagged VMs by Resource Group (Top 10):\" -ForegroundColor White\n$untaggedByRG | Sort-Object Count -Descending | Select-Object -First 10 | ForEach-Object {\n    Write-Host \"  $($_.Name): $($_.Count) VMs\" -ForegroundColor White\n}\n\nWrite-Host \"\"\nWrite-Host \"=== POWER STATE ANALYSIS ===\" -ForegroundColor Cyan\n$powerStates = $untaggedVMs | Group-Object PowerState\n$powerStates | Sort-Object Count -Descending | ForEach-Object {\n    Write-Host \"$($_.Name): $($_.Count) VMs\" -ForegroundColor White\n}\n\nWrite-Host \"\"\nWrite-Host \"=== EXPORT OPTIONS ===\" -ForegroundColor Cyan\nWrite-Host \"You can export this data for further analysis:\" -ForegroundColor White\n\n# Export to CSV option\n$timestamp = Get-Date -Format \"yyyyMMdd-HHmm\"\n$csvPath = \"D:\\UntaggedVMs-$timestamp.csv\"\n\ntry {\n    $untaggedVMs | Export-Csv -Path $csvPath -NoTypeInformation\n    Write-Host \"\u2713 Exported untagged VMs to: $csvPath\" -ForegroundColor Green\n} catch {\n    Write-Host \"\u2717 Failed to export CSV: $($_.Exception.Message)\" -ForegroundColor Red\n}\n\n# Show simple list for easy copying\nWrite-Host \"\"\nWrite-Host \"=== SIMPLE VM NAME LIST (for copy/paste) ===\" -ForegroundColor Cyan\nWrite-Host \"VM Names:\" -ForegroundColor White\n$untaggedVMs | Sort-Object Name | ForEach-Object { Write-Host \"  $($_.Name)\" -ForegroundColor Yellow }\n\nWrite-Host \"\"\nWrite-Host \"=== NEXT STEPS RECOMMENDATIONS ===\" -ForegroundColor Cyan\nWrite-Host \"1. Review the untagged VMs list above\" -ForegroundColor White\nWrite-Host \"2. Investigate why these VMs were not in the original patching schedule\" -ForegroundColor White\nWrite-Host \"3. Determine appropriate maintenance windows for these VMs\" -ForegroundColor White\nWrite-Host \"4. Consider grouping by:\" -ForegroundColor White\nWrite-Host \"   \u2022 Application/workload (Resource Group analysis)\" -ForegroundColor Gray\nWrite-Host \"   \u2022 Environment (naming patterns, tags)\" -ForegroundColor Gray\nWrite-Host \"   \u2022 Business criticality\" -ForegroundColor Gray\nWrite-Host \"   \u2022 Maintenance window preferences\" -ForegroundColor Gray\nWrite-Host \"5. Run the tagging script to assign maintenance windows\" -ForegroundColor White\n\nWrite-Host \"\"\nWrite-Host \"=== AZURE RESOURCE GRAPH QUERY ===\" -ForegroundColor Cyan\nWrite-Host \"Use this query in Azure Resource Graph Explorer to verify results:\" -ForegroundColor White\nWrite-Host \"\"\nWrite-Host @\"\nResources\n| where type == \"microsoft.compute/virtualmachines\"\n| where tags.PatchWindow == \"\" or isempty(tags.PatchWindow) or isnull(tags.PatchWindow)\n| project name, resourceGroup, subscriptionId, location, \n          osType = properties.storageProfile.osDisk.osType,\n          vmSize = properties.hardwareProfile.vmSize,\n          powerState = properties.extended.instanceView.powerState.displayStatus,\n          tags\n| sort by name asc\n\"@ -ForegroundColor Gray\n\nWrite-Host \"\"\nWrite-Host \"Script completed at $(Get-Date)\" -ForegroundColor Cyan\nWrite-Host \"Total runtime: $((Get-Date) - $scriptStart)\" -ForegroundColor Gray\n</code></pre> <p>Discovery results:</p> <ul> <li>35 VMs from the original MSP schedule (our planned list)</li> <li>12 additional VMs not in the MSP schedule (the \"stragglers\")</li> <li>Total: 90 VMs needing Update Manager tags</li> </ul> <p>Key insight: The MSP wasn't managing everything. Several dev/test VMs and a few production systems were missing from their schedule.</p>","tags":["Azure Update Manager","PowerShell","Azure Policy","VM Patching","FinOps"]},{"location":"blog/bringing-patch-management-in-house-migrating-from-msp-to-azure-update-manager/#step-6-bulk-tag-all-vms-with-patch-windows","title":"\u270d\ufe0f Step 6 \u2013 Bulk Tag All VMs with Patch Windows","text":"<p>Now for the main event: tagging all VMs with their maintenance windows. This includes both our planned VMs and the newly discovered ones.</p>","tags":["Azure Update Manager","PowerShell","Azure Policy","VM Patching","FinOps"]},{"location":"blog/bringing-patch-management-in-house-migrating-from-msp-to-azure-update-manager/#main-vm-tagging-planned-schedule","title":"\ud83c\udfaf Main VM Tagging (Planned Schedule)","text":"<p>Each tag serves a specific purpose:</p> <ul> <li><code>PatchWindow</code> \u2014 The key tag used by dynamic scopes to assign VMs to maintenance configurations</li> <li><code>Owner</code> \u2014 For accountability and filtering</li> <li><code>Updates</code> \u2014 Identifies VMs managed by Azure Update Manager</li> </ul> Click to expand: Multi-Subscription Azure Update Manager VM Tagging (Sample Script) <pre><code># Multi-Subscription Azure Update Manager VM Tagging Script\n# This script discovers VMs across multiple subscriptions and tags them appropriately\n\nWrite-Host \"=== Multi-Subscription Azure Update Manager - VM Tagging Script ===\" -ForegroundColor Cyan\n\n# Function to safely tag a VM\nfunction Set-VMMaintenanceTags {\n    param(\n        [string]$VMName,\n        [string]$ResourceGroupName,\n        [string]$SubscriptionId,\n        [hashtable]$Tags,\n        [string]$MaintenanceWindow\n    )\n\n    try {\n        # Set context to the VM's subscription\n        $null = Set-AzContext -SubscriptionId $SubscriptionId -ErrorAction Stop\n\n        Write-Host \"  Processing: $VMName...\" -ForegroundColor Yellow\n\n        # Get the VM and update tags\n        $vm = Get-AzVM -ResourceGroupName $ResourceGroupName -Name $VMName -ErrorAction Stop\n\n        if ($vm.Tags) {\n            $Tags.Keys | ForEach-Object { $vm.Tags[$_] = $Tags[$_] }\n        } else {\n            $vm.Tags = $Tags\n        }\n\n        $null = Update-AzVM -VM $vm -ResourceGroupName $ResourceGroupName -Tag $vm.Tags -ErrorAction Stop\n        Write-Host \"  \u2713 Successfully tagged $VMName for $MaintenanceWindow maintenance\" -ForegroundColor Green\n\n        return $true\n    }\n    catch {\n        Write-Host \"  \u2717 Failed to tag $VMName`: $($_.Exception.Message)\" -ForegroundColor Red\n        return $false\n    }\n}\n\n# Define all target VMs organized by maintenance window\n$maintenanceGroups = @{\n    \"Monday\" = @{\n        \"VMs\" = @(\"WEB-PROD-01\", \"DB-PROD-01\", \"APP-PROD-01\", \"FILE-PROD-01\", \"DC-PROD-01\")\n        \"Tags\" = @{\n            \"Owner\" = \"Contoso\"\n            \"Updates\" = \"Azure Update Manager\"\n            \"PatchWindow\" = \"mon\"\n        }\n    }\n    \"Tuesday\" = @{\n        \"VMs\" = @(\"WEB-PROD-02\", \"DB-PROD-02\", \"APP-PROD-02\", \"FILE-PROD-02\", \"DC-PROD-02\")\n        \"Tags\" = @{\n            \"Owner\" = \"Contoso\"\n            \"Updates\" = \"Azure Update Manager\"\n            \"PatchWindow\" = \"tue\"\n        }\n    }\n    \"Wednesday\" = @{\n        \"VMs\" = @(\"WEB-PROD-03\", \"DB-PROD-03\", \"APP-PROD-03\", \"FILE-PROD-03\", \"DC-PROD-03\")\n        \"Tags\" = @{\n            \"Owner\" = \"Contoso\"\n            \"Updates\" = \"Azure Update Manager\"\n            \"PatchWindow\" = \"wed\"\n        }\n    }\n    \"Thursday\" = @{\n        \"VMs\" = @(\"WEB-PROD-04\", \"DB-PROD-04\", \"APP-PROD-04\", \"FILE-PROD-04\", \"PRINT-PROD-01\")\n        \"Tags\" = @{\n            \"Owner\" = \"Contoso\"\n            \"Updates\" = \"Azure Update Manager\"\n            \"PatchWindow\" = \"thu\"\n        }\n    }\n    \"Friday\" = @{\n        \"VMs\" = @(\"WEB-PROD-05\", \"DB-PROD-05\", \"APP-PROD-05\", \"FILE-PROD-05\", \"MONITOR-PROD-01\")\n        \"Tags\" = @{\n            \"Owner\" = \"Contoso\"\n            \"Updates\" = \"Azure Update Manager\"\n            \"PatchWindow\" = \"fri\"\n        }\n    }\n    \"Saturday\" = @{\n        \"VMs\" = @(\"WEB-DEV-01\", \"DB-DEV-01\", \"APP-DEV-01\", \"TEST-SERVER-01\", \"SANDBOX-01\")\n        \"Tags\" = @{\n            \"Owner\" = \"Contoso\"\n            \"Updates\" = \"Azure Update Manager\"\n            \"PatchWindow\" = \"sat-09\"\n        }\n    }\n    \"Sunday\" = @{\n        \"VMs\" = @(\"WEB-UAT-01\", \"DB-UAT-01\", \"APP-UAT-01\", \"BACKUP-PROD-01\", \"MGMT-PROD-01\")\n        \"Tags\" = @{\n            \"Owner\" = \"Contoso\"\n            \"Updates\" = \"Azure Update Manager\"\n            \"PatchWindow\" = \"sun\"\n        }\n    }\n}\n\n# Function to discover VMs across all subscriptions\nfunction Find-VMsAcrossSubscriptions {\n    param([array]$TargetVMNames)\n\n    $subscriptions = Get-AzSubscription | Where-Object { $_.State -eq \"Enabled\" }\n    $vmInventory = @{}\n\n    foreach ($subscription in $subscriptions) {\n        try {\n            $null = Set-AzContext -SubscriptionId $subscription.Id -ErrorAction Stop\n            $vms = Get-AzVM -ErrorAction Continue\n\n            foreach ($vm in $vms) {\n                if ($vm.Name -in $TargetVMNames) {\n                    $vmInventory[$vm.Name] = @{\n                        Name = $vm.Name\n                        ResourceGroupName = $vm.ResourceGroupName\n                        SubscriptionId = $subscription.Id\n                        SubscriptionName = $subscription.Name\n                        Location = $vm.Location\n                    }\n                }\n            }\n        }\n        catch {\n            Write-Host \"Error scanning subscription $($subscription.Name): $($_.Exception.Message)\" -ForegroundColor Red\n        }\n    }\n\n    return $vmInventory\n}\n\n# Get all unique VM names and discover their locations\n$allTargetVMs = @()\n$maintenanceGroups.Values | ForEach-Object { $allTargetVMs += $_.VMs }\n$allTargetVMs = $allTargetVMs | Sort-Object -Unique\n\nWrite-Host \"Discovering locations for $($allTargetVMs.Count) target VMs...\" -ForegroundColor White\n$vmInventory = Find-VMsAcrossSubscriptions -TargetVMNames $allTargetVMs\n\n# Process each maintenance window\n$totalSuccess = 0\n$totalFailed = 0\n\nforeach ($windowName in $maintenanceGroups.Keys) {\n    $group = $maintenanceGroups[$windowName]\n    Write-Host \"`n=== $windowName MAINTENANCE WINDOW ===\" -ForegroundColor Magenta\n\n    foreach ($vmName in $group.VMs) {\n        if ($vmInventory.ContainsKey($vmName)) {\n            $vmInfo = $vmInventory[$vmName]\n            $result = Set-VMMaintenanceTags -VMName $vmInfo.Name -ResourceGroupName $vmInfo.ResourceGroupName -SubscriptionId $vmInfo.SubscriptionId -Tags $group.Tags -MaintenanceWindow $windowName\n            if ($result) { $totalSuccess++ } else { $totalFailed++ }\n        } else {\n            Write-Host \"  \u26a0\ufe0f VM not found: $vmName\" -ForegroundColor Yellow\n            $totalFailed++\n        }\n    }\n}\n\nWrite-Host \"`n=== TAGGING SUMMARY ===\" -ForegroundColor Cyan\nWrite-Host \"Successfully tagged: $totalSuccess VMs\" -ForegroundColor Green\nWrite-Host \"Failed to tag: $totalFailed VMs\" -ForegroundColor Red\n</code></pre>","tags":["Azure Update Manager","PowerShell","Azure Policy","VM Patching","FinOps"]},{"location":"blog/bringing-patch-management-in-house-migrating-from-msp-to-azure-update-manager/#handle-the-stragglers","title":"\ud83e\uddf9 Handle the Stragglers","text":"<p>For the 12 VMs not in the original MSP schedule, I used intelligent assignment based on their function:</p> Click to expand: Tagging Script for Remaining Untagged VMs (Sample Script) <pre><code># Intelligent VM Tagging Script for Remaining Untagged VMs\n# This script analyzes and tags the remaining VMs based on workload patterns and load balancing\n\n$scriptStart = Get-Date\n\nWrite-Host \"=== Intelligent VM Tagging for Remaining VMs ===\" -ForegroundColor Cyan\nWrite-Host \"Analyzing and tagging 26 untagged VMs with optimal maintenance window distribution...\" -ForegroundColor White\nWrite-Host \"\"\n\n# Function to safely tag a VM across subscriptions\nfunction Set-VMMaintenanceTags {\n    param(\n        [string]$VMName,\n        [string]$ResourceGroupName,\n        [string]$SubscriptionId,\n        [hashtable]$Tags,\n        [string]$MaintenanceWindow\n    )\n\n    try {\n        # Set context to the VM's subscription\n        $currentContext = Get-AzContext\n        if ($currentContext.Subscription.Id -ne $SubscriptionId) {\n            $null = Set-AzContext -SubscriptionId $SubscriptionId -ErrorAction Stop\n        }\n\n        Write-Host \"  Processing: $VMName...\" -ForegroundColor Yellow\n\n        # Get the VM\n        $vm = Get-AzVM -ResourceGroupName $ResourceGroupName -Name $VMName -ErrorAction Stop\n\n        # Add maintenance tags to existing tags (preserve existing tags)\n        if ($vm.Tags) {\n            $Tags.Keys | ForEach-Object {\n                $vm.Tags[$_] = $Tags[$_]\n            }\n        } else {\n            $vm.Tags = $Tags\n        }\n\n        # Update the VM tags\n        $null = Update-AzVM -VM $vm -ResourceGroupName $ResourceGroupName -Tag $vm.Tags -ErrorAction Stop\n        Write-Host \"  \u2713 Successfully tagged $VMName for $MaintenanceWindow maintenance\" -ForegroundColor Green\n\n        return $true\n    }\n    catch {\n        Write-Host \"  \u2717 Failed to tag $VMName`: $($_.Exception.Message)\" -ForegroundColor Red\n        return $false\n    }\n}\n\n# Define current maintenance window loads (after existing 59 VMs)\n$currentLoad = @{\n    \"Monday\" = 7\n    \"Tuesday\" = 7 \n    \"Wednesday\" = 10\n    \"Thursday\" = 6\n    \"Friday\" = 6\n    \"Saturday\" = 17  # Dev/Test at 09:00\n    \"Sunday\" = 6\n}\n\nWrite-Host \"=== CURRENT MAINTENANCE WINDOW LOAD ===\" -ForegroundColor Cyan\n$currentLoad.GetEnumerator() | Sort-Object Name | ForEach-Object {\n    Write-Host \"$($_.Key): $($_.Value) VMs\" -ForegroundColor White\n}\n\n# Initialize counters for new assignments\n$newAssignments = @{\n    \"Monday\" = 0\n    \"Tuesday\" = 0\n    \"Wednesday\" = 0\n    \"Thursday\" = 0\n    \"Friday\" = 0\n    \"Saturday\" = 0  # Will use sat-09 for dev/test\n    \"Sunday\" = 0\n}\n\nWrite-Host \"\"\nWrite-Host \"=== INTELLIGENT VM GROUPING AND ASSIGNMENT ===\" -ForegroundColor Cyan\n\n# Define VM groups with intelligent maintenance window assignments\n$vmGroups = @{\n\n    # CRITICAL PRODUCTION SYSTEMS - Spread across different days\n    \"Critical Infrastructure\" = @{\n        \"VMs\" = @(\n            @{ Name = \"DC-PROD-01\"; RG = \"rg-infrastructure\"; Sub = \"Production\"; Window = \"Sunday\"; Reason = \"Domain Controller - critical infrastructure\" },\n            @{ Name = \"DC-PROD-02\"; RG = \"rg-infrastructure\"; Sub = \"Production\"; Window = \"Monday\"; Reason = \"Domain Controller - spread from other DCs\" },\n            @{ Name = \"BACKUP-PROD-01\"; RG = \"rg-backup\"; Sub = \"Production\"; Window = \"Tuesday\"; Reason = \"Backup Server - spread across week\" }\n        )\n    }\n\n    # PRODUCTION BUSINESS APPLICATIONS - Spread for business continuity\n    \"Production Applications\" = @{\n        \"VMs\" = @(\n            @{ Name = \"WEB-PROD-01\"; RG = \"rg-web-production\"; Sub = \"Production\"; Window = \"Monday\"; Reason = \"Web Server - Monday for week start\" },\n            @{ Name = \"DB-PROD-01\"; RG = \"rg-database-production\"; Sub = \"Production\"; Window = \"Tuesday\"; Reason = \"Database Server - Tuesday\" },\n            @{ Name = \"APP-PROD-01\"; RG = \"rg-app-production\"; Sub = \"Production\"; Window = \"Wednesday\"; Reason = \"Application Server - mid-week\" }\n        )\n    }\n\n    # DEV/TEST SYSTEMS - Saturday morning maintenance (like existing dev/test)\n    \"Development Systems\" = @{\n        \"VMs\" = @(\n            @{ Name = \"WEB-DEV-01\"; RG = \"rg-web-development\"; Sub = \"Development\"; Window = \"Saturday\"; Reason = \"Web Dev - join existing dev/test window\" },\n            @{ Name = \"DB-DEV-01\"; RG = \"rg-database-development\"; Sub = \"Development\"; Window = \"Saturday\"; Reason = \"Database Dev - join existing dev/test window\" },\n            @{ Name = \"TEST-SERVER-01\"; RG = \"rg-testing\"; Sub = \"Development\"; Window = \"Saturday\"; Reason = \"Test Server - join existing dev/test window\" }\n            # ... additional dev/test VMs\n        )\n    }\n}\n\n# Initialize counters\n$totalProcessed = 0\n$totalSuccess = 0\n$totalFailed = 0\n\n# Process each group\nforeach ($groupName in $vmGroups.Keys) {\n    $group = $vmGroups[$groupName]\n    Write-Host \"`n=== $groupName ===\" -ForegroundColor Magenta\n    Write-Host \"Processing $($group.VMs.Count) VMs in this group\" -ForegroundColor White\n\n    foreach ($vmInfo in $group.VMs) {\n        $window = $vmInfo.Window\n        $vmName = $vmInfo.Name\n\n        Write-Host \"`n\ufffd\ufe0f $vmName \u2192 $window maintenance window\" -ForegroundColor Yellow\n        Write-Host \"   Reason: $($vmInfo.Reason)\" -ForegroundColor Gray\n\n        # Determine subscription ID from name\n        $subscriptionId = switch ($vmInfo.Sub) {\n            \"Production\" { (Get-AzSubscription -SubscriptionName \"Production\").Id }\n            \"DevTest\" { (Get-AzSubscription -SubscriptionName \"DevTest\").Id }\n            \"Identity\" { (Get-AzSubscription -SubscriptionName \"Identity\").Id }\n            \"DMZ\" { (Get-AzSubscription -SubscriptionName \"DMZ\").Id }\n        }\n\n        # Create appropriate tags based on maintenance window\n        $tags = @{\n            \"Owner\" = \"Contoso\"\n            \"Updates\" = \"Azure Update Manager\"\n        }\n\n        if ($window -eq \"Saturday\") {\n            $tags[\"PatchWindow\"] = \"sat-09\"  # Saturday 09:00 for dev/test\n        } else {\n            $tags[\"PatchWindow\"] = $window.ToLower().Substring(0,3)  # mon, tue, wed, etc.\n        }\n\n        $result = Set-VMMaintenanceTags -VMName $vmInfo.Name -ResourceGroupName $vmInfo.RG -SubscriptionId $subscriptionId -Tags $tags -MaintenanceWindow $window\n\n        $totalProcessed++\n        if ($result) { \n            $totalSuccess++\n            $newAssignments[$window]++\n        } else { \n            $totalFailed++ \n        }\n    }\n}\n\nWrite-Host \"\"\nWrite-Host \"=== TAGGING SUMMARY ===\" -ForegroundColor Cyan\nWrite-Host \"Total VMs processed: $totalProcessed\" -ForegroundColor White\nWrite-Host \"Successfully tagged: $totalSuccess\" -ForegroundColor Green\nWrite-Host \"Failed to tag: $totalFailed\" -ForegroundColor Red\n\nWrite-Host \"\"\nWrite-Host \"=== NEW MAINTENANCE WINDOW DISTRIBUTION ===\" -ForegroundColor Cyan\nWrite-Host \"VMs added to each maintenance window:\" -ForegroundColor White\n\n$newAssignments.GetEnumerator() | Sort-Object Name | ForEach-Object {\n    if ($_.Value -gt 0) {\n        $newTotal = $currentLoad[$_.Key] + $_.Value\n        Write-Host \"$($_.Key): +$($_.Value) VMs (total: $newTotal VMs)\" -ForegroundColor Green\n    }\n}\n\nWrite-Host \"\"\nWrite-Host \"=== FINAL MAINTENANCE WINDOW LOAD ===\" -ForegroundColor Cyan\n$finalLoad = @{}\n$currentLoad.Keys | ForEach-Object {\n    $finalLoad[$_] = $currentLoad[$_] + $newAssignments[$_]\n}\n\n$finalLoad.GetEnumerator() | Sort-Object Name | ForEach-Object {\n    $status = if ($_.Value -le 8) { \"Green\" } elseif ($_.Value -le 12) { \"Yellow\" } else { \"Red\" }\n    Write-Host \"$($_.Key): $($_.Value) VMs\" -ForegroundColor $status\n}\n\n$grandTotal = ($finalLoad.Values | Measure-Object -Sum).Sum\nWrite-Host \"`nGrand Total: $grandTotal VMs across all maintenance windows\" -ForegroundColor White\n\nWrite-Host \"\"\nWrite-Host \"=== BUSINESS LOGIC APPLIED ===\" -ForegroundColor Cyan\nWrite-Host \"\u2705 Critical systems spread across different days for resilience\" -ForegroundColor Green\nWrite-Host \"\u2705 Domain Controllers distributed to avoid single points of failure\" -ForegroundColor Green\nWrite-Host \"\u2705 Dev/Test systems consolidated to Saturday morning (existing pattern)\" -ForegroundColor Green\nWrite-Host \"\u2705 Production workstations spread to minimize user impact\" -ForegroundColor Green\nWrite-Host \"\u2705 Business applications distributed for operational continuity\" -ForegroundColor Green\nWrite-Host \"\u2705 Load balancing maintained across the week\" -ForegroundColor Green\n\nWrite-Host \"\"\nWrite-Host \"=== VERIFICATION STEPS ===\" -ForegroundColor Cyan\nWrite-Host \"1. Verify tags in Azure Portal across all subscriptions\" -ForegroundColor White\nWrite-Host \"2. Check that critical systems are on different days\" -ForegroundColor White\nWrite-Host \"3. Confirm dev/test systems are in Saturday morning window\" -ForegroundColor White\nWrite-Host \"4. Review production systems distribution\" -ForegroundColor White\n\nWrite-Host \"\"\nWrite-Host \"=== AZURE RESOURCE GRAPH VERIFICATION QUERY ===\" -ForegroundColor Cyan\nWrite-Host \"Use this query to verify all VMs are now tagged:\" -ForegroundColor White\nWrite-Host \"\"\nWrite-Host @\"\nResources\n| where type == \"microsoft.compute/virtualmachines\"\n| where tags.Updates == \"Azure Update Manager\"\n| project name, resourceGroup, subscriptionId, \n          patchWindow = tags.PatchWindow,\n          owner = tags.Owner,\n          updates = tags.Updates\n| sort by patchWindow, name\n| summarize count() by patchWindow\n\"@ -ForegroundColor Gray\n\nif ($totalFailed -eq 0) {\n    Write-Host \"\"\n    Write-Host \"\ufffd ALL VMs SUCCESSFULLY TAGGED WITH INTELLIGENT DISTRIBUTION! \ufffd\" -ForegroundColor Green\n} else {\n    Write-Host \"\"\n    Write-Host \"\u26a0\ufe0f Some VMs failed to tag. Please review errors above.\" -ForegroundColor Yellow\n}\n\nWrite-Host \"\"\nWrite-Host \"Script completed at $(Get-Date)\" -ForegroundColor Cyan\nWrite-Host \"Total runtime: $((Get-Date) - $scriptStart)\" -ForegroundColor Gray\n</code></pre> <p>Key insight: I grouped VMs by function and criticality, not just by convenience. Domain controllers got spread across different days, dev/test systems joined the existing Saturday morning window, and production applications were distributed for business continuity.</p>","tags":["Azure Update Manager","PowerShell","Azure Policy","VM Patching","FinOps"]},{"location":"blog/bringing-patch-management-in-house-migrating-from-msp-to-azure-update-manager/#step-7-configure-azure-policy-prerequisites","title":"\ud83e\uddf0 Step 7 \u2013 Configure Azure Policy Prerequisites","text":"<p>Here's where things get interesting. Update Manager is built on compliance \u2014 but your VMs won't show up in dynamic scopes unless they meet certain prerequisites. Enter Azure Policy to save the day.</p> <p>You'll need two specific built-in policies assigned at the subscription (or management group) level:</p>","tags":["Azure Update Manager","PowerShell","Azure Policy","VM Patching","FinOps"]},{"location":"blog/bringing-patch-management-in-house-migrating-from-msp-to-azure-update-manager/#policy-1-set-prerequisites-for-scheduling-recurring-updates-on-azure-virtual-machines","title":"\u2705 Policy 1: <code>Set prerequisites for scheduling recurring updates on Azure virtual machines</code>","text":"<p>What it does: This policy ensures your VMs have the necessary configurations to participate in Azure Update Manager. It automatically:</p> <ul> <li>Installs the Azure Update Manager extension on Windows VMs</li> <li>Registers required resource providers</li> <li>Configures the VM to report its update compliance status</li> <li>Sets the patch orchestration mode appropriately</li> </ul> <p>Why this matters: Without this policy, VMs won't appear in Update Manager scopes even if they're tagged correctly. The policy handles all the \"plumbing\" automatically.</p> <p>Assignment scope: Apply this at subscription or management group level to catch all VMs.</p>","tags":["Azure Update Manager","PowerShell","Azure Policy","VM Patching","FinOps"]},{"location":"blog/bringing-patch-management-in-house-migrating-from-msp-to-azure-update-manager/#policy-2-configure-periodic-checking-for-missing-system-updates-on-azure-virtual-machines","title":"\u2705 Policy 2: <code>Configure periodic checking for missing system updates on Azure virtual machines</code>","text":"<p>What it does: This is your compliance engine. It configures VMs to:</p> <ul> <li>Regularly scan for available updates (but not install them automatically)</li> <li>Report update status back to Azure Update Manager</li> <li>Enable the compliance dashboard views in the portal</li> <li>Provide the data needed for maintenance configuration targeting</li> </ul> <p>Why this matters: This policy turns on the \"update awareness\" for your VMs. Without it, Azure Update Manager has no visibility into what patches are needed.</p> <p>Assignment scope: Same as above \u2014 subscription or management group level.</p>","tags":["Azure Update Manager","PowerShell","Azure Policy","VM Patching","FinOps"]},{"location":"blog/bringing-patch-management-in-house-migrating-from-msp-to-azure-update-manager/#assigning-the-policies","title":"\ud83c\udfaf Assigning the Policies","text":"<p>Step-by-step in Azure Portal:</p> <ol> <li>Navigate to Azure Policy</li> <li> <p>Azure Portal \u2192 Search \"Policy\" \u2192 Select \"Policy\"</p> </li> <li> <p>Find the First Policy</p> </li> <li>Left menu: Definitions</li> <li>Search: <code>Set prerequisites for scheduling recurring updates</code></li> <li> <p>Click on the policy title</p> </li> <li> <p>Assign the Policy</p> </li> <li>Click Assign button</li> <li>Scope: Select your subscription(s)</li> <li>Basics: Leave policy name as default</li> <li>Parameters: Leave as default</li> <li>Remediation: \u2705 Check \"Create remediation task\"</li> <li> <p>Review + create</p> </li> <li> <p>Repeat for Second Policy</p> </li> <li>Search: <code>Configure periodic checking for missing system updates</code></li> <li>Follow same assignment process</li> </ol> <p>\u26a0\ufe0f Important: Policy compliance can take 30+ minutes to evaluate and apply. Perfect time for that brew I mentioned earlier.</p>","tags":["Azure Update Manager","PowerShell","Azure Policy","VM Patching","FinOps"]},{"location":"blog/bringing-patch-management-in-house-migrating-from-msp-to-azure-update-manager/#monitoring-compliance","title":"\ud83d\udd0d Monitoring Compliance","text":"<p>Once assigned, you can track compliance in Azure Policy &gt; Compliance. Look for:</p> <ul> <li>Non-compliant VMs that need the extension installed</li> <li>VMs that aren't reporting update status properly</li> <li>Any policy assignment errors that need investigation</li> </ul> <p>Learn more about Azure Policy for Update Management</p>","tags":["Azure Update Manager","PowerShell","Azure Policy","VM Patching","FinOps"]},{"location":"blog/bringing-patch-management-in-house-migrating-from-msp-to-azure-update-manager/#step-8-create-dynamic-scopes-in-update-manager","title":"\ud83e\uddea Step 8 \u2013 Create Dynamic Scopes in Update Manager","text":"<p>This is where it all comes together \u2014 and where the magic happens.</p> <p>Dynamic scopes use those <code>PatchWindow</code> tags to assign VMs to the correct patch config automatically. No more manual VM assignment, no more \"did we remember to add the new server?\" conversations.</p>","tags":["Azure Update Manager","PowerShell","Azure Policy","VM Patching","FinOps"]},{"location":"blog/bringing-patch-management-in-house-migrating-from-msp-to-azure-update-manager/#the-portal-dance","title":"\ud83c\udfaf The Portal Dance","text":"<p>Unfortunately, as of writing, dynamic scopes can only be configured through the Azure portal \u2014 no PowerShell or ARM template support yet.</p> <p>Why portal only? Dynamic scopes are still in preview, and Microsoft hasn't released the PowerShell cmdlets or ARM template schemas yet. This means you can't fully automate the deployment, but the functionality itself works perfectly.</p> <p>Here's the step-by-step:</p> <ol> <li>Navigate to Azure Update Manager</li> <li> <p>Portal \u2192 All Services \u2192 Azure Update Manager</p> </li> <li> <p>Access Maintenance Configurations</p> </li> <li>Go to Maintenance Configurations (Preview)</li> <li> <p>Select one of your configs (e.g., <code>contoso-maintenance-config-vms-mon</code>)</p> </li> <li> <p>Create Dynamic Scope</p> </li> <li>Click Dynamic Scopes \u2192 Add</li> <li>Name: <code>DynamicScope-Monday-VMs</code></li> <li> <p>Description: <code>Auto-assign Windows VMs tagged for Monday maintenance</code></p> </li> <li> <p>Configure Scope Settings</p> </li> <li>Subscription: Select your subscription(s)</li> <li>Resource Type: <code>Microsoft.Compute/virtualMachines</code></li> <li> <p>OS Type: <code>Windows</code> (create separate scopes for Linux if needed)</p> </li> <li> <p>Set Tag Filters</p> </li> <li>Tag Name: <code>PatchWindow</code></li> <li>Tag Value: <code>mon</code> (must match your maintenance config naming)</li> <li> <p>Additional filters (optional):</p> <ul> <li><code>Owner</code> = <code>Contoso</code></li> <li><code>Updates</code> = <code>Azure Update Manager</code></li> </ul> </li> <li> <p>Review and Create</p> </li> <li>Verify the filter logic</li> <li>Click Create</li> </ol>","tags":["Azure Update Manager","PowerShell","Azure Policy","VM Patching","FinOps"]},{"location":"blog/bringing-patch-management-in-house-migrating-from-msp-to-azure-update-manager/#repeat-for-all-days","title":"\ud83d\udd04 Repeat for All Days","text":"<p>You'll need to create dynamic scopes for each maintenance configuration:</p> Maintenance Config Dynamic Scope Name Tag Filter <code>contoso-maintenance-config-vms-mon</code> <code>DynamicScope-Monday-VMs</code> <code>PatchWindow = mon</code> <code>contoso-maintenance-config-vms-tue</code> <code>DynamicScope-Tuesday-VMs</code> <code>PatchWindow = tue</code> <code>contoso-maintenance-config-vms-wed</code> <code>DynamicScope-Wednesday-VMs</code> <code>PatchWindow = wed</code> <code>contoso-maintenance-config-vms-thu</code> <code>DynamicScope-Thursday-VMs</code> <code>PatchWindow = thu</code> <code>contoso-maintenance-config-vms-fri</code> <code>DynamicScope-Friday-VMs</code> <code>PatchWindow = fri</code> <code>contoso-maintenance-config-vms-sat</code> <code>DynamicScope-Saturday-VMs</code> <code>PatchWindow = sat-09</code> <code>contoso-maintenance-config-vms-sun</code> <code>DynamicScope-Sunday-VMs</code> <code>PatchWindow = sun</code>","tags":["Azure Update Manager","PowerShell","Azure Policy","VM Patching","FinOps"]},{"location":"blog/bringing-patch-management-in-house-migrating-from-msp-to-azure-update-manager/#verify-dynamic-scope-assignment","title":"\ud83d\udd0d Verify Dynamic Scope Assignment","text":"<p>Once created, you can verify the scopes are working:</p> <ol> <li>In the Maintenance Configuration:</li> <li>Go to Dynamic Scopes</li> <li>Check Resources tab to see matched VMs</li> <li>Verify expected VM count matches your tagging</li> <li> <p>Wait time: Allow 15-30 minutes for newly tagged VMs to appear</p> </li> <li> <p>What success looks like:</p> </li> <li>Monday scope shows 5 VMs (WEB-PROD-01, DB-PROD-01, etc.)</li> <li>Saturday scope shows 5 VMs (WEB-DEV-01, DB-DEV-01, etc.)</li> <li> <p>No VMs showing? Check tag case sensitivity and filters</p> </li> <li> <p>In Azure Resource Graph:</p> </li> </ol> <pre><code>MaintenanceResources\n| where type == \"microsoft.maintenance/configurationassignments\"\n| extend vmName = tostring(split(resourceId, \"/\")[8])\n| extend configName = tostring(properties.maintenanceConfigurationId)\n| project vmName, configName, resourceGroup\n| order by configName, vmName\n</code></pre> <ol> <li>Troubleshoot empty scopes:</li> <li>Verify subscription selection includes all your VMs</li> <li>Check tag spelling: <code>PatchWindow</code> (case sensitive)</li> <li>Confirm resource type filter: <code>Microsoft.Compute/virtualMachines</code></li> <li>Wait longer - it can take up to 30 minutes</li> </ol>","tags":["Azure Update Manager","PowerShell","Azure Policy","VM Patching","FinOps"]},{"location":"blog/bringing-patch-management-in-house-migrating-from-msp-to-azure-update-manager/#common-gotchas","title":"\u26a0\ufe0f Common Gotchas","text":"<p>Tag Case Sensitivity: Dynamic scopes are case-sensitive. <code>mon</code> \u2260 <code>Mon</code> \u2260 <code>MON</code></p> <p>Subscription Scope: Ensure you've selected all relevant subscriptions in the scope configuration.</p> <p>Resource Type Filter: Don't forget to set the resource type filter \u2014 without it, you'll match storage accounts, networking, etc.</p> <p>Timing: It can take 15-30 minutes for newly tagged VMs to appear in dynamic scopes.</p> <p>Dynamic scope configuration docs</p>","tags":["Azure Update Manager","PowerShell","Azure Policy","VM Patching","FinOps"]},{"location":"blog/bringing-patch-management-in-house-migrating-from-msp-to-azure-update-manager/#step-9-test-verify-the-moment-of-truth","title":"\ud83d\ude80 Step 9 \u2013 Test &amp; Verify (The Moment of Truth)","text":"<p>The acid test: does it actually patch stuff properly?</p>","tags":["Azure Update Manager","PowerShell","Azure Policy","VM Patching","FinOps"]},{"location":"blog/bringing-patch-management-in-house-migrating-from-msp-to-azure-update-manager/#proof-of-concept-test","title":"\ud83c\udfaa Proof of Concept Test","text":"<p>I started conservatively \u2014 scoped <code>contoso-maintenance-config-vms-sun</code> to a few non-critical VMs and let it run overnight on Sunday.</p> <p>Monday morning verification:</p> <ul> <li>\u2714\ufe0f Patch compliance dashboard: All green ticks</li> <li>\u2714\ufe0f Reboot timing: Machines restarted within their 4-hour window (21:00-01:00)</li> <li>\u2714\ufe0f Update logs: Activity logs showed expected patching behavior</li> <li>\u2714\ufe0f Business impact: Zero helpdesk tickets on Monday morning</li> </ul>","tags":["Azure Update Manager","PowerShell","Azure Policy","VM Patching","FinOps"]},{"location":"blog/bringing-patch-management-in-house-migrating-from-msp-to-azure-update-manager/#full-rollout-verification","title":"\ud83d\udcca Full Rollout Verification","text":"<p>Once confident with the Sunday test, I enabled all remaining dynamic scopes and monitored the week:</p> <p>Key metrics tracked:</p> <ul> <li>Patch compliance percentage across all VMs</li> <li>Failed patch installations (and root causes)</li> <li>Reboot timing adherence</li> <li>Business hours impact (spoiler: zero)</li> </ul>","tags":["Azure Update Manager","PowerShell","Azure Policy","VM Patching","FinOps"]},{"location":"blog/bringing-patch-management-in-house-migrating-from-msp-to-azure-update-manager/#monitoring-validation-tools","title":"\ud83d\udd0d Monitoring &amp; Validation Tools","text":"<p>Azure Update Manager Dashboard:</p> <pre><code>Azure Portal \u2192 Update Manager \u2192 Overview\n- Patch compliance summary\n- Recent patch installations\n- Failed installations with details\n</code></pre> <p>Azure Resource Graph Queries:</p> <pre><code>// Verify all VMs have maintenance tags\nResources\n| where type == \"microsoft.compute/virtualmachines\"\n| where tags.Updates == \"Azure Update Manager\"\n| project name, resourceGroup, subscriptionId, \n          patchWindow = tags.PatchWindow,\n          owner = tags.Owner\n| summarize count() by patchWindow\n| order by patchWindow\n\n// Check maintenance configuration assignments\nMaintenanceResources\n| where type == \"microsoft.maintenance/configurationassignments\"\n| extend vmName = tostring(split(resourceId, \"/\")[8])\n| extend configName = tostring(properties.maintenanceConfigurationId)\n| project vmName, configName, subscriptionId\n| summarize VMCount = count() by configName\n| order by configName\n</code></pre> <p>PowerShell Verification:</p> <pre><code># Quick check of maintenance configuration status\nGet-AzMaintenanceConfiguration -ResourceGroupName \"rg-maintenance-uksouth-001\" | \n    Select-Object Name, MaintenanceScope, RecurEvery | \n    Format-Table -AutoSize\n\n# Verify VM tag distribution\n$subscriptions = Get-AzSubscription | Where-Object { $_.State -eq \"Enabled\" }\n$tagSummary = @{}\n\nforeach ($sub in $subscriptions) {\n    Set-AzContext -SubscriptionId $sub.Id | Out-Null\n    $vms = Get-AzVM | Where-Object { $_.Tags.PatchWindow }\n\n    foreach ($vm in $vms) {\n        $window = $vm.Tags.PatchWindow\n        if (-not $tagSummary.ContainsKey($window)) {\n            $tagSummary[$window] = 0\n        }\n        $tagSummary[$window]++\n    }\n}\n\nWrite-Host \"=== VM DISTRIBUTION BY PATCH WINDOW ===\" -ForegroundColor Cyan\n$tagSummary.GetEnumerator() | Sort-Object Name | ForEach-Object {\n    Write-Host \"$($_.Key): $($_.Value) VMs\" -ForegroundColor White\n}\n</code></pre>","tags":["Azure Update Manager","PowerShell","Azure Policy","VM Patching","FinOps"]},{"location":"blog/bringing-patch-management-in-house-migrating-from-msp-to-azure-update-manager/#success-metrics","title":"\ud83d\udcc8 Success Metrics","text":"<p>After two full weeks of operation:</p> <ul> <li>Better control: Direct management of patch schedules and policies  </li> <li>Increased visibility: Real-time compliance dashboards vs. periodic reports</li> <li>Reduced complexity: Native Azure tooling vs. third-party solutions</li> </ul> <p>Monitor updates in Azure Update Manager</p>","tags":["Azure Update Manager","PowerShell","Azure Policy","VM Patching","FinOps"]},{"location":"blog/bringing-patch-management-in-house-migrating-from-msp-to-azure-update-manager/#final-thoughts-tips","title":"\ud83d\udcc3 Final Thoughts &amp; Tips","text":"<p>\u2705 Cost-neutral \u2014 No more third-party patch agents \u2705 Policy-driven \u2014 Enforced consistency with Azure Policy \u2705 Easily auditable \u2014 Tag-based scoping is clean and visible \u2705 Scalable \u2014 New VMs auto-join patch schedules via tagging</p>","tags":["Azure Update Manager","PowerShell","Azure Policy","VM Patching","FinOps"]},{"location":"blog/bringing-patch-management-in-house-migrating-from-msp-to-azure-update-manager/#troubleshooting-guide-common-issues","title":"\u26a0\ufe0f Troubleshooting Guide &amp; Common Issues","text":"<p>Here's what I learned the hard way, so you don't have to:</p> Symptom Possible Cause Fix VM not showing in dynamic scope Tag typo or case mismatch Verify <code>PatchWindow</code> tag exactly matches config name Maintenance config creation fails Invalid duration format Use ISO 8601 format: <code>\"03:00\"</code> not <code>\"3 hours\"</code> VM skipped during patching Policy prerequisites not met Check Azure Policy compliance dashboard No updates applied despite schedule VM needs pending reboot Clear previous reboots, check update history Dynamic scope shows zero VMs Wrong subscription scope Verify subscription selection in scope config Extension installation failed Insufficient permissions Ensure VM contributor rights and resource provider registration Policy compliance stuck at 0% Assignment scope too narrow Check policy is assigned at subscription level VMs appear/disappear from scope Tag inconsistency Run tag verification script across all subscriptions","tags":["Azure Update Manager","PowerShell","Azure Policy","VM Patching","FinOps"]},{"location":"blog/bringing-patch-management-in-house-migrating-from-msp-to-azure-update-manager/#advanced-troubleshooting-commands","title":"\ud83d\udd27 Advanced Troubleshooting Commands","text":"<p>Check VM Update Readiness:</p> <pre><code># Verify VM has required extensions and configuration\n$vmName = \"your-vm-name\"\n$rgName = \"your-resource-group\"\n\n$vm = Get-AzVM -Name $vmName -ResourceGroupName $rgName -Status\n$vm.Extensions | Where-Object { $_.Name -like \"*Update*\" -or $_.Name -like \"*Maintenance*\" }\n</code></pre> <p>Validate Maintenance Configuration:</p> <pre><code># Test maintenance configuration is properly formed\n$config = Get-AzMaintenanceConfiguration -ResourceGroupName \"rg-maintenance-uksouth-001\" -Name \"contoso-maintenance-config-vms-mon\"\nWrite-Host \"Config Name: $($config.Name)\"\nWrite-Host \"Recurrence: $($config.RecurEvery)\"\nWrite-Host \"Duration: $($config.Duration)\"\nWrite-Host \"Start Time: $($config.StartDateTime)\"\nWrite-Host \"Timezone: $($config.TimeZone)\"\n</code></pre> <p>Policy Compliance Deep Dive:</p> <pre><code># Check specific VMs for policy compliance\n$policyName = \"Set prerequisites for scheduling recurring updates on Azure virtual machines\"\n$assignments = Get-AzPolicyAssignment | Where-Object { $_.Properties.DisplayName -eq $policyName }\nforeach ($assignment in $assignments) {\n    Get-AzPolicyState -PolicyAssignmentId $assignment.PolicyAssignmentId | \n        Where-Object { $_.ComplianceState -eq \"NonCompliant\" } |\n        Select-Object ResourceId, ComplianceState, @{Name=\"Reason\";Expression={$_.PolicyEvaluationDetails.EvaluatedExpressions.ExpressionValue}}\n}\n</code></pre> <p>As always, comments and suggestions welcome over on GitHub or LinkedIn. If you've migrated patching in a different way, I'd love to hear how you approached it.</p> <p>Share on  Share on </p>","tags":["Azure Update Manager","PowerShell","Azure Policy","VM Patching","FinOps"]},{"location":"blog/customizing-your-mkdocs-blog/","title":"Customizing Your MkDocs Blog \ud83c\udfa8","text":"<p>Once you've set up your MkDocs blog, it's time to personalize it. In this post, I'll cover various customizations, including social media sharing hooks, changing the blog icon and favicon, adding authors, and using tags. These modifications will make your blog more interactive and visually appealing.</p> <p>All of the steps covered in this guide are available in the MKDocs Documentation or via the superb video tutorials created by James Willett</p>","tags":["technology","learning"]},{"location":"blog/customizing-your-mkdocs-blog/#adding-social-media-sharing-hooks","title":"Adding Social Media Sharing Hooks \ud83d\udd04","text":"<p>To allow users to share your blog posts easily, you can create a <code>socialmedia.py</code> hook. This script appends sharing buttons to each post.</p>","tags":["technology","learning"]},{"location":"blog/customizing-your-mkdocs-blog/#create-the-hook-file","title":"Create the Hook File","text":"<p>Inside your project, create a folder named <code>hooks/</code> if it doesn't already exist, and then add a file called <code>socialmedia.py</code>:</p> <pre><code>mkdir hooks\nnano hooks/socialmedia.py\n</code></pre>","tags":["technology","learning"]},{"location":"blog/customizing-your-mkdocs-blog/#add-the-social-media-sharing-code","title":"Add the Social Media Sharing Code","text":"<p>Paste the following into <code>socialmedia.py</code>:</p> <pre><code>from textwrap import dedent\nimport urllib.parse\nimport re\n\nx_intent = \"https://x.com/intent/tweet\"\nfb_sharer = \"https://www.facebook.com/sharer/sharer.php\"\ninclude = re.compile(r\"posts/.*\")\n\ndef on_page_markdown(markdown, **kwargs):\n    page = kwargs['page']\n    config = kwargs['config']\n    if not include.match(page.url):\n        return markdown\n\n    page_url = config.site_url + page.url\n    page_title = urllib.parse.quote(page.title + '\\n')\n\n    return markdown + dedent(f\"\"\"\n    [Share on :simple-x:]({x_intent}?text={page_title}&amp;url={page_url}){{ .md-button }}\n    [Share on :simple-facebook:]({fb_sharer}?u={page_url}){{ .md-button }}\n    \"\"\")\n</code></pre> <p>How the Code Works:</p> <ul> <li>The script identifies blog post pages using a regular expression (<code>posts/.*</code>)</li> <li>It gets the current page URL and title from MkDocs</li> <li>It adds formatted markdown buttons at the end of your content</li> <li>The buttons link to X (Twitter) and Facebook with pre-filled sharing information</li> </ul>","tags":["technology","learning"]},{"location":"blog/customizing-your-mkdocs-blog/#enable-the-hook-in-mkdocsyml","title":"Enable the Hook in <code>mkdocs.yml</code>","text":"<p>Modify <code>mkdocs.yml</code> to include the hook:</p> <pre><code>hooks:\n  - hooks/socialmedia.py\n</code></pre> <p>This will append social media sharing buttons to your posts.</p>","tags":["technology","learning"]},{"location":"blog/customizing-your-mkdocs-blog/#changing-the-blog-icon-and-favicon","title":"Changing the Blog Icon and Favicon \ud83d\uddbc\ufe0f","text":"<p>Updating your blog's favicon and site icon enhances branding.</p>","tags":["technology","learning"]},{"location":"blog/customizing-your-mkdocs-blog/#prepare-the-icons","title":"Prepare the Icons","text":"<p>Save your icons in <code>docs/images/</code> as:</p> <ul> <li><code>favicon.ico</code> (16x16 or 32x32 pixels) - Used by browsers in tabs and bookmarks</li> <li><code>logo.png</code> (recommended 512x512 pixels) - Displayed in your site header/navigation</li> </ul>","tags":["technology","learning"]},{"location":"blog/customizing-your-mkdocs-blog/#update-mkdocsyml","title":"Update <code>mkdocs.yml</code>","text":"<pre><code>extra:\n  logo: images/logo.png\n  favicon: images/favicon.ico\n</code></pre> <p>Note: These paths are relative to your <code>docs/</code> directory, and both settings should be nested under the <code>extra:</code> key in your configuration file.</p>","tags":["technology","learning"]},{"location":"blog/customizing-your-mkdocs-blog/#restart-the-server","title":"Restart the Server","text":"<p>Run <code>mkdocs serve</code> to preview the changes.</p>","tags":["technology","learning"]},{"location":"blog/customizing-your-mkdocs-blog/#adding-authors","title":"Adding Authors \ud83d\udc65","text":"<p>To attribute posts to different authors, create an <code>authors.yml</code> file.</p>","tags":["technology","learning"]},{"location":"blog/customizing-your-mkdocs-blog/#create-authorsyml","title":"Create <code>authors.yml</code>","text":"<p>In the <code>docs/</code> directory of your project, create a file named <code>authors.yml</code>:</p> <pre><code>matthew:\n  name: \"Matthew Pollock\"\n  email: \"matthew@example.com\"\n  website: \"https://matthewblog.com\"\n\nteam:\n  name: \"Blog Team\"\n  website: \"https://teamwebsite.com\"\n\nsquidfunk:\n  name: \"SquidFunk\"\n  website: \"https://squidfunk.github.io/\"\n</code></pre>","tags":["technology","learning"]},{"location":"blog/customizing-your-mkdocs-blog/#link-authors-to-posts","title":"Link Authors to Posts","text":"<p>Modify each post's metadata:</p> <pre><code>authors:\n  - matthew\n  - team\n</code></pre>","tags":["technology","learning"]},{"location":"blog/customizing-your-mkdocs-blog/#adding-tags","title":"Adding Tags \ud83c\udff7\ufe0f","text":"<p>Tags help categorize your blog posts.</p>","tags":["technology","learning"]},{"location":"blog/customizing-your-mkdocs-blog/#create-tagsmd","title":"Create <code>tags.md</code>","text":"<p>Create a <code>docs/tags.md</code> file. Below are the tags used in this blog:</p> <pre><code># Tags\ntags:\n  - technology\n  - learning\n</code></pre>","tags":["technology","learning"]},{"location":"blog/customizing-your-mkdocs-blog/#enable-tags-in-mkdocsyml","title":"Enable Tags in <code>mkdocs.yml</code>","text":"<p>Modify <code>mkdocs.yml</code>:</p> <pre><code>plugins:\n  - tags:\n      tags_file: tags.md\n</code></pre> <p>Now, you can tag posts like this:</p> <pre><code>tags:\n  - technology\n  - learning\n</code></pre>","tags":["technology","learning"]},{"location":"blog/customizing-your-mkdocs-blog/#enabling-comments-on-blog-posts","title":"Enabling Comments on Blog Posts \ud83d\udcac","text":"<p>If you want to enable comments on blog posts, follow these steps:</p>","tags":["technology","learning"]},{"location":"blog/customizing-your-mkdocs-blog/#create-the-comments-template","title":"Create the Comments Template","text":"<p>Create a directory <code>overrides/partials/</code> if it doesn't exist, then add a file called <code>comments.html</code> with your Disqus integration code:</p> <pre><code>&lt;div id=\"disqus_thread\"&gt;&lt;/div&gt;\n&lt;script&gt;\n  var disqus_config = function () {\n    this.page.url = window.location.href;\n    this.page.identifier = document.title;\n  };\n  (function() {\n    var d = document, s = d.createElement('script');\n    s.src = 'https://your-disqus-name.disqus.com/embed.js';\n    s.setAttribute('data-timestamp', +new Date());\n    (d.head || d.body).appendChild(s);\n  })();\n&lt;/script&gt;\n</code></pre> <p>Note: Replace <code>your-disqus-name</code> with your Disqus shortname, which you can find in your Disqus admin panel after creating a site.</p>","tags":["technology","learning"]},{"location":"blog/customizing-your-mkdocs-blog/#enable-comments-in-mkdocsyml","title":"Enable Comments in <code>mkdocs.yml</code>","text":"<pre><code>extra:\n  comments: true\n</code></pre> <p>Restart MkDocs and test:</p> <pre><code>mkdocs serve\n</code></pre>","tags":["technology","learning"]},{"location":"blog/customizing-your-mkdocs-blog/#theme-customization","title":"Theme Customization \ud83c\udfad","text":"<p>The Material theme offers extensive customization options for colors, fonts, and more.</p>","tags":["technology","learning"]},{"location":"blog/customizing-your-mkdocs-blog/#custom-color-scheme","title":"Custom Color Scheme","text":"<p>Add this to your <code>mkdocs.yml</code>:</p> <pre><code>theme:\n  name: material\n  palette:\n    # Light mode\n    - media: \"(prefers-color-scheme: light)\"\n      scheme: default\n      primary: indigo\n      accent: indigo\n      toggle:\n        icon: material/toggle-switch-off-outline\n        name: Switch to dark mode\n    # Dark mode\n    - media: \"(prefers-color-scheme: dark)\"\n      scheme: slate\n      primary: blue\n      accent: blue\n      toggle:\n        icon: material/toggle-switch\n        name: Switch to light mode\n</code></pre>","tags":["technology","learning"]},{"location":"blog/customizing-your-mkdocs-blog/#custom-fonts","title":"Custom Fonts","text":"<pre><code>theme:\n  font:\n    text: Roboto\n    code: Roboto Mono\n</code></pre>","tags":["technology","learning"]},{"location":"blog/customizing-your-mkdocs-blog/#analytics-integration","title":"Analytics Integration \ud83d\udcca","text":"<p>Add analytics to track your blog's performance.</p>","tags":["technology","learning"]},{"location":"blog/customizing-your-mkdocs-blog/#google-analytics","title":"Google Analytics","text":"<pre><code>extra:\n  analytics:\n    provider: google\n    property: G-XXXXXXXXXX\n</code></pre>","tags":["technology","learning"]},{"location":"blog/customizing-your-mkdocs-blog/#plausible-analytics","title":"Plausible Analytics","text":"<pre><code>extra:\n  analytics:\n    provider: plausible\n    domain: yourdomain.com\n</code></pre>","tags":["technology","learning"]},{"location":"blog/customizing-your-mkdocs-blog/#seo-optimization","title":"SEO Optimization \ud83d\udd0d","text":"<p>Improve your blog's search engine visibility.</p>","tags":["technology","learning"]},{"location":"blog/customizing-your-mkdocs-blog/#add-meta-tags","title":"Add Meta Tags","text":"<pre><code>plugins:\n  - meta\n</code></pre> <p>Then in each post, add:</p> <pre><code>meta:\n  description: \"A detailed guide to customizing MkDocs blogs\"\n  keywords: mkdocs, blog, customization, web development\n  robots: index, follow\n  og:image: /assets/social-card.png\n</code></pre>","tags":["technology","learning"]},{"location":"blog/customizing-your-mkdocs-blog/#performance-optimization","title":"Performance Optimization \u26a1","text":"<p>Keep your blog fast and responsive.</p>","tags":["technology","learning"]},{"location":"blog/customizing-your-mkdocs-blog/#image-optimization","title":"Image Optimization","text":"<ol> <li>Compress all images before adding them to your blog</li> <li>Use modern formats like WebP</li> <li>Specify image dimensions in HTML to prevent layout shifts</li> </ol>","tags":["technology","learning"]},{"location":"blog/customizing-your-mkdocs-blog/#lazy-loading","title":"Lazy Loading","text":"<p>Enable lazy loading of images using the <code>loading=\"lazy\"</code> attribute:</p> <pre><code>![Alt text](image.jpg){ loading=lazy }\n</code></pre>","tags":["technology","learning"]},{"location":"blog/customizing-your-mkdocs-blog/#understanding-your-mkdocs-project-structure","title":"Understanding Your MkDocs Project Structure \ud83d\udcc1","text":"<p>Once you have created an MkDocs project and added the components listed in the two posts in this series, you'll see a folder structure similar to this:</p> <pre><code>project-blog/\n\u251c\u2500\u2500 docs/                 # Documentation files (Markdown content)\n\u2502   \u251c\u2500\u2500 index.md          # Homepage of your site\n\u2502   \u251c\u2500\u2500 tags.md           # Tags page for blog posts\n\u2502   \u251c\u2500\u2500 authors.yml       # Defines author metadata\n\u2502   \u251c\u2500\u2500 posts/            # Blog post storage\n\u2502   \u2502   \u251c\u2500\u2500 firstpost.md  \n\u2502   \u2502   \u251c\u2500\u2500 secondpost.md  \n\u2502   \u2502   \u251c\u2500\u2500 thirdpost.md  \n\u2502   \u251c\u2500\u2500 images/           # Store all your images here\n\u2502   \u2502   \u251c\u2500\u2500 logo.png  \n\u2502   \u2502   \u251c\u2500\u2500 favicon.ico\n\u251c\u2500\u2500 hooks/                # Custom MkDocs hooks (like social media sharing)\n\u251c\u2500\u2500 overrides/            # Custom HTML overrides for Material theme\n\u2502   \u251c\u2500\u2500 partials/comments.html  # Comment system (if enabled)\n\u251c\u2500\u2500 mkdocs.yml            # Configuration file for MkDocs\n\u251c\u2500\u2500 requirements.txt      # Python dependencies\n\u251c\u2500\u2500 .gitignore            # Files to exclude from Git\n</code></pre> <p>This structure keeps content organized, making it easy to scale your documentation or blog.</p>","tags":["technology","learning"]},{"location":"blog/customizing-your-mkdocs-blog/#deploying-updates","title":"Deploying Updates \ud83d\ude80","text":"<p>Whenever you make changes, redeploy your site:</p> <pre><code>mkdocs gh-deploy\n</code></pre> <p>This command builds your site and pushes it to the <code>gh-pages</code> branch of your repository.</p>","tags":["technology","learning"]},{"location":"blog/customizing-your-mkdocs-blog/#troubleshooting-tips","title":"Troubleshooting Tips \ud83d\udd27","text":"","tags":["technology","learning"]},{"location":"blog/customizing-your-mkdocs-blog/#social-media-buttons-not-showing","title":"Social Media Buttons Not Showing","text":"<ul> <li>Ensure your <code>mkdocs.yml</code> has the <code>site_url</code> property set correctly</li> <li>Verify the hook is correctly installed in the <code>hooks/</code> directory</li> </ul>","tags":["technology","learning"]},{"location":"blog/customizing-your-mkdocs-blog/#favicon-not-displaying","title":"Favicon Not Displaying","text":"<ul> <li>Clear your browser cache</li> <li>Ensure the path in <code>mkdocs.yml</code> is correct relative to the <code>docs/</code> directory</li> </ul>","tags":["technology","learning"]},{"location":"blog/customizing-your-mkdocs-blog/#comments-not-loading","title":"Comments Not Loading","text":"<ul> <li>Check your browser console for JavaScript errors</li> <li>Verify your Disqus shortname is correct</li> <li>Ensure <code>extra.comments</code> is set to <code>true</code> in <code>mkdocs.yml</code></li> </ul>","tags":["technology","learning"]},{"location":"blog/customizing-your-mkdocs-blog/#deployment-issues","title":"Deployment Issues","text":"<p>If <code>mkdocs gh-deploy</code> fails:</p> <pre><code># Ensure you have the latest version of MkDocs\npip install --upgrade mkdocs mkdocs-material\n\n# Check your git configuration\ngit config --global user.name \"Your Name\"\ngit config --global user.email \"your.email@example.com\"\n</code></pre>","tags":["technology","learning"]},{"location":"blog/customizing-your-mkdocs-blog/#conclusion","title":"Conclusion \ud83c\udf89","text":"<p>With these customizations, your MkDocs blog will be more interactive and visually engaging. The additions of social sharing, comments, better visualization with icons, and proper author attribution will make your blog more professional and user-friendly.</p> <p>In our next post, we'll cover advanced MkDocs features including content reuse, advanced search configuration, and integration with other tools in your workflow.</p> <p>Stay tuned for more tips!</p> <p>\ud83d\udccc Published on: <code>2025-03-09</code> \u23f3 Read time: 8 min</p> <p>Share on  Share on </p>","tags":["technology","learning"]},{"location":"blog/enhancing-my-mkdocs-blog-with-custom-features/","title":"Enhancing My MkDocs Blog with Custom Features \ud83d\ude80","text":"<p>Once I set up my MkDocs blog, I wanted to personalize it by adding navigation links, social media icons, an announcement bar, and a custom footer. These enhancements improve user experience, branding, and site functionality. This post walks through each customization step with code examples and configurations.</p>","tags":["mkdocs","material-theme","customization","web-development"]},{"location":"blog/enhancing-my-mkdocs-blog-with-custom-features/#adding-navigation-links-to-the-header","title":"\ud83d\udd17 Adding Navigation Links to the Header","text":"<p>To provide quick access to key profiles and resources, I added navigation links.</p>","tags":["mkdocs","material-theme","customization","web-development"]},{"location":"blog/enhancing-my-mkdocs-blog-with-custom-features/#update-mkdocsyml","title":"\ud83d\udcdd Update <code>mkdocs.yml</code>","text":"<pre><code>nav:\n  - Home: index.md\n  - Blog: blog.md\n  - About: about.md\n  - Contact: contact.md\n  - GitHub: https://github.com/cloudlabmp\n  - LinkedIn: https://linkedin.com/in/matthew-pollock-76831920/\n  - Website: https://profile.pollockweb.com\n</code></pre> <p>This allows visitors to access my GitHub, LinkedIn, and personal site from the navigation menu.</p>","tags":["mkdocs","material-theme","customization","web-development"]},{"location":"blog/enhancing-my-mkdocs-blog-with-custom-features/#adding-social-media-icons-to-the-header","title":"\ud83c\udf10 Adding Social Media Icons to the Header","text":"<p>Instead of plain text links, I enabled social media icons in the header.</p>","tags":["mkdocs","material-theme","customization","web-development"]},{"location":"blog/enhancing-my-mkdocs-blog-with-custom-features/#update-mkdocsyml_1","title":"\ud83d\udcdd Update <code>mkdocs.yml</code>","text":"<pre><code>extra:\n  social:\n    - icon: fontawesome/brands/github\n      link: https://github.com/cloudlabmp\n    - icon: fontawesome/brands/linkedin\n      link: https://linkedin.com/in/matthew-pollock-76831920/\n    - icon: fontawesome/solid/globe\n      link: https://profile.pollockweb.com\n</code></pre> <p>These icons now appear in the top-right of the header.</p>","tags":["mkdocs","material-theme","customization","web-development"]},{"location":"blog/enhancing-my-mkdocs-blog-with-custom-features/#enabling-the-announcement-bar","title":"\ud83d\udce2 Enabling the Announcement Bar","text":"<p>A dismissible announcement bar allows for important updates.</p>","tags":["mkdocs","material-theme","customization","web-development"]},{"location":"blog/enhancing-my-mkdocs-blog-with-custom-features/#update-mkdocsyml_2","title":"\ud83d\udcdd Update <code>mkdocs.yml</code>","text":"<pre><code>theme:\n  name: material\n  features:\n    - announce.dismiss\n</code></pre>","tags":["mkdocs","material-theme","customization","web-development"]},{"location":"blog/enhancing-my-mkdocs-blog-with-custom-features/#customize-overridesmainhtml","title":"\ud83d\udcdd Customize <code>overrides/main.html</code>","text":"<pre><code>{% extends \"base.html\" %}\n\n{% block announce %}\n  &lt;div class=\"announcement-content\"&gt;\n    &lt;p&gt;Welcome to my blog! Connect with me:&lt;/p&gt;\n    &lt;a href=\"https://github.com/cloudlabmp\" target=\"_blank\"&gt;\n      &lt;i class=\"fab fa-github fa-2x\"&gt;&lt;/i&gt;\n    &lt;/a&gt;\n    &lt;a href=\"https://www.linkedin.com/in/matthew-pollock-76831920/\" target=\"_blank\"&gt;\n      &lt;i class=\"fab fa-linkedin fa-2x\"&gt;&lt;/i&gt;\n    &lt;/a&gt;\n  &lt;/div&gt;\n{% endblock %}\n</code></pre>","tags":["mkdocs","material-theme","customization","web-development"]},{"location":"blog/enhancing-my-mkdocs-blog-with-custom-features/#customizing-the-footer","title":"\ud83d\udccc Customizing the Footer","text":"<p>To personalize the footer, I added a copyright notice and aligned social media icons.</p>","tags":["mkdocs","material-theme","customization","web-development"]},{"location":"blog/enhancing-my-mkdocs-blog-with-custom-features/#customize-overridespartialsfooterhtml","title":"\ud83d\udcdd Customize <code>overrides/partials/footer.html</code>","text":"<pre><code>{% block content %}\n  &lt;div class=\"custom-footer\"&gt;\n    &lt;div class=\"custom-footer-left\"&gt;\n      &lt;p&gt;Copyright &amp;copy; 2025 Matthew Pollock&lt;/p&gt;\n    &lt;/div&gt;\n    &lt;div class=\"custom-footer-right\"&gt;\n      &lt;a href=\"https://github.com/cloudlabmp\" target=\"_blank\"&gt;\n        &lt;i class=\"fab fa-github\"&gt;&lt;/i&gt;\n      &lt;/a&gt;\n      &lt;a href=\"https://linkedin.com/in/matthew-pollock-76831920/\" target=\"_blank\"&gt;\n        &lt;i class=\"fab fa-linkedin\"&gt;&lt;/i&gt;\n      &lt;/a&gt;\n      &lt;a href=\"https://profile.pollockweb.com\" target=\"_blank\"&gt;\n        &lt;i class=\"fas fa-globe\"&gt;&lt;/i&gt;\n      &lt;/a&gt;\n    &lt;/div&gt;\n  &lt;/div&gt;\n{% endblock %}\n</code></pre>","tags":["mkdocs","material-theme","customization","web-development"]},{"location":"blog/enhancing-my-mkdocs-blog-with-custom-features/#custom-footer-styling-extracss","title":"\ud83c\udfa8 Custom Footer Styling (<code>extra.css</code>)","text":"<pre><code>/* Custom footer styling */\n.custom-footer {\n  display: flex;\n  justify-content: space-between;\n  align-items: center;\n  padding: 15px 20px;\n  width: 100%;\n  background: var(--md-default-bg-color);\n  border-top: 1px solid var(--md-default-fg-color--light);\n}\n\n.custom-footer-left {\n  text-align: left;\n  font-size: 0.9em;\n  color: var(--md-default-fg-color);\n}\n\n.custom-footer-right {\n  display: flex;\n  gap: 15px;\n}\n\n.custom-footer-right a {\n  font-size: 1.8em;\n  color: var(--md-default-fg-color);\n  transition: transform 0.2s ease-in-out;\n}\n\n.custom-footer-right a:hover {\n  transform: scale(1.2);\n  color: #673AB7; /* Deep Purple (Accent Color) */\n}\n\n/* Add padding to the bottom of the page */\n.md-content {\n  padding-bottom: 40px;\n}\n</code></pre>","tags":["mkdocs","material-theme","customization","web-development"]},{"location":"blog/enhancing-my-mkdocs-blog-with-custom-features/#final-results","title":"\ud83c\udf89 Final Results","text":"<p>\u2714 Clickable social media icons in the header and footer \u2714 A dismissible announcement bar for updates \u2714 Navigation links to external sites \u2714 A fully customized footer with copyright and icons \u2714 Refined color scheme using deep purple accents  </p> <p>Each of these enhancements has made my MkDocs blog more functional, visually appealing, and user-friendly. If you\u2019re looking to implement similar customizations, these steps should get you started! \ud83d\ude80</p> <p>Share on  Share on </p>","tags":["mkdocs","material-theme","customization","web-development"]},{"location":"blog/setting-up-mkdocs-for-your-blog/","title":"Setting Up MkDocs for Your Blog \ud83d\udcdd","text":"<p>If you're looking for a simple yet powerful way to create and manage your documentation or blog, MkDocs is a fantastic option. MkDocs is a fast, static site generator that's geared towards building project documentation but works wonderfully for blogs too! In this post, I'll walk you through the steps to set up MkDocs with the popular Material theme and get it hosted on GitHub Pages.</p> <p>All of the steps covered in this guide are available in the MkDocs Documentation or via the superb video tutorials created by James Willett.</p>","tags":["technology","learning"]},{"location":"blog/setting-up-mkdocs-for-your-blog/#why-i-chose-mkdocs","title":"Why I Chose MkDocs \ud83e\udd14","text":"<p>After exploring several blogging platforms, I settled on MkDocs for its simplicity, flexibility, and Markdown support. Unlike WordPress or Ghost, MkDocs is lightweight and doesn't require a database. The Material theme provides beautiful out-of-the-box styling, and since everything is in Markdown, I can easily version control my content using Git.</p>","tags":["technology","learning"]},{"location":"blog/setting-up-mkdocs-for-your-blog/#prerequisites","title":"Prerequisites \ud83d\udee0\ufe0f","text":"<p>Before we begin, ensure you have the following installed:</p> <ul> <li>Python 3.x \u2013 MkDocs is a Python-based tool. You can check if Python is installed by running:</li> </ul> <pre><code>python --version\n</code></pre> <p>If Python is not installed, download it from python.org and follow the installation instructions.</p> <ul> <li>pip \u2013 Python's package manager. It usually comes with Python, but you can verify it with:</li> </ul> <pre><code>pip --version\n</code></pre> <ul> <li>Git \u2013 To manage version control and push changes to GitHub. You can install Git from git-scm.com.</li> </ul>","tags":["technology","learning"]},{"location":"blog/setting-up-mkdocs-for-your-blog/#setting-up-your-environment","title":"Setting Up Your Environment \ud83c\udf31","text":"","tags":["technology","learning"]},{"location":"blog/setting-up-mkdocs-for-your-blog/#creating-a-virtual-environment-recommended","title":"Creating a Virtual Environment (Recommended)","text":"<p>It's a good practice to use a virtual environment (venv) when working with Python projects to avoid dependency conflicts:</p> <ol> <li>Navigate to your project directory:</li> </ol> <pre><code>cd /path/to/your/project\n</code></pre> <ol> <li>Create a virtual environment:</li> </ol> <pre><code>python -m venv venv\n</code></pre> <ol> <li>Activate the virtual environment:</li> </ol> <p>Choose your OS</p> On WindowsOn macOS/Linux <pre><code>venv\\Scripts\\activate\n</code></pre> <pre><code>source venv/bin/activate\n</code></pre>","tags":["technology","learning"]},{"location":"blog/setting-up-mkdocs-for-your-blog/#installing-mkdocs","title":"Installing MkDocs","text":"<p>Once your virtual environment is activated, install MkDocs:</p> <pre><code>pip install mkdocs\n</code></pre> <p>To verify the installation, run:</p> <pre><code>mkdocs --version\n</code></pre> <p>If you see the installed version displayed, the installation was successful.</p>","tags":["technology","learning"]},{"location":"blog/setting-up-mkdocs-for-your-blog/#creating-a-new-mkdocs-project","title":"Creating a New MkDocs Project \ud83c\udfd7\ufe0f","text":"<p>Navigate to the directory where you want to create your blog and run:</p> <pre><code>mkdocs new my-blog\ncd my-blog\n</code></pre> <p>This creates a basic MkDocs project structure, including:</p> <ul> <li>A default <code>mkdocs.yml</code> configuration file</li> <li>A <code>docs/</code> directory containing an <code>index.md</code> file</li> </ul>","tags":["technology","learning"]},{"location":"blog/setting-up-mkdocs-for-your-blog/#installing-dependencies","title":"Installing Dependencies \u2699\ufe0f","text":"<p>To enhance your MkDocs site with additional features, create a <code>requirements.txt</code> file in the root of your project:</p> <pre><code># Requirements for core\njinja2~=3.0\nmarkdown~=3.2\nmkdocs~=1.6\nmkdocs-material~=9.5.46\nmkdocs-material-extensions~=1.3\npygments~=2.16\npymdown-extensions~=10.2\n\n# Requirements for plugins\nbabel~=2.10\ncolorama~=0.4\npaginate~=0.5\nregex&gt;=2022.4\nrequests~=2.26\n\n# Additional Material and MkDocs plugins\nmkdocs-glightbox~=0.4.0\nmkdocs-get-deps~=0.2.0\nmkdocs-minify-plugin~=0.8.0\nmkdocs-git-committers-plugin-2~=2.4.1\nmkdocs-git-revision-date-localized-plugin~=1.3.0\nmkdocs-rss-plugin~=1.16.0\n</code></pre> <p>Install these dependencies with:</p> <pre><code>pip install -r requirements.txt\n</code></pre>","tags":["technology","learning"]},{"location":"blog/setting-up-mkdocs-for-your-blog/#customizing-your-blog","title":"Customizing Your Blog \u2728","text":"","tags":["technology","learning"]},{"location":"blog/setting-up-mkdocs-for-your-blog/#basic-configuration","title":"Basic Configuration","text":"<p>The <code>mkdocs.yml</code> file is the configuration file for your blog. Open it in a text editor and modify it:</p> <pre><code>site_name: My Tech Blog\nsite_description: A blog documenting my projects and insights\nsite_author: Your Name\nrepo_url: https://github.com/yourusername/my-blog\n</code></pre>","tags":["technology","learning"]},{"location":"blog/setting-up-mkdocs-for-your-blog/#adding-the-material-theme","title":"Adding the Material Theme","text":"<p>The Material theme provides a clean, responsive design for your blog:</p> <pre><code>theme:\n  name: material\n  palette:\n    primary: indigo\n    accent: indigo\n  features:\n    - navigation.tabs\n    - navigation.top\n    - search.suggest\n    - search.highlight\n</code></pre>","tags":["technology","learning"]},{"location":"blog/setting-up-mkdocs-for-your-blog/#setting-up-blog-features","title":"Setting Up Blog Features","text":"<p>To turn your MkDocs site into a proper blog, add the blog plugin configuration:</p> <pre><code>plugins:\n  - blog:\n      blog_dir: blog\n      post_date_format: yyyy-MM-dd\n      post_url_format: \"{date}/{slug}\"\n  - search\n  - rss:\n      match_path: blog/posts/.*\n      date_from_meta:\n        as_creation: date\n      categories:\n        - categories\n</code></pre>","tags":["technology","learning"]},{"location":"blog/setting-up-mkdocs-for-your-blog/#creating-blog-posts","title":"Creating Blog Posts \ud83d\udcf0","text":"","tags":["technology","learning"]},{"location":"blog/setting-up-mkdocs-for-your-blog/#folder-structure","title":"Folder Structure","text":"<p>Create the following structure for your blog posts:</p> <pre><code>docs/\n\u251c\u2500\u2500 blog/\n\u2502   \u251c\u2500\u2500 posts/\n\u2502   \u2502   \u251c\u2500\u2500 2025-03-01-hello-world.md\n\u2502   \u2502   \u2514\u2500\u2500 2025-03-09-setting-up-mkdocs.md\n\u2502   \u2514\u2500\u2500 index.md\n\u2514\u2500\u2500 index.md\n</code></pre>","tags":["technology","learning"]},{"location":"blog/setting-up-mkdocs-for-your-blog/#post-frontmatter","title":"Post Frontmatter","text":"<p>Each blog post should have frontmatter at the top, like this:</p> <pre><code>---\ntitle: \"Your Post Title\"\ndate: 2025-03-09\nauthors:\n  - yourname\ndescription: \"A brief description of your post.\"\ncategories:\n  - Category1\n  - Category2\ntags:\n  - tag1\n  - tag2\n---\n\n# Your Post Title\n\nContent goes here...\n</code></pre>","tags":["technology","learning"]},{"location":"blog/setting-up-mkdocs-for-your-blog/#adding-images-and-media","title":"Adding Images and Media","text":"<p>To include images in your posts:</p> <ol> <li>Create an <code>assets</code> folder in your <code>docs</code> directory:</li> </ol> <pre><code>docs/\n\u251c\u2500\u2500 assets/\n\u2502   \u2514\u2500\u2500 images/\n\u2502       \u2514\u2500\u2500 screenshot.png\n</code></pre> <ol> <li>Reference the image in your Markdown:</li> </ol> <pre><code>![Screenshot of MkDocs site](../assets/images/screenshot.png)\n</code></pre>","tags":["technology","learning"]},{"location":"blog/setting-up-mkdocs-for-your-blog/#running-mkdocs-locally","title":"Running MkDocs Locally \ud83d\udda5\ufe0f","text":"<p>To preview your blog locally and check how it looks before publishing, run:</p> <pre><code>mkdocs serve\n</code></pre> <p>This will start a local web server. Open your browser and go to:</p> <pre><code>http://127.0.0.1:8000/\n</code></pre> <p>to view your blog. The server will automatically reload when you make changes to your files.</p>","tags":["technology","learning"]},{"location":"blog/setting-up-mkdocs-for-your-blog/#deploying-to-github-pages","title":"Deploying to GitHub Pages \ud83d\ude80","text":"","tags":["technology","learning"]},{"location":"blog/setting-up-mkdocs-for-your-blog/#initialize-a-git-repository","title":"Initialize a Git Repository","text":"<p>First, navigate to your project folder and initialize a Git repository:</p> <pre><code>git init\ngit add .\ngit commit -m \"Initial commit\"\n</code></pre>","tags":["technology","learning"]},{"location":"blog/setting-up-mkdocs-for-your-blog/#create-a-github-repository","title":"Create a GitHub Repository","text":"<ol> <li>Go to GitHub and log in.</li> <li>Click the \"+\" button in the top-right and select \"New repository\".</li> <li>Enter a Repository name (e.g., <code>my-blog</code>).</li> <li>Choose Public or Private, based on your preference.</li> <li>DO NOT initialize with a README, <code>.gitignore</code>, or license (since we are pushing an existing project).</li> <li>Click \"Create repository\".</li> </ol> <p>After creating the repository, copy the repository URL (e.g., <code>https://github.com/yourusername/my-blog.git</code>).</p>","tags":["technology","learning"]},{"location":"blog/setting-up-mkdocs-for-your-blog/#link-your-local-repository-to-github","title":"Link Your Local Repository to GitHub","text":"<p>Now, link your local project to the GitHub repository:</p> <pre><code>git remote add origin https://github.com/yourusername/my-blog.git\ngit branch -M main\ngit push -u origin main\n</code></pre>","tags":["technology","learning"]},{"location":"blog/setting-up-mkdocs-for-your-blog/#deploy-your-blog-to-github-pages","title":"Deploy Your Blog to GitHub Pages","text":"<p>To publish your blog on GitHub Pages, run:</p> <pre><code>mkdocs gh-deploy\n</code></pre> <p>This command builds your MkDocs project and pushes the static files to the <code>gh-pages</code> branch of your repository.</p>","tags":["technology","learning"]},{"location":"blog/setting-up-mkdocs-for-your-blog/#enable-github-pages-in-repository-settings","title":"Enable GitHub Pages in Repository Settings","text":"<ol> <li>Go to your GitHub repository.</li> <li>Navigate to Settings &gt; Pages.</li> <li>Under Branch, select <code>gh-pages</code> and click Save.</li> <li>Your site will be live at <code>https://yourusername.github.io/my-blog/</code> (Note: It may take a few minutes for your site to appear live after deployment).</li> </ol>","tags":["technology","learning"]},{"location":"blog/setting-up-mkdocs-for-your-blog/#enhancing-your-development-experience","title":"Enhancing Your Development Experience \ud83d\udcbb","text":"<p>For an easier development experience, I recommend using Visual Studio Code (VS Code). You can install it from code.visualstudio.com.</p>","tags":["technology","learning"]},{"location":"blog/setting-up-mkdocs-for-your-blog/#recommended-vscode-extensions","title":"Recommended VSCode Extensions","text":"<ul> <li>Python (for virtual environment support)</li> <li>Markdown Preview Enhanced (for writing and previewing Markdown files)</li> <li>YAML (for editing <code>mkdocs.yml</code>)</li> <li>Material Theme Icons (for a nicer file tree visualization)</li> </ul>","tags":["technology","learning"]},{"location":"blog/setting-up-mkdocs-for-your-blog/#troubleshooting-common-issues","title":"Troubleshooting Common Issues \ud83d\udd27","text":"","tags":["technology","learning"]},{"location":"blog/setting-up-mkdocs-for-your-blog/#site-not-deploying","title":"Site Not Deploying","text":"<p>If your site isn't appearing after running <code>mkdocs gh-deploy</code>:</p> <ul> <li>Check if you've configured GitHub Pages in your repository settings</li> <li>Ensure you've pushed your changes to the correct branch</li> <li>Wait a few minutes as GitHub Pages deployment can take time</li> </ul>","tags":["technology","learning"]},{"location":"blog/setting-up-mkdocs-for-your-blog/#styling-issues","title":"Styling Issues","text":"<p>If your theme isn't applying correctly:</p> <ul> <li>Verify the theme is installed (<code>pip install mkdocs-material</code>)</li> <li>Check for syntax errors in your <code>mkdocs.yml</code> file</li> <li>Try clearing your browser cache</li> </ul>","tags":["technology","learning"]},{"location":"blog/setting-up-mkdocs-for-your-blog/#conclusion","title":"Conclusion \ud83c\udf89","text":"<p>Setting up MkDocs is straightforward, and with GitHub Pages, you can host your blog for free. The Material theme provides excellent styling out of the box, and with the right plugins, you can create a fully-featured blog with minimal effort.</p> <p>In future posts, I'll cover more customizations, themes, and plugins to enhance your MkDocs blog. Stay tuned!</p> <p>Share on  Share on </p>","tags":["technology","learning"]},{"location":"blog/cloud-resume-challenge-with-terraform-final-reflections--future-directions/","title":"Cloud Resume Challenge with Terraform: Final Reflections &amp; Future Directions \ud83c\udfaf","text":"","tags":["cloud-resume-challenge","terraform","aws","infrastructure-as-code","devops","career"]},{"location":"blog/cloud-resume-challenge-with-terraform-final-reflections--future-directions/#journey-complete-what-weve-built","title":"Journey Complete: What We've Built \ud83c\udfd7\ufe0f","text":"<p>Over the course of this blog series, we've successfully completed the Cloud Resume Challenge using Terraform as our infrastructure-as-code tool. Let's recap what we've accomplished:</p> <ol> <li>Set up our development environment with Terraform and AWS credentials</li> <li>Deployed a static website using S3, CloudFront, Route 53, and ACM</li> <li>Built a serverless backend API with API Gateway, Lambda, and DynamoDB</li> <li>Implemented CI/CD pipelines with GitHub Actions for automated deployments</li> <li>Added security enhancements like OIDC authentication and least-privilege IAM policies</li> </ol> <p>The final architecture we've created looks like this:</p> <p></p> <p>The most valuable aspect of this project is that we've built a completely automated, production-quality cloud solution. Every component is defined as code, enabling us to track changes, rollback if needed, and redeploy the entire infrastructure with minimal effort.</p>","tags":["cloud-resume-challenge","terraform","aws","infrastructure-as-code","devops","career"]},{"location":"blog/cloud-resume-challenge-with-terraform-final-reflections--future-directions/#key-learnings-from-the-challenge","title":"Key Learnings from the Challenge \ud83e\udde0","text":"","tags":["cloud-resume-challenge","terraform","aws","infrastructure-as-code","devops","career"]},{"location":"blog/cloud-resume-challenge-with-terraform-final-reflections--future-directions/#technical-skills-gained","title":"Technical Skills Gained \ud83d\udcbb","text":"<p>Throughout this challenge, I've gained significant technical skills:</p> <ol> <li>Terraform expertise: I've moved from basic understanding to writing modular, reusable infrastructure code</li> <li>AWS service integration: Learned how multiple AWS services work together to create a cohesive system</li> <li>CI/CD implementation: Set up professional GitHub Actions workflows for continuous deployment</li> <li>Security best practices: Implemented OIDC, least privilege, encryption, and more</li> <li>Serverless architecture: Built and connected serverless components for a scalable, cost-effective solution</li> </ol>","tags":["cloud-resume-challenge","terraform","aws","infrastructure-as-code","devops","career"]},{"location":"blog/cloud-resume-challenge-with-terraform-final-reflections--future-directions/#unexpected-challenges-solutions","title":"Unexpected Challenges &amp; Solutions \ud83d\udd04","text":"<p>The journey wasn't without obstacles. Here are some challenges I faced and how I overcame them:</p>","tags":["cloud-resume-challenge","terraform","aws","infrastructure-as-code","devops","career"]},{"location":"blog/cloud-resume-challenge-with-terraform-final-reflections--future-directions/#1-state-management-complexity","title":"1. State Management Complexity","text":"<p>Challenge: As the project grew, managing Terraform state became more complex, especially when working across different environments.</p> <p>Solution: I restructured the project to use workspaces and remote state with careful output references between modules. This improved state organization and made multi-environment deployments more manageable.</p>","tags":["cloud-resume-challenge","terraform","aws","infrastructure-as-code","devops","career"]},{"location":"blog/cloud-resume-challenge-with-terraform-final-reflections--future-directions/#2-cloudfront-cache-invalidation","title":"2. CloudFront Cache Invalidation","text":"<p>Challenge: Updates to the website weren't immediately visible due to CloudFront caching.</p> <p>Solution: Implemented proper cache invalidation in the CI/CD pipeline and set appropriate cache behaviors for different file types.</p>","tags":["cloud-resume-challenge","terraform","aws","infrastructure-as-code","devops","career"]},{"location":"blog/cloud-resume-challenge-with-terraform-final-reflections--future-directions/#3-cors-configuration","title":"3. CORS Configuration","text":"<p>Challenge: The frontend JavaScript couldn't connect to the API due to CORS issues.</p> <p>Solution: Added comprehensive CORS handling at both the API Gateway and Lambda levels, ensuring proper headers were returned.</p>","tags":["cloud-resume-challenge","terraform","aws","infrastructure-as-code","devops","career"]},{"location":"blog/cloud-resume-challenge-with-terraform-final-reflections--future-directions/#4-cicd-authentication-security","title":"4. CI/CD Authentication Security","text":"<p>Challenge: Initially used long-lived AWS credentials in GitHub Secrets, which posed security risks.</p> <p>Solution: Replaced with OIDC for keyless authentication between GitHub Actions and AWS, eliminating credential management concerns.</p>","tags":["cloud-resume-challenge","terraform","aws","infrastructure-as-code","devops","career"]},{"location":"blog/cloud-resume-challenge-with-terraform-final-reflections--future-directions/#real-world-applications-of-this-project","title":"Real-World Applications of This Project \ud83c\udf10","text":"<p>The skills demonstrated in this challenge directly translate to real-world cloud engineering roles:</p>","tags":["cloud-resume-challenge","terraform","aws","infrastructure-as-code","devops","career"]},{"location":"blog/cloud-resume-challenge-with-terraform-final-reflections--future-directions/#1-infrastructure-as-code-expertise","title":"1. Infrastructure as Code Expertise","text":"<p>The ability to define, version, and automate infrastructure is increasingly essential in modern IT environments. This project showcases expertise with Terraform that can be applied to any cloud provider or on-premises infrastructure.</p>","tags":["cloud-resume-challenge","terraform","aws","infrastructure-as-code","devops","career"]},{"location":"blog/cloud-resume-challenge-with-terraform-final-reflections--future-directions/#2-devops-pipeline-creation","title":"2. DevOps Pipeline Creation","text":"<p>Setting up CI/CD workflows that automate testing and deployment demonstrates key DevOps skills that organizations need to accelerate their development cycles.</p>","tags":["cloud-resume-challenge","terraform","aws","infrastructure-as-code","devops","career"]},{"location":"blog/cloud-resume-challenge-with-terraform-final-reflections--future-directions/#3-serverless-architecture-design","title":"3. Serverless Architecture Design","text":"<p>The backend API implementation shows understanding of event-driven, serverless architecture patterns that are becoming standard for new cloud applications.</p>","tags":["cloud-resume-challenge","terraform","aws","infrastructure-as-code","devops","career"]},{"location":"blog/cloud-resume-challenge-with-terraform-final-reflections--future-directions/#4-security-implementation","title":"4. Security Implementation","text":"<p>The security considerations throughout the project - from IAM roles to OIDC authentication - demonstrate the ability to build secure systems from the ground up.</p>","tags":["cloud-resume-challenge","terraform","aws","infrastructure-as-code","devops","career"]},{"location":"blog/cloud-resume-challenge-with-terraform-final-reflections--future-directions/#maintaining-your-cloud-resume","title":"Maintaining Your Cloud Resume \ud83d\udd27","text":"<p>Now that your resume is live, here are some tips for maintaining it:</p>","tags":["cloud-resume-challenge","terraform","aws","infrastructure-as-code","devops","career"]},{"location":"blog/cloud-resume-challenge-with-terraform-final-reflections--future-directions/#1-regular-updates","title":"1. Regular Updates","text":"<p>Set a schedule to update both your resume content and the underlying infrastructure. I recommend:</p> <ul> <li>Monthly content refreshes to keep your experience and skills current</li> <li>Quarterly infrastructure reviews to apply security patches and update dependencies</li> <li>Annual architecture reviews to consider new AWS services or features</li> </ul>","tags":["cloud-resume-challenge","terraform","aws","infrastructure-as-code","devops","career"]},{"location":"blog/cloud-resume-challenge-with-terraform-final-reflections--future-directions/#2-cost-management","title":"2. Cost Management","text":"<p>While this solution is relatively inexpensive, it's good practice to set up AWS Budgets and alerts to monitor costs. My current monthly costs are approximately:</p> <ul> <li>S3: ~$0.10 for storage</li> <li>CloudFront: ~$0.50 for data transfer</li> <li>Route 53: $0.50 for hosted zone</li> <li>Lambda: Free tier covers typical usage</li> <li>DynamoDB: Free tier covers typical usage</li> <li>API Gateway: ~$1.00 for API calls</li> <li>Total: ~$2.10/month</li> </ul>","tags":["cloud-resume-challenge","terraform","aws","infrastructure-as-code","devops","career"]},{"location":"blog/cloud-resume-challenge-with-terraform-final-reflections--future-directions/#3-monitoring-and-alerting","title":"3. Monitoring and Alerting","text":"<p>I've set up CloudWatch alarms for:</p> <ul> <li>API errors exceeding normal thresholds</li> <li>Unusual traffic patterns that might indicate abuse</li> <li>Lambda function failures</li> </ul> <p>Consider adding application performance monitoring tools like AWS X-Ray for deeper insights.</p>","tags":["cloud-resume-challenge","terraform","aws","infrastructure-as-code","devops","career"]},{"location":"blog/cloud-resume-challenge-with-terraform-final-reflections--future-directions/#future-enhancements","title":"Future Enhancements \ud83d\ude80","text":"<p>There are many ways to extend this project further:</p>","tags":["cloud-resume-challenge","terraform","aws","infrastructure-as-code","devops","career"]},{"location":"blog/cloud-resume-challenge-with-terraform-final-reflections--future-directions/#1-content-management-system-integration","title":"1. Content Management System Integration","text":"<p>Add a headless CMS like Contentful or Sanity to make resume updates easier without needing to edit HTML directly:</p> <pre><code>module \"contentful_integration\" {\n  source = \"./modules/contentful\"\n\n  api_key     = var.contentful_api_key\n  space_id    = var.contentful_space_id\n  environment = var.environment\n}\n\nresource \"aws_lambda_function\" \"content_sync\" {\n  function_name = \"resume-content-sync-${var.environment}\"\n  handler       = \"index.handler\"\n  runtime       = \"nodejs14.x\"\n  role          = aws_iam_role.content_sync_role.arn\n\n  environment {\n    variables = {\n      CONTENTFUL_API_KEY = var.contentful_api_key\n      CONTENTFUL_SPACE_ID = var.contentful_space_id\n      S3_BUCKET = module.frontend.website_bucket_name\n    }\n  }\n}\n</code></pre>","tags":["cloud-resume-challenge","terraform","aws","infrastructure-as-code","devops","career"]},{"location":"blog/cloud-resume-challenge-with-terraform-final-reflections--future-directions/#2-advanced-analytics","title":"2. Advanced Analytics","text":"<p>Implement sophisticated visitor analytics beyond simple counting:</p> <pre><code>resource \"aws_kinesis_firehose_delivery_stream\" \"visitor_analytics\" {\n  name        = \"resume-visitor-analytics-${var.environment}\"\n  destination = \"extended_s3\"\n\n  extended_s3_configuration {\n    role_arn   = aws_iam_role.firehose_role.arn\n    bucket_arn = aws_s3_bucket.analytics.arn\n\n    processing_configuration {\n      enabled = \"true\"\n\n      processors {\n        type = \"Lambda\"\n\n        parameters {\n          parameter_name  = \"LambdaArn\"\n          parameter_value = aws_lambda_function.analytics_processor.arn\n        }\n      }\n    }\n  }\n}\n\nresource \"aws_athena_workgroup\" \"analytics\" {\n  name = \"resume-analytics-${var.environment}\"\n\n  configuration {\n    result_configuration {\n      output_location = \"s3://${aws_s3_bucket.analytics_results.bucket}/results/\"\n    }\n  }\n}\n</code></pre>","tags":["cloud-resume-challenge","terraform","aws","infrastructure-as-code","devops","career"]},{"location":"blog/cloud-resume-challenge-with-terraform-final-reflections--future-directions/#3-multi-region-deployment","title":"3. Multi-Region Deployment","text":"<p>Enhance reliability and performance by deploying to multiple AWS regions:</p> <pre><code>module \"frontend_us_east_1\" {\n  source = \"./modules/frontend\"\n\n  providers = {\n    aws = aws.us_east_1\n  }\n\n  # Configuration for US East region\n}\n\nmodule \"frontend_eu_west_1\" {\n  source = \"./modules/frontend\"\n\n  providers = {\n    aws = aws.eu_west_1\n  }\n\n  # Configuration for EU West region\n}\n\nresource \"aws_route53_health_check\" \"primary_region\" {\n  fqdn              = module.frontend_us_east_1.cloudfront_domain_name\n  port              = 443\n  type              = \"HTTPS\"\n  resource_path     = \"/\"\n  failure_threshold = 3\n  request_interval  = 30\n}\n\nresource \"aws_route53_record\" \"global\" {\n  zone_id = data.aws_route53_zone.selected.zone_id\n  name    = var.domain_name\n  type    = \"CNAME\"\n\n  failover_routing_policy {\n    type = \"PRIMARY\"\n  }\n\n  health_check_id = aws_route53_health_check.primary_region.id\n  set_identifier  = \"primary\"\n  records         = [module.frontend_us_east_1.cloudfront_domain_name]\n  ttl             = 300\n}\n</code></pre>","tags":["cloud-resume-challenge","terraform","aws","infrastructure-as-code","devops","career"]},{"location":"blog/cloud-resume-challenge-with-terraform-final-reflections--future-directions/#4-infrastructure-testing","title":"4. Infrastructure Testing","text":"<p>Add comprehensive testing using Terratest:</p> <pre><code>package test\n\nimport (\n    \"testing\"\n    \"github.com/gruntwork-io/terratest/modules/terraform\"\n    \"github.com/stretchr/testify/assert\"\n)\n\nfunc TestResumeFrontend(t *testing.T) {\n    terraformOptions := terraform.WithDefaultRetryableErrors(t, &amp;terraform.Options{\n        TerraformDir: \"../modules/frontend\",\n        Vars: map[string]interface{}{\n            \"environment\": \"test\",\n            \"domain_name\": \"test.example.com\",\n        },\n    })\n\n    defer terraform.Destroy(t, terraformOptions)\n    terraform.InitAndApply(t, terraformOptions)\n\n    // Verify outputs\n    bucketName := terraform.Output(t, terraformOptions, \"website_bucket_name\")\n    assert.Contains(t, bucketName, \"resume-website-test\")\n}\n</code></pre>","tags":["cloud-resume-challenge","terraform","aws","infrastructure-as-code","devops","career"]},{"location":"blog/cloud-resume-challenge-with-terraform-final-reflections--future-directions/#career-impact-personal-growth","title":"Career Impact &amp; Personal Growth \ud83d\udcc8","text":"<p>Completing this challenge has had a significant impact on my career development:</p>","tags":["cloud-resume-challenge","terraform","aws","infrastructure-as-code","devops","career"]},{"location":"blog/cloud-resume-challenge-with-terraform-final-reflections--future-directions/#technical-growth","title":"Technical Growth","text":"<p>I've moved from basic cloud knowledge to being able to architect and implement complex, multi-service solutions. The hands-on experience with Terraform has been particularly valuable, as it's a highly sought-after skill in the job market.</p>","tags":["cloud-resume-challenge","terraform","aws","infrastructure-as-code","devops","career"]},{"location":"blog/cloud-resume-challenge-with-terraform-final-reflections--future-directions/#portfolio-enhancement","title":"Portfolio Enhancement","text":"<p>This project now serves as both my resume and a demonstration of my cloud engineering capabilities. I've included the GitHub repository links on my resume, allowing potential employers to see the code behind the deployment.</p>","tags":["cloud-resume-challenge","terraform","aws","infrastructure-as-code","devops","career"]},{"location":"blog/cloud-resume-challenge-with-terraform-final-reflections--future-directions/#community-engagement","title":"Community Engagement","text":"<p>Sharing this project through blog posts has connected me with the broader cloud community. The feedback and discussions have been invaluable for refining my approach and learning from others.</p>","tags":["cloud-resume-challenge","terraform","aws","infrastructure-as-code","devops","career"]},{"location":"blog/cloud-resume-challenge-with-terraform-final-reflections--future-directions/#final-thoughts","title":"Final Thoughts \ud83d\udcad","text":"<p>The Cloud Resume Challenge has been an invaluable learning experience. By implementing it with Terraform, I've gained practical experience with both AWS services and infrastructure as code - skills that are directly applicable to professional cloud engineering roles.</p> <p>What makes this challenge particularly powerful is how it combines so many aspects of modern cloud development:</p> <ul> <li>Front-end web development</li> <li>Back-end serverless APIs</li> <li>Infrastructure as code</li> <li>CI/CD automation</li> <li>Security implementation</li> <li>DNS configuration</li> <li>Content delivery networks</li> </ul> <p>If you're following along with this series, I encourage you to customize and extend the project to showcase your unique skills and interests. The foundational architecture we've built provides a flexible platform that can evolve with your career.</p> <p>For those just starting their cloud journey, this challenge offers a perfect blend of practical skills in a realistic project that demonstrates end-to-end capabilities. It's far more valuable than isolated tutorials or theoretical knowledge alone.</p> <p>The cloud engineering field continues to evolve rapidly, but the principles we've applied throughout this project - automation, security, scalability, and operational excellence - remain constants regardless of which specific technologies are in favor.</p>","tags":["cloud-resume-challenge","terraform","aws","infrastructure-as-code","devops","career"]},{"location":"blog/cloud-resume-challenge-with-terraform-final-reflections--future-directions/#whats-next","title":"What's Next? \ud83d\udd2e","text":"<p>While this concludes our Cloud Resume Challenge series, my cloud learning journey continues. Some areas I'm exploring next include:</p> <ul> <li>Kubernetes and container orchestration</li> <li>Infrastructure testing frameworks</li> <li>Cloud cost optimization</li> <li>Multi-cloud deployments</li> <li>Infrastructure security scanning</li> <li>Service mesh implementations</li> </ul> <p>I hope this series has been helpful in your own cloud journey. Feel free to reach out with questions or to share your own implementations of the challenge!</p> <p>This post concludes our Cloud Resume Challenge with Terraform series. Thanks for following along!</p> <p>Want to see the Cloud Resume Challenge in action? Visit my resume website and check out the GitHub repositories for the complete code.</p> <p>Share on  Share on </p>","tags":["cloud-resume-challenge","terraform","aws","infrastructure-as-code","devops","career"]},{"location":"blog/cloud-resume-challenge-with-terraform-introduction--setup/","title":"Cloud Resume Challenge with Terraform: Introduction &amp; Setup \ud83d\ude80","text":"","tags":["cloud","terraform","aws","infrastructure-as-code","resume-challenge"]},{"location":"blog/cloud-resume-challenge-with-terraform-introduction--setup/#introduction","title":"Introduction \ud83c\udf0d","text":"<p>The Cloud Resume Challenge is a hands-on project designed to build a real-world cloud application while showcasing your skills in AWS, serverless architecture, and automation. Many implementations of this challenge use AWS SAM or manual setup via the AWS console, but in this series, I will demonstrate how to build the entire infrastructure using Terraform. \ud83d\udca1</p>","tags":["cloud","terraform","aws","infrastructure-as-code","resume-challenge"]},{"location":"blog/cloud-resume-challenge-with-terraform-introduction--setup/#my-journey-to-terraform","title":"My Journey to Terraform \ud83e\uddf0","text":"<p>When I first discovered the Cloud Resume Challenge, I was immediately intrigued by the hands-on approach to learning cloud technologies. Having some experience with traditional IT but wanting to transition to a more cloud-focused role, I saw this challenge as the perfect opportunity to showcase my skills.</p> <p>I chose Terraform over AWS SAM or CloudFormation because:</p> <ol> <li>Multi-cloud flexibility - While this challenge focuses on AWS, Terraform skills transfer to Azure, GCP, and other providers</li> <li>Declarative approach - I find the HCL syntax more intuitive than YAML for defining infrastructure</li> <li>Industry adoption - In my research, I found that Terraform was highly sought after in job postings</li> <li>Strong community - The extensive module registry and community support made learning easier</li> </ol> <p>This series reflects my personal journey through the challenge, including the obstacles I overcame and the lessons I learned along the way.</p>","tags":["cloud","terraform","aws","infrastructure-as-code","resume-challenge"]},{"location":"blog/cloud-resume-challenge-with-terraform-introduction--setup/#why-terraform","title":"Why Terraform? \ud83d\udee0\ufe0f","text":"<p>Terraform allows for Infrastructure as Code (IaC), which:</p> <ul> <li>Automates resource provisioning \ud83e\udd16</li> <li>Ensures consistency across environments \u2705</li> <li>Improves security by managing configurations centrally \ud83d\udd12</li> <li>Enables version control for infrastructure changes \ud83d\udcdd</li> </ul> <p>This series assumes basic knowledge of Terraform and will focus on highlighting key Terraform code snippets rather than full configuration files.</p>","tags":["cloud","terraform","aws","infrastructure-as-code","resume-challenge"]},{"location":"blog/cloud-resume-challenge-with-terraform-introduction--setup/#project-overview","title":"Project Overview \ud83c\udfd7\ufe0f","text":"<p>Let's visualize the architecture we'll be building throughout this series:</p> <p></p>","tags":["cloud","terraform","aws","infrastructure-as-code","resume-challenge"]},{"location":"blog/cloud-resume-challenge-with-terraform-introduction--setup/#aws-services-used","title":"AWS Services Used \u2601\ufe0f","text":"<p>The project consists of the following AWS components:</p> <ul> <li>Frontend: Static website hosted on S3 and delivered via CloudFront.</li> <li>Backend API: API Gateway, Lambda, and DynamoDB to track visitor counts.</li> <li>Security: IAM roles, API Gateway security, and AWS Certificate Manager (ACM) for HTTPS \ud83d\udd10.</li> <li>Automation: CI/CD with GitHub Actions to deploy infrastructure and update website content \u26a1.</li> </ul>","tags":["cloud","terraform","aws","infrastructure-as-code","resume-challenge"]},{"location":"blog/cloud-resume-challenge-with-terraform-introduction--setup/#terraform-module-breakdown","title":"Terraform Module Breakdown \ud83e\udde9","text":"<p>To keep the infrastructure modular and maintainable, we will define Terraform modules for each major component:</p> <ol> <li>S3 Module \ud83d\udcc2: Manages the static website hosting.</li> <li>CloudFront Module \ud83c\udf0d: Ensures fast delivery and HTTPS encryption.</li> <li>Route 53 Module \ud83d\udce1: Handles DNS configuration.</li> <li>DynamoDB Module \ud83d\udcca: Stores visitor count data.</li> <li>Lambda Module \ud83c\udfd7\ufe0f: Defines the backend API logic.</li> <li>API Gateway Module \ud83d\udd17: Exposes the Lambda function via a REST API.</li> <li>ACM Module \ud83d\udd12: Provides SSL/TLS certificates for secure communication.</li> </ol>","tags":["cloud","terraform","aws","infrastructure-as-code","resume-challenge"]},{"location":"blog/cloud-resume-challenge-with-terraform-introduction--setup/#setting-up-terraform","title":"Setting Up Terraform \u2699\ufe0f","text":"<p>Before deploying any resources, we need to set up Terraform and backend state management to store infrastructure changes securely.</p>","tags":["cloud","terraform","aws","infrastructure-as-code","resume-challenge"]},{"location":"blog/cloud-resume-challenge-with-terraform-introduction--setup/#1-install-terraform-aws-cli","title":"1. Install Terraform &amp; AWS CLI \ud83d\udda5\ufe0f","text":"<p>Ensure you have the necessary tools installed:</p> <pre><code># Install Terraform\ncurl -fsSL https://apt.releases.hashicorp.com/gpg | sudo gpg --dearmor -o /usr/share/keyrings/hashicorp-archive-keyring.gpg\necho \"deb [signed-by=/usr/share/keyrings/hashicorp-archive-keyring.gpg] https://apt.releases.hashicorp.com $(lsb_release -cs) main\" | sudo tee /etc/apt/sources.list.d/hashicorp.list\nsudo apt update &amp;&amp; sudo apt install terraform\n\n# Install AWS CLI\ncurl \"https://awscli.amazonaws.com/AWSCLIV2.pkg\" -o \"AWSCLIV2.pkg\"\nsudo installer -pkg AWSCLIV2.pkg -target /\n</code></pre>","tags":["cloud","terraform","aws","infrastructure-as-code","resume-challenge"]},{"location":"blog/cloud-resume-challenge-with-terraform-introduction--setup/#2-configure-aws-credentials-securely","title":"2. Configure AWS Credentials Securely \ud83d\udd11","text":"<p>Terraform interacts with AWS via credentials. Setting these up securely is crucial to avoid exposing sensitive information.</p>","tags":["cloud","terraform","aws","infrastructure-as-code","resume-challenge"]},{"location":"blog/cloud-resume-challenge-with-terraform-introduction--setup/#setting-up-aws-account-structure","title":"Setting up AWS Account Structure","text":"<p>Following cloud security best practices, I recommend creating a proper AWS account structure:</p> <ol> <li>Create a management AWS account for your organization</li> <li>Enable Multi-Factor Authentication (MFA) on the root account</li> <li>Create separate AWS accounts for development and production environments</li> <li>Set up AWS IAM Identity Center (formerly SSO) for secure access</li> </ol> <p>If you're just getting started, you can begin with a simpler setup:</p> <pre><code># Configure AWS CLI with a dedicated IAM user (not root account)\naws configure\n\n# Test your configuration\naws sts get-caller-identity\n</code></pre> <p>Set up IAM permissions for Terraform by ensuring your IAM user has the necessary policies for provisioning resources. Start with a least privilege approach and add permissions as needed.</p>","tags":["cloud","terraform","aws","infrastructure-as-code","resume-challenge"]},{"location":"blog/cloud-resume-challenge-with-terraform-introduction--setup/#3-set-up-remote-backend-for-terraform-state","title":"3. Set Up Remote Backend for Terraform State \ud83c\udfe2","text":"<p>Using a remote backend (such as an S3 bucket) prevents local state loss and enables collaboration.</p>","tags":["cloud","terraform","aws","infrastructure-as-code","resume-challenge"]},{"location":"blog/cloud-resume-challenge-with-terraform-introduction--setup/#project-directory-structure","title":"Project Directory Structure","text":"<p>Here's how I've organized my Terraform project:</p> <pre><code>cloud-resume-challenge/\n\u251c\u2500\u2500 modules/\n\u2502   \u251c\u2500\u2500 frontend/\n\u2502   \u2502   \u251c\u2500\u2500 main.tf\n\u2502   \u2502   \u251c\u2500\u2500 variables.tf\n\u2502   \u2502   \u2514\u2500\u2500 outputs.tf\n\u2502   \u251c\u2500\u2500 backend/\n\u2502   \u2502   \u251c\u2500\u2500 main.tf\n\u2502   \u2502   \u251c\u2500\u2500 variables.tf\n\u2502   \u2502   \u2514\u2500\u2500 outputs.tf\n\u2502   \u2514\u2500\u2500 networking/\n\u2502       \u251c\u2500\u2500 main.tf\n\u2502       \u251c\u2500\u2500 variables.tf\n\u2502       \u2514\u2500\u2500 outputs.tf\n\u251c\u2500\u2500 environments/\n\u2502   \u251c\u2500\u2500 dev/\n\u2502   \u2502   \u2514\u2500\u2500 main.tf\n\u2502   \u2514\u2500\u2500 prod/\n\u2502       \u2514\u2500\u2500 main.tf\n\u251c\u2500\u2500 terraform.tf (backend config)\n\u251c\u2500\u2500 variables.tf\n\u251c\u2500\u2500 outputs.tf\n\u2514\u2500\u2500 main.tf\n</code></pre>","tags":["cloud","terraform","aws","infrastructure-as-code","resume-challenge"]},{"location":"blog/cloud-resume-challenge-with-terraform-introduction--setup/#define-the-backend-in-terraformtf","title":"Define the backend in <code>terraform.tf</code>","text":"<pre><code>terraform {\n  backend \"s3\" {\n    bucket         = \"my-terraform-state-bucket\"\n    key            = \"cloud-resume/state.tfstate\"\n    region         = \"us-east-1\"\n    encrypt        = true\n    dynamodb_table = \"terraform-lock\"\n  }\n\n  required_providers {\n    aws = {\n      source  = \"hashicorp/aws\"\n      version = \"~&gt; 4.16\"\n    }\n  }\n\n  required_version = \"&gt;= 1.2.0\"\n}\n</code></pre>","tags":["cloud","terraform","aws","infrastructure-as-code","resume-challenge"]},{"location":"blog/cloud-resume-challenge-with-terraform-introduction--setup/#create-s3-bucket-and-dynamodb-table-for-backend","title":"Create S3 Bucket and DynamoDB Table for Backend","text":"<p>Before you can use an S3 backend, you need to create the bucket and DynamoDB table. I prefer to do this via Terraform as well, using a separate configuration:</p> <pre><code># backend-setup/main.tf\nprovider \"aws\" {\n  region = \"us-east-1\"\n}\n\nresource \"aws_s3_bucket\" \"terraform_state\" {\n  bucket = \"my-terraform-state-bucket\"\n}\n\nresource \"aws_s3_bucket_versioning\" \"terraform_state\" {\n  bucket = aws_s3_bucket.terraform_state.id\n  versioning_configuration {\n    status = \"Enabled\"\n  }\n}\n\nresource \"aws_s3_bucket_server_side_encryption_configuration\" \"terraform_state\" {\n  bucket = aws_s3_bucket.terraform_state.id\n  rule {\n    apply_server_side_encryption_by_default {\n      sse_algorithm = \"AES256\"\n    }\n  }\n}\n\nresource \"aws_dynamodb_table\" \"terraform_locks\" {\n  name         = \"terraform-lock\"\n  billing_mode = \"PAY_PER_REQUEST\"\n  hash_key     = \"LockID\"\n\n  attribute {\n    name = \"LockID\"\n    type = \"S\"\n  }\n}\n</code></pre> <p>Run these commands to set up your backend:</p> <pre><code>cd backend-setup\nterraform init\nterraform apply\ncd ..\nterraform init  # Initialize with the S3 backend\n</code></pre>","tags":["cloud","terraform","aws","infrastructure-as-code","resume-challenge"]},{"location":"blog/cloud-resume-challenge-with-terraform-introduction--setup/#a-note-on-security","title":"A Note on Security \ud83d\udd12","text":"<p>Throughout this series, I'll be emphasizing security best practices. Some key principles to keep in mind:</p> <ol> <li>Never commit AWS credentials to your repository</li> <li>Use IAM roles with least privilege for all resources</li> <li>Enable encryption for sensitive data</li> <li>Implement proper security groups and network ACLs</li> <li>Regularly rotate credentials and keys</li> </ol> <p>These principles will be applied to our infrastructure as we build it in the upcoming posts.</p>","tags":["cloud","terraform","aws","infrastructure-as-code","resume-challenge"]},{"location":"blog/cloud-resume-challenge-with-terraform-introduction--setup/#lessons-learned","title":"Lessons Learned \ud83d\udca1","text":"<p>In my initial attempts at setting up the Terraform environment, I encountered several challenges:</p> <ol> <li> <p>State file management: I initially stored state locally, which caused problems when working from different computers. Switching to S3 backend solved this issue.</p> </li> <li> <p>Module organization: I tried several directory structures before settling on the current one. Organizing by component type rather than AWS service made the most sense for this project.</p> </li> <li> <p>Version constraints: Not specifying version constraints for providers led to unexpected behavior when Terraform updated. Always specify your provider versions!</p> </li> </ol>","tags":["cloud","terraform","aws","infrastructure-as-code","resume-challenge"]},{"location":"blog/cloud-resume-challenge-with-terraform-introduction--setup/#next-steps","title":"Next Steps \u23ed\ufe0f","text":"<p>In the next post, we'll build the static website infrastructure with S3, CloudFront, Route 53, and ACM. We'll create Terraform modules for each component and deploy them together to host our resume.</p>","tags":["cloud","terraform","aws","infrastructure-as-code","resume-challenge"]},{"location":"blog/cloud-resume-challenge-with-terraform-introduction--setup/#developer-mod-advanced-terraform-techniques","title":"Developer Mod: Advanced Terraform Techniques \ud83d\ude80","text":"<p>If you're familiar with Terraform and want to take this challenge further, consider implementing these enhancements:</p> <ol> <li> <p>Terraform Cloud Integration: Connect your repository to Terraform Cloud for enhanced collaboration and run history.</p> </li> <li> <p>Terratest: Add infrastructure tests using the Terratest framework to validate your configurations.</p> </li> <li> <p>Custom Terraform Modules: Create reusable modules and publish them to the Terraform Registry.</p> </li> <li> <p>Terraform Workspaces: Use workspaces to manage multiple environments (dev, staging, prod) within the same Terraform configuration.</p> </li> </ol> <p>Up Next: [Cloud Resume Challenge with Terraform: Deploying the Static Website] \ud83d\udd17</p> <p>Share on  Share on </p>","tags":["cloud","terraform","aws","infrastructure-as-code","resume-challenge"]},{"location":"blog/cloud-resume-challenge-with-terraform-deploying-the-static-website/","title":"Cloud Resume Challenge with Terraform: Deploying the Static Website \ud83d\ude80","text":"","tags":["s3","cloudfront","route53","tls","terraform","website-hosting"]},{"location":"blog/cloud-resume-challenge-with-terraform-deploying-the-static-website/#introduction","title":"Introduction \ud83c\udf0d","text":"<p>In the previous post, we set up our Terraform environment and outlined the architecture for our Cloud Resume Challenge project. Now it's time to start building! In this post, we'll focus on deploying the first component: the static website that will host our resume.</p>","tags":["s3","cloudfront","route53","tls","terraform","website-hosting"]},{"location":"blog/cloud-resume-challenge-with-terraform-deploying-the-static-website/#frontend-architecture-overview","title":"Frontend Architecture Overview \ud83c\udfd7\ufe0f","text":"<p>Let's look at the specific architecture we'll implement for our frontend:</p> <pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510     \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510     \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510     \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502           \u2502     \u2502            \u2502     \u2502          \u2502     \u2502            \u2502\n\u2502  Route 53 \u251c\u2500\u2500\u2500\u2500\u2500\u25ba CloudFront \u251c\u2500\u2500\u2500\u2500\u2500\u25ba    S3    \u2502     \u2502    ACM     \u2502\n\u2502           \u2502     \u2502            \u2502     \u2502          \u2502     \u2502 Certificate\u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518     \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518     \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518     \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n      \u25b2                                    \u25b2                 \u2502\n      \u2502                                    \u2502                 \u2502\n      \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n           DNS &amp; Certificate Validation\n</code></pre> <p>The frontend consists of:</p> <ol> <li>S3 Bucket: Hosts our HTML, CSS, and JavaScript files</li> <li>CloudFront: Provides CDN capabilities for global distribution and HTTPS</li> <li>Route 53: Manages our custom domain's DNS</li> <li>ACM: Provides SSL/TLS certificate for HTTPS</li> </ol>","tags":["s3","cloudfront","route53","tls","terraform","website-hosting"]},{"location":"blog/cloud-resume-challenge-with-terraform-deploying-the-static-website/#my-htmlcss-resume-design-approach","title":"My HTML/CSS Resume Design Approach \ud83c\udfa8","text":"<p>Before diving into Terraform, I spent some time creating my resume in HTML and CSS. Rather than starting from scratch, I decided to use a minimalist approach with a focus on readability.</p> <p>Here's a snippet of my HTML structure:</p> <pre><code>&lt;!DOCTYPE html&gt;\n&lt;html lang=\"en\"&gt;\n&lt;head&gt;\n    &lt;meta charset=\"UTF-8\"&gt;\n    &lt;meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\"&gt;\n    &lt;title&gt;Matthew's Cloud Resume&lt;/title&gt;\n    &lt;link rel=\"stylesheet\" href=\"styles.css\"&gt;\n&lt;/head&gt;\n&lt;body&gt;\n    &lt;header&gt;\n        &lt;h1&gt;Matthew Johnson&lt;/h1&gt;\n        &lt;p&gt;Cloud Engineer&lt;/p&gt;\n    &lt;/header&gt;\n\n    &lt;section id=\"contact\"&gt;\n        &lt;!-- Contact information --&gt;\n    &lt;/section&gt;\n\n    &lt;section id=\"skills\"&gt;\n        &lt;!-- Skills list --&gt;\n    &lt;/section&gt;\n\n    &lt;section id=\"experience\"&gt;\n        &lt;!-- Work experience --&gt;\n    &lt;/section&gt;\n\n    &lt;section id=\"education\"&gt;\n        &lt;!-- Education history --&gt;\n    &lt;/section&gt;\n\n    &lt;section id=\"certifications\"&gt;\n        &lt;!-- AWS certifications --&gt;\n    &lt;/section&gt;\n\n    &lt;section id=\"projects\"&gt;\n        &lt;!-- Project descriptions including this challenge --&gt;\n    &lt;/section&gt;\n\n    &lt;section id=\"counter\"&gt;\n        &lt;p&gt;This page has been viewed &lt;span id=\"count\"&gt;0&lt;/span&gt; times.&lt;/p&gt;\n    &lt;/section&gt;\n\n    &lt;footer&gt;\n        &lt;!-- Footer content --&gt;\n    &lt;/footer&gt;\n\n    &lt;script src=\"counter.js\"&gt;&lt;/script&gt;\n&lt;/body&gt;\n&lt;/html&gt;\n</code></pre> <p>For CSS, I went with a responsive design that works well on both desktop and mobile devices:</p> <pre><code>:root {\n    --primary-color: #0066cc;\n    --secondary-color: #f4f4f4;\n    --text-color: #333;\n    --heading-color: #222;\n}\n\nbody {\n    font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif;\n    line-height: 1.6;\n    color: var(--text-color);\n    max-width: 800px;\n    margin: 0 auto;\n    padding: 1rem;\n}\n\nheader {\n    text-align: center;\n    margin-bottom: 2rem;\n}\n\nh1, h2, h3 {\n    color: var(--heading-color);\n}\n\nsection {\n    margin-bottom: 2rem;\n}\n\n/* Responsive design */\n@media (max-width: 600px) {\n    body {\n        padding: 0.5rem;\n    }\n}\n</code></pre> <p>These files will be uploaded to our S3 bucket once we've provisioned it with Terraform.</p>","tags":["s3","cloudfront","route53","tls","terraform","website-hosting"]},{"location":"blog/cloud-resume-challenge-with-terraform-deploying-the-static-website/#deploying-the-static-website-with-terraform","title":"Deploying the Static Website with Terraform \ud83c\udf10","text":"<p>Now, let's implement the Terraform code for our frontend infrastructure. We'll create modules for each component, starting with S3.</p>","tags":["s3","cloudfront","route53","tls","terraform","website-hosting"]},{"location":"blog/cloud-resume-challenge-with-terraform-deploying-the-static-website/#1-s3-module-for-website-hosting","title":"1. S3 Module for Website Hosting \ud83d\udcc2","text":"<p>Create a file at <code>modules/frontend/s3.tf</code>:</p> <pre><code>resource \"aws_s3_bucket\" \"website\" {\n  bucket = var.website_bucket_name\n\n  tags = {\n    Name        = \"Resume Website\"\n    Environment = var.environment\n    Project     = \"Cloud Resume Challenge\"\n  }\n}\n\nresource \"aws_s3_bucket_website_configuration\" \"website\" {\n  bucket = aws_s3_bucket.website.id\n\n  index_document {\n    suffix = \"index.html\"\n  }\n\n  error_document {\n    key = \"error.html\"\n  }\n}\n\nresource \"aws_s3_bucket_cors_configuration\" \"website\" {\n  bucket = aws_s3_bucket.website.id\n\n  cors_rule {\n    allowed_headers = [\"*\"]\n    allowed_methods = [\"GET\", \"HEAD\"]\n    allowed_origins = [\"*\"]  # In production, restrict to your domain\n    expose_headers  = [\"ETag\"]\n    max_age_seconds = 3000\n  }\n}\n\nresource \"aws_s3_bucket_policy\" \"website\" {\n  bucket = aws_s3_bucket.website.id\n  policy = jsonencode({\n    Version = \"2012-10-17\"\n    Statement = [\n      {\n        Sid       = \"PublicReadGetObject\"\n        Effect    = \"Allow\"\n        Principal = \"*\"\n        Action    = \"s3:GetObject\"\n        Resource  = \"${aws_s3_bucket.website.arn}/*\"\n      }\n    ]\n  })\n}\n\n# Enable versioning for rollback capability\nresource \"aws_s3_bucket_versioning\" \"website\" {\n  bucket = aws_s3_bucket.website.id\n  versioning_configuration {\n    status = \"Enabled\"\n  }\n}\n\n# Add encryption for security\nresource \"aws_s3_bucket_server_side_encryption_configuration\" \"website\" {\n  bucket = aws_s3_bucket.website.id\n\n  rule {\n    apply_server_side_encryption_by_default {\n      sse_algorithm = \"AES256\"\n    }\n  }\n}\n</code></pre> <p>Notice that I've included CORS configuration, which will be essential later when we integrate with our API. I also added encryption and versioning for better security and disaster recovery.</p>","tags":["s3","cloudfront","route53","tls","terraform","website-hosting"]},{"location":"blog/cloud-resume-challenge-with-terraform-deploying-the-static-website/#2-acm-certificate-module","title":"2. ACM Certificate Module \ud83d\udd12","text":"<p>Create a file at <code>modules/frontend/acm.tf</code>:</p> <pre><code>resource \"aws_acm_certificate\" \"website\" {\n  domain_name       = var.domain_name\n  validation_method = \"DNS\"\n\n  subject_alternative_names = [\"www.${var.domain_name}\"]\n\n  lifecycle {\n    create_before_destroy = true\n  }\n\n  tags = {\n    Name        = \"Resume Website Certificate\"\n    Environment = var.environment\n  }\n}\n\nresource \"aws_acm_certificate_validation\" \"website\" {\n  certificate_arn         = aws_acm_certificate.website.arn\n  validation_record_fqdns = [for record in aws_route53_record.certificate_validation : record.fqdn]\n\n  # Wait for DNS propagation\n  timeouts {\n    create = \"30m\"\n  }\n}\n</code></pre>","tags":["s3","cloudfront","route53","tls","terraform","website-hosting"]},{"location":"blog/cloud-resume-challenge-with-terraform-deploying-the-static-website/#3-route-53-for-dns-configuration","title":"3. Route 53 for DNS Configuration \ud83d\udce1","text":"<p>Create a file at <code>modules/frontend/route53.tf</code>:</p> <pre><code>data \"aws_route53_zone\" \"selected\" {\n  name         = var.root_domain_name\n  private_zone = false\n}\n\nresource \"aws_route53_record\" \"website\" {\n  zone_id = data.aws_route53_zone.selected.zone_id\n  name    = var.domain_name\n  type    = \"A\"\n\n  alias {\n    name                   = aws_cloudfront_distribution.website.domain_name\n    zone_id                = aws_cloudfront_distribution.website.hosted_zone_id\n    evaluate_target_health = false\n  }\n}\n\nresource \"aws_route53_record\" \"www\" {\n  zone_id = data.aws_route53_zone.selected.zone_id\n  name    = \"www.${var.domain_name}\"\n  type    = \"A\"\n\n  alias {\n    name                   = aws_cloudfront_distribution.website.domain_name\n    zone_id                = aws_cloudfront_distribution.website.hosted_zone_id\n    evaluate_target_health = false\n  }\n}\n\nresource \"aws_route53_record\" \"certificate_validation\" {\n  for_each = {\n    for dvo in aws_acm_certificate.website.domain_validation_options : dvo.domain_name =&gt; {\n      name   = dvo.resource_record_name\n      record = dvo.resource_record_value\n      type   = dvo.resource_record_type\n    }\n  }\n\n  allow_overwrite = true\n  name            = each.value.name\n  records         = [each.value.record]\n  ttl             = 60\n  type            = each.value.type\n  zone_id         = data.aws_route53_zone.selected.zone_id\n}\n</code></pre>","tags":["s3","cloudfront","route53","tls","terraform","website-hosting"]},{"location":"blog/cloud-resume-challenge-with-terraform-deploying-the-static-website/#4-cloudfront-distribution-for-cdn-and-https","title":"4. CloudFront Distribution for CDN and HTTPS \ud83c\udf0d","text":"<p>Create a file at <code>modules/frontend/cloudfront.tf</code>:</p> <pre><code>resource \"aws_cloudfront_distribution\" \"website\" {\n  origin {\n    domain_name = aws_s3_bucket.website.bucket_regional_domain_name\n    origin_id   = \"S3-${var.website_bucket_name}\"\n\n    s3_origin_config {\n      origin_access_identity = aws_cloudfront_origin_access_identity.website.cloudfront_access_identity_path\n    }\n  }\n\n  enabled             = true\n  is_ipv6_enabled     = true\n  default_root_object = \"index.html\"\n  aliases             = [var.domain_name, \"www.${var.domain_name}\"]\n  price_class         = \"PriceClass_100\"\n\n  default_cache_behavior {\n    allowed_methods  = [\"GET\", \"HEAD\", \"OPTIONS\"]\n    cached_methods   = [\"GET\", \"HEAD\"]\n    target_origin_id = \"S3-${var.website_bucket_name}\"\n\n    forwarded_values {\n      query_string = false\n      cookies {\n        forward = \"none\"\n      }\n    }\n\n    viewer_protocol_policy = \"redirect-to-https\"\n    min_ttl                = 0\n    default_ttl            = 3600\n    max_ttl                = 86400\n    compress               = true\n  }\n\n  # Cache behaviors for specific patterns\n  ordered_cache_behavior {\n    path_pattern     = \"*.js\"\n    allowed_methods  = [\"GET\", \"HEAD\", \"OPTIONS\"]\n    cached_methods   = [\"GET\", \"HEAD\"]\n    target_origin_id = \"S3-${var.website_bucket_name}\"\n\n    forwarded_values {\n      query_string = false\n      cookies {\n        forward = \"none\"\n      }\n    }\n\n    viewer_protocol_policy = \"redirect-to-https\"\n    min_ttl                = 0\n    default_ttl            = 86400\n    max_ttl                = 31536000\n    compress               = true\n  }\n\n  ordered_cache_behavior {\n    path_pattern     = \"*.css\"\n    allowed_methods  = [\"GET\", \"HEAD\", \"OPTIONS\"]\n    cached_methods   = [\"GET\", \"HEAD\"]\n    target_origin_id = \"S3-${var.website_bucket_name}\"\n\n    forwarded_values {\n      query_string = false\n      cookies {\n        forward = \"none\"\n      }\n    }\n\n    viewer_protocol_policy = \"redirect-to-https\"\n    min_ttl                = 0\n    default_ttl            = 86400\n    max_ttl                = 31536000\n    compress               = true\n  }\n\n  # Restrict access to North America and Europe\n  restrictions {\n    geo_restriction {\n      restriction_type = \"whitelist\"\n      locations        = [\"US\", \"CA\", \"GB\", \"DE\", \"FR\", \"ES\", \"IT\"]\n    }\n  }\n\n  viewer_certificate {\n    acm_certificate_arn      = aws_acm_certificate.website.arn\n    ssl_support_method       = \"sni-only\"\n    minimum_protocol_version = \"TLSv1.2_2021\"\n  }\n\n  # Add custom error response\n  custom_error_response {\n    error_code            = 404\n    response_code         = 404\n    response_page_path    = \"/error.html\"\n    error_caching_min_ttl = 10\n  }\n\n  tags = {\n    Name        = \"Resume Website CloudFront\"\n    Environment = var.environment\n  }\n\n  depends_on = [aws_acm_certificate_validation.website]\n}\n\nresource \"aws_cloudfront_origin_access_identity\" \"website\" {\n  comment = \"Access identity for Resume Website CloudFront\"\n}\n\n# Update S3 bucket policy to allow access from CloudFront\nresource \"aws_s3_bucket_policy\" \"cloudfront_access\" {\n  bucket = aws_s3_bucket.website.id\n  policy = jsonencode({\n    Version = \"2012-10-17\"\n    Statement = [\n      {\n        Sid       = \"AllowCloudFrontServicePrincipal\"\n        Effect    = \"Allow\"\n        Principal = {\n          Service = \"cloudfront.amazonaws.com\"\n        }\n        Action    = \"s3:GetObject\"\n        Resource  = \"${aws_s3_bucket.website.arn}/*\"\n        Condition = {\n          StringEquals = {\n            \"AWS:SourceArn\" = aws_cloudfront_distribution.website.arn\n          }\n        }\n      }\n    ]\n  })\n}\n</code></pre> <p>I've implemented several security enhancements:</p> <ul> <li>Using origin access control for CloudFront</li> <li>Restricting the content to specific geographic regions</li> <li>Setting TLS to more modern protocols</li> <li>Creating custom error pages</li> <li>Adding better cache controls for different file types</li> </ul>","tags":["s3","cloudfront","route53","tls","terraform","website-hosting"]},{"location":"blog/cloud-resume-challenge-with-terraform-deploying-the-static-website/#5-variables-and-outputs","title":"5. Variables and Outputs \ud83d\udcdd","text":"<p>Create files at <code>modules/frontend/variables.tf</code> and <code>modules/frontend/outputs.tf</code>:</p> <p>variables.tf:</p> <pre><code>variable \"website_bucket_name\" {\n  description = \"Name of the S3 bucket to store website content\"\n  type        = string\n}\n\nvariable \"domain_name\" {\n  description = \"Domain name for the website\"\n  type        = string\n}\n\nvariable \"root_domain_name\" {\n  description = \"Root domain name to find Route 53 hosted zone\"\n  type        = string\n}\n\nvariable \"environment\" {\n  description = \"Deployment environment (e.g., dev, prod)\"\n  type        = string\n  default     = \"dev\"\n}\n</code></pre> <p>outputs.tf:</p> <pre><code>output \"website_bucket_name\" {\n  description = \"Name of the S3 bucket hosting the website\"\n  value       = aws_s3_bucket.website.id\n}\n\noutput \"cloudfront_distribution_id\" {\n  description = \"ID of the CloudFront distribution\"\n  value       = aws_cloudfront_distribution.website.id\n}\n\noutput \"website_domain\" {\n  description = \"Domain name of the website\"\n  value       = var.domain_name\n}\n\noutput \"cloudfront_domain_name\" {\n  description = \"CloudFront domain name\"\n  value       = aws_cloudfront_distribution.website.domain_name\n}\n</code></pre>","tags":["s3","cloudfront","route53","tls","terraform","website-hosting"]},{"location":"blog/cloud-resume-challenge-with-terraform-deploying-the-static-website/#6-main-module-configuration","title":"6. Main Module Configuration \ud83d\udd04","text":"<p>Now, let's create the main configuration in <code>main.tf</code> that uses our frontend module:</p> <pre><code>provider \"aws\" {\n  region = \"us-east-1\"\n}\n\nmodule \"frontend\" {\n  source = \"./modules/frontend\"\n\n  website_bucket_name = \"my-resume-website-${var.environment}\"\n  domain_name         = var.domain_name\n  root_domain_name    = var.root_domain_name\n  environment         = var.environment\n}\n</code></pre> <p>In <code>variables.tf</code> at the root level:</p> <pre><code>variable \"environment\" {\n  description = \"Deployment environment (e.g., dev, prod)\"\n  type        = string\n  default     = \"dev\"\n}\n\nvariable \"domain_name\" {\n  description = \"Domain name for the website\"\n  type        = string\n}\n\nvariable \"root_domain_name\" {\n  description = \"Root domain name to find Route 53 hosted zone\"\n  type        = string\n}\n</code></pre>","tags":["s3","cloudfront","route53","tls","terraform","website-hosting"]},{"location":"blog/cloud-resume-challenge-with-terraform-deploying-the-static-website/#7-uploading-content-to-s3","title":"7. Uploading Content to S3 \ud83d\udce4","text":"<p>We can use Terraform to upload our website files to S3:</p> <pre><code># Add to modules/frontend/s3.tf\nresource \"aws_s3_object\" \"html\" {\n  bucket       = aws_s3_bucket.website.id\n  key          = \"index.html\"\n  source       = \"${path.module}/../../website/index.html\"\n  content_type = \"text/html\"\n  etag         = filemd5(\"${path.module}/../../website/index.html\")\n}\n\nresource \"aws_s3_object\" \"css\" {\n  bucket       = aws_s3_bucket.website.id\n  key          = \"styles.css\"\n  source       = \"${path.module}/../../website/styles.css\"\n  content_type = \"text/css\"\n  etag         = filemd5(\"${path.module}/../../website/styles.css\")\n}\n\nresource \"aws_s3_object\" \"js\" {\n  bucket       = aws_s3_bucket.website.id\n  key          = \"counter.js\"\n  source       = \"${path.module}/../../website/counter.js\"\n  content_type = \"application/javascript\"\n  etag         = filemd5(\"${path.module}/../../website/counter.js\")\n}\n\nresource \"aws_s3_object\" \"error_page\" {\n  bucket       = aws_s3_bucket.website.id\n  key          = \"error.html\"\n  source       = \"${path.module}/../../website/error.html\"\n  content_type = \"text/html\"\n  etag         = filemd5(\"${path.module}/../../website/error.html\")\n}\n</code></pre>","tags":["s3","cloudfront","route53","tls","terraform","website-hosting"]},{"location":"blog/cloud-resume-challenge-with-terraform-deploying-the-static-website/#testing-your-deployment","title":"Testing Your Deployment \ud83e\uddea","text":"<p>After applying these Terraform configurations, you'll want to test that everything is working correctly:</p> <pre><code># Initialize Terraform\nterraform init\n\n# Plan the deployment\nterraform plan -var=\"domain_name=resume.yourdomain.com\" -var=\"root_domain_name=yourdomain.com\" -var=\"environment=dev\"\n\n# Apply the changes\nterraform apply -var=\"domain_name=resume.yourdomain.com\" -var=\"root_domain_name=yourdomain.com\" -var=\"environment=dev\"\n</code></pre> <p>Once deployment is complete, verify:</p> <ol> <li>Your domain resolves to your CloudFront distribution</li> <li>HTTPS is working correctly</li> <li>Your resume appears as expected</li> <li>The website is accessible from different locations</li> </ol>","tags":["s3","cloudfront","route53","tls","terraform","website-hosting"]},{"location":"blog/cloud-resume-challenge-with-terraform-deploying-the-static-website/#troubleshooting-common-issues","title":"Troubleshooting Common Issues \u26a0\ufe0f","text":"<p>During my implementation, I encountered several challenges:</p> <ol> <li> <p>ACM Certificate Validation Delays: It can take up to 30 minutes for certificate validation to complete. Be patient or use the AWS console to monitor progress.</p> </li> <li> <p>CloudFront Distribution Propagation: CloudFront changes can take 15-20 minutes to propagate globally. If your site isn't loading correctly, wait and try again.</p> </li> <li> <p>S3 Bucket Policy Conflicts: If you receive errors about conflicting bucket policies, ensure that you're not applying multiple policies to the same bucket.</p> </li> <li> <p>CORS Configuration: Without proper CORS headers, your JavaScript won't be able to communicate with your API when we build it in the next post.</p> </li> </ol>","tags":["s3","cloudfront","route53","tls","terraform","website-hosting"]},{"location":"blog/cloud-resume-challenge-with-terraform-deploying-the-static-website/#cors-configuration-for-api-integration","title":"CORS Configuration for API Integration \ud83d\udd04","text":"<p>The Cloud Resume Challenge requires a JavaScript visitor counter that communicates with an API. To prepare for this, I've added CORS configuration to our S3 bucket. When we implement the API in the next post, we'll need to ensure it allows requests from our domain.</p> <p>Here's the JavaScript snippet we'll use for the counter (to be implemented fully in the next post):</p> <pre><code>// counter.js\ndocument.addEventListener('DOMContentLoaded', function() {\n  // We'll need to fetch from our API\n  // Example: https://api.yourdomain.com/visitor-count\n\n  // For now, just a placeholder\n  document.getElementById('count').innerText = 'Loading...';\n\n  // This will be implemented fully when we create our API\n  // fetch('https://api.yourdomain.com/visitor-count')\n  //   .then(response =&gt; response.json())\n  //   .then(data =&gt; {\n  //     document.getElementById('count').innerText = data.count;\n  //   })\n  //   .catch(error =&gt; console.error('Error fetching visitor count:', error));\n});\n</code></pre>","tags":["s3","cloudfront","route53","tls","terraform","website-hosting"]},{"location":"blog/cloud-resume-challenge-with-terraform-deploying-the-static-website/#lessons-learned","title":"Lessons Learned \ud83d\udca1","text":"<ol> <li> <p>Domain Verification: I initially struggled with ACM certificate validation. The key lesson was to ensure that the Route 53 hosted zone existed before attempting to create validation records.</p> </li> <li> <p>Terraform State Management: When modifying existing resources, it's important to understand how Terraform tracks state. A single typo can lead to resource recreation rather than updates.</p> </li> <li> <p>Performance Optimization: Adding specific cache behaviors for CSS and JS files significantly improved page load times. It's worth taking the time to optimize these settings.</p> </li> <li> <p>Security Considerations: Setting up proper bucket policies and CloudFront origin access identity is critical to prevent direct access to your S3 bucket while still allowing CloudFront to serve content.</p> </li> </ol>","tags":["s3","cloudfront","route53","tls","terraform","website-hosting"]},{"location":"blog/cloud-resume-challenge-with-terraform-deploying-the-static-website/#enhancements-and-mods","title":"Enhancements and Mods \ud83d\ude80","text":"<p>Here are some ways to extend this part of the challenge:</p>","tags":["s3","cloudfront","route53","tls","terraform","website-hosting"]},{"location":"blog/cloud-resume-challenge-with-terraform-deploying-the-static-website/#developer-mod-static-site-generator","title":"Developer Mod: Static Site Generator","text":"<p>Instead of writing plain HTML/CSS, consider using a static site generator like Hugo or Jekyll:</p> <ol> <li>Install Hugo: <code>brew install hugo</code> (on macOS) or equivalent for your OS</li> <li>Create a new site: <code>hugo new site resume-site</code></li> <li>Choose a theme or create your own</li> <li>Generate the site: <code>hugo -D</code></li> <li>Modify your Terraform to upload the <code>public</code> directory contents to S3</li> </ol> <p>This approach gives you templating capabilities, making it easier to update and maintain your resume.</p>","tags":["s3","cloudfront","route53","tls","terraform","website-hosting"]},{"location":"blog/cloud-resume-challenge-with-terraform-deploying-the-static-website/#devops-mod-content-invalidation-lambda","title":"DevOps Mod: Content Invalidation Lambda","text":"<p>Create a Lambda function that automatically invalidates CloudFront cache when new content is uploaded to S3:</p> <pre><code>resource \"aws_lambda_function\" \"invalidation\" {\n  filename      = \"lambda_function.zip\"\n  function_name = \"cloudfront-invalidation\"\n  role          = aws_iam_role.lambda_role.arn\n  handler       = \"index.handler\"\n  runtime       = \"nodejs14.x\"\n\n  environment {\n    variables = {\n      DISTRIBUTION_ID = aws_cloudfront_distribution.website.id\n    }\n  }\n}\n\nresource \"aws_s3_bucket_notification\" \"bucket_notification\" {\n  bucket = aws_s3_bucket.website.id\n\n  lambda_function {\n    lambda_function_arn = aws_lambda_function.invalidation.arn\n    events              = [\"s3:ObjectCreated:*\", \"s3:ObjectRemoved:*\"]\n  }\n}\n</code></pre>","tags":["s3","cloudfront","route53","tls","terraform","website-hosting"]},{"location":"blog/cloud-resume-challenge-with-terraform-deploying-the-static-website/#security-mod-implement-dnssec","title":"Security Mod: Implement DNSSEC","text":"<p>To prevent DNS spoofing attacks, implement DNSSEC for your domain:</p> <pre><code>resource \"aws_route53_key_signing_key\" \"example\" {\n  hosted_zone_id             = data.aws_route53_zone.selected.id\n  key_management_service_arn = aws_kms_key.dnssec.arn\n  name                       = \"example\"\n}\n\nresource \"aws_route53_hosted_zone_dnssec\" \"example\" {\n  hosted_zone_id = aws_route53_key_signing_key.example.hosted_zone_id\n}\n\nresource \"aws_kms_key\" \"dnssec\" {\n  customer_master_key_spec = \"ECC_NIST_P256\"\n  deletion_window_in_days  = 7\n  key_usage                = \"SIGN_VERIFY\"\n  policy = jsonencode({\n    Statement = [\n      {\n        Action = [\n          \"kms:DescribeKey\",\n          \"kms:GetPublicKey\",\n          \"kms:Sign\",\n        ],\n        Effect = \"Allow\",\n        Principal = {\n          Service = \"dnssec-route53.amazonaws.com\"\n        },\n        Resource = \"*\"\n      },\n      {\n        Action = \"kms:*\",\n        Effect = \"Allow\",\n        Principal = {\n          AWS = \"*\"\n        },\n        Resource = \"*\"\n      }\n    ]\n    Version = \"2012-10-17\"\n  })\n}\n</code></pre>","tags":["s3","cloudfront","route53","tls","terraform","website-hosting"]},{"location":"blog/cloud-resume-challenge-with-terraform-deploying-the-static-website/#next-steps","title":"Next Steps \u23ed\ufe0f","text":"<p>With our static website infrastructure in place, we now have a live resume hosted on AWS with a custom domain and HTTPS. In the next post, we'll build the backend API using API Gateway, Lambda, and DynamoDB to track visitor counts.</p> <p>Stay tuned to see how we implement the serverless backend and connect it to our frontend!</p> <p>Up Next: [Cloud Resume Challenge with Terraform: Building the Backend API] \ud83d\udd17</p> <p>Share on  Share on </p>","tags":["s3","cloudfront","route53","tls","terraform","website-hosting"]},{"location":"blog/cloud-resume-challenge-with-terraform-building-the-backend-api/","title":"Cloud Resume Challenge with Terraform: Building the Backend API \ud83d\ude80","text":"<p>In our previous post, we set up the frontend infrastructure for our resume website using Terraform. Now it's time to build the backend API that will power our visitor counter.</p>","tags":["lambda","api-gateway","dynamodb","terraform","serverless"]},{"location":"blog/cloud-resume-challenge-with-terraform-building-the-backend-api/#backend-architecture-overview","title":"Backend Architecture Overview \ud83c\udfd7\ufe0f","text":"<p>Let's take a look at the serverless architecture we'll be implementing:</p> <pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510     \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510     \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502             \u2502     \u2502             \u2502     \u2502             \u2502\n\u2502 API Gateway \u251c\u2500\u2500\u2500\u2500\u2500\u25ba Lambda      \u251c\u2500\u2500\u2500\u2500\u2500\u25ba DynamoDB    \u2502\n\u2502             \u2502     \u2502             \u2502     \u2502             \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518     \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518     \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n       \u2502                   \u2502                   \u2502\n       \u2502                   \u2502                   \u2502\n       \u25bc                   \u25bc                   \u25bc\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510     \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510     \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502             \u2502     \u2502             \u2502     \u2502             \u2502\n\u2502 CloudWatch  \u2502     \u2502 CloudWatch  \u2502     \u2502 CloudWatch  \u2502\n\u2502   Logs      \u2502     \u2502   Logs      \u2502     \u2502   Logs      \u2502\n\u2502             \u2502     \u2502             \u2502     \u2502             \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518     \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518     \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre> <p>This architecture includes:</p> <ol> <li>API Gateway: Exposes our Lambda function as a REST API</li> <li>Lambda Function: Contains the Python code to increment and return the visitor count</li> <li>DynamoDB: Stores the visitor count data</li> <li>CloudWatch: Monitors and logs activity across all services</li> </ol>","tags":["lambda","api-gateway","dynamodb","terraform","serverless"]},{"location":"blog/cloud-resume-challenge-with-terraform-building-the-backend-api/#my-approach-to-dynamodb-design","title":"My Approach to DynamoDB Design \ud83d\udcbe","text":"<p>Before diving into the Terraform code, I want to share my thought process on DynamoDB table design. When I initially approached this challenge, I had to decide between two approaches:</p> <ol> <li>Single-counter approach: A simple table with just one item for the counter</li> <li>Visitor log approach: A more detailed table that logs each visit with timestamps</li> </ol> <p>I chose the second approach for a few reasons:</p> <ul> <li>It allows for more detailed analytics in the future</li> <li>It provides a history of visits that can be queried</li> <li>It demonstrates a more realistic use case for DynamoDB</li> </ul> <p>Here's my table design:</p> Attribute Type Description visit_id String Primary key (UUID) timestamp String ISO8601 timestamp of the visit visitor_ip String Hashed IP address for privacy user_agent String Browser/device information path String Page path visited <p>This approach gives us flexibility while keeping the solution serverless and cost-effective.</p>","tags":["lambda","api-gateway","dynamodb","terraform","serverless"]},{"location":"blog/cloud-resume-challenge-with-terraform-building-the-backend-api/#implementing-the-backend-api-with-terraform","title":"Implementing the Backend API with Terraform \ud83d\udee0\ufe0f","text":"<p>Now, let's start implementing our backend infrastructure using Terraform. We'll create modules for each component, starting with DynamoDB.</p>","tags":["lambda","api-gateway","dynamodb","terraform","serverless"]},{"location":"blog/cloud-resume-challenge-with-terraform-building-the-backend-api/#1-dynamodb-table-for-visitor-counting","title":"1. DynamoDB Table for Visitor Counting \ud83d\udcca","text":"<p>Create a file at <code>modules/backend/dynamodb.tf</code>:</p> <pre><code>resource \"aws_dynamodb_table\" \"visitor_counter\" {\n  name           = \"ResumeVisitorCounter-${var.environment}\"\n  billing_mode   = \"PAY_PER_REQUEST\"  # On-demand capacity for cost savings\n  hash_key       = \"visit_id\"\n\n  attribute {\n    name = \"visit_id\"\n    type = \"S\"\n  }\n\n  # Add TTL for automatic data cleanup after 90 days\n  ttl {\n    attribute_name = \"expiration_time\"\n    enabled        = true\n  }\n\n  point_in_time_recovery {\n    enabled = true  # Enable PITR for recovery options\n  }\n\n  # Use server-side encryption\n  server_side_encryption {\n    enabled = true\n  }\n\n  tags = {\n    Name        = \"Resume Visitor Counter\"\n    Environment = var.environment\n    Project     = \"Cloud Resume Challenge\"\n  }\n}\n\n# Create a GSI for timestamp-based queries\nresource \"aws_dynamodb_table_item\" \"counter_init\" {\n  table_name = aws_dynamodb_table.visitor_counter.name\n  hash_key   = aws_dynamodb_table.visitor_counter.hash_key\n\n  # Initialize the counter with a value of 0\n  item = jsonencode({\n    \"visit_id\": {\"S\": \"total\"},\n    \"count\": {\"N\": \"0\"}\n  })\n\n  # Only create this item on initial deployment\n  lifecycle {\n    ignore_changes = [item]\n  }\n}\n</code></pre> <p>I've implemented several enhancements:</p> <ul> <li>Point-in-time recovery for data protection</li> <li>TTL for automatic cleanup of old records</li> <li>Server-side encryption for security</li> <li>An initial counter item to ensure we don't have \"cold start\" issues</li> </ul>","tags":["lambda","api-gateway","dynamodb","terraform","serverless"]},{"location":"blog/cloud-resume-challenge-with-terraform-building-the-backend-api/#2-lambda-function-for-the-api-logic","title":"2. Lambda Function for the API Logic \ud83c\udfd7\ufe0f","text":"<p>Now, let's create our Lambda function. First, we'll need the Python code. Create a file at <code>modules/backend/lambda/visitor_counter.py</code>:</p> <pre><code>import boto3\nimport json\nimport os\nimport uuid\nimport logging\nfrom datetime import datetime, timedelta\nimport hashlib\nfrom botocore.exceptions import ClientError\n\n# Set up logging\nlogger = logging.getLogger()\nlogger.setLevel(logging.INFO)\n\n# Initialize DynamoDB client\ndynamodb = boto3.resource('dynamodb')\ntable_name = os.environ['DYNAMODB_TABLE']\ntable = dynamodb.Table(table_name)\n\ndef lambda_handler(event, context):\n    \"\"\"\n    Lambda handler to process API Gateway requests for visitor counting.\n    Increments the visitor counter and returns the updated count.\n    \"\"\"\n    logger.info(f\"Processing event: {json.dumps(event)}\")\n\n    try:\n        # Extract request information\n        request_context = event.get('requestContext', {})\n        http_method = event.get('httpMethod', '')\n        path = event.get('path', '')\n        headers = event.get('headers', {})\n        ip_address = request_context.get('identity', {}).get('sourceIp', 'unknown')\n        user_agent = headers.get('User-Agent', 'unknown')\n\n        # Generate a unique visit ID\n        visit_id = str(uuid.uuid4())\n\n        # Hash the IP address for privacy\n        hashed_ip = hashlib.sha256(ip_address.encode()).hexdigest()\n\n        # Get current timestamp\n        timestamp = datetime.utcnow().isoformat()\n\n        # Calculate expiration time (90 days from now)\n        expiration_time = int((datetime.utcnow() + timedelta(days=90)).timestamp())\n\n        # Log the visit\n        table.put_item(\n            Item={\n                'visit_id': visit_id,\n                'timestamp': timestamp,\n                'visitor_ip': hashed_ip,\n                'user_agent': user_agent,\n                'path': path,\n                'expiration_time': expiration_time\n            }\n        )\n\n        # Update the total counter\n        response = table.update_item(\n            Key={'visit_id': 'total'},\n            UpdateExpression='ADD #count :incr',\n            ExpressionAttributeNames={'#count': 'count'},\n            ExpressionAttributeValues={':incr': 1},\n            ReturnValues='UPDATED_NEW'\n        )\n\n        count = int(response['Attributes']['count'])\n\n        # Return the response\n        return {\n            'statusCode': 200,\n            'headers': {\n                'Content-Type': 'application/json',\n                'Access-Control-Allow-Origin': os.environ['ALLOWED_ORIGIN'],\n                'Access-Control-Allow-Methods': 'GET, OPTIONS',\n                'Access-Control-Allow-Headers': 'Content-Type'\n            },\n            'body': json.dumps({\n                'count': count,\n                'message': 'Visitor count updated successfully'\n            })\n        }\n\n    except ClientError as e:\n        logger.error(f\"DynamoDB error: {e}\")\n        return {\n            'statusCode': 500,\n            'headers': {\n                'Content-Type': 'application/json',\n                'Access-Control-Allow-Origin': os.environ.get('ALLOWED_ORIGIN', '*')\n            },\n            'body': json.dumps({\n                'error': 'Database error',\n                'message': str(e)\n            })\n        }\n    except Exception as e:\n        logger.error(f\"General error: {e}\")\n        return {\n            'statusCode': 500,\n            'headers': {\n                'Content-Type': 'application/json',\n                'Access-Control-Allow-Origin': os.environ.get('ALLOWED_ORIGIN', '*')\n            },\n            'body': json.dumps({\n                'error': 'Server error',\n                'message': str(e)\n            })\n        }\n\ndef options_handler(event, context):\n    \"\"\"\n    Handler for OPTIONS requests to support CORS\n    \"\"\"\n    return {\n        'statusCode': 200,\n        'headers': {\n            'Access-Control-Allow-Origin': os.environ.get('ALLOWED_ORIGIN', '*'),\n            'Access-Control-Allow-Methods': 'GET, OPTIONS',\n            'Access-Control-Allow-Headers': 'Content-Type'\n        },\n        'body': ''\n    }\n</code></pre> <p>Now, let's create the Lambda function using Terraform. Create a file at <code>modules/backend/lambda.tf</code>:</p> <pre><code># Archive the Lambda function code\ndata \"archive_file\" \"lambda_zip\" {\n  type        = \"zip\"\n  source_file = \"${path.module}/lambda/visitor_counter.py\"\n  output_path = \"${path.module}/lambda/visitor_counter.zip\"\n}\n\n# Create the Lambda function\nresource \"aws_lambda_function\" \"visitor_counter\" {\n  filename         = data.archive_file.lambda_zip.output_path\n  function_name    = \"resume-visitor-counter-${var.environment}\"\n  role             = aws_iam_role.lambda_role.arn\n  handler          = \"visitor_counter.lambda_handler\"\n  source_code_hash = data.archive_file.lambda_zip.output_base64sha256\n  runtime          = \"python3.9\"\n  timeout          = 10  # Increased timeout for better error handling\n  memory_size      = 128\n\n  environment {\n    variables = {\n      DYNAMODB_TABLE = aws_dynamodb_table.visitor_counter.name\n      ALLOWED_ORIGIN = var.website_domain\n    }\n  }\n\n  tracing_config {\n    mode = \"Active\"  # Enable X-Ray tracing\n  }\n\n  tags = {\n    Name        = \"Resume Visitor Counter Lambda\"\n    Environment = var.environment\n    Project     = \"Cloud Resume Challenge\"\n  }\n}\n\n# Create an IAM role for the Lambda function\nresource \"aws_iam_role\" \"lambda_role\" {\n  name = \"resume-visitor-counter-lambda-role-${var.environment}\"\n\n  assume_role_policy = jsonencode({\n    Version = \"2012-10-17\"\n    Statement = [\n      {\n        Action = \"sts:AssumeRole\"\n        Effect = \"Allow\"\n        Principal = {\n          Service = \"lambda.amazonaws.com\"\n        }\n      }\n    ]\n  })\n}\n\n# Create a custom policy for the Lambda function with least privilege\nresource \"aws_iam_policy\" \"lambda_policy\" {\n  name        = \"resume-visitor-counter-lambda-policy-${var.environment}\"\n  description = \"IAM policy for the visitor counter Lambda function\"\n\n  policy = jsonencode({\n    Version = \"2012-10-17\"\n    Statement = [\n      {\n        Action = [\n          \"dynamodb:GetItem\",\n          \"dynamodb:PutItem\",\n          \"dynamodb:UpdateItem\"\n        ]\n        Effect   = \"Allow\"\n        Resource = aws_dynamodb_table.visitor_counter.arn\n      },\n      {\n        Action = [\n          \"logs:CreateLogGroup\",\n          \"logs:CreateLogStream\",\n          \"logs:PutLogEvents\"\n        ]\n        Effect   = \"Allow\"\n        Resource = \"arn:aws:logs:*:*:*\"\n      },\n      {\n        Action = [\n          \"xray:PutTraceSegments\",\n          \"xray:PutTelemetryRecords\"\n        ]\n        Effect   = \"Allow\"\n        Resource = \"*\"\n      }\n    ]\n  })\n}\n\n# Attach the policy to the IAM role\nresource \"aws_iam_role_policy_attachment\" \"lambda_policy_attachment\" {\n  role       = aws_iam_role.lambda_role.name\n  policy_arn = aws_iam_policy.lambda_policy.arn\n}\n\n# Create a CloudWatch log group for the Lambda function\nresource \"aws_cloudwatch_log_group\" \"lambda_log_group\" {\n  name              = \"/aws/lambda/${aws_lambda_function.visitor_counter.function_name}\"\n  retention_in_days = 30\n\n  tags = {\n    Environment = var.environment\n    Project     = \"Cloud Resume Challenge\"\n  }\n}\n\n# Create a Lambda function for handling OPTIONS requests (CORS)\nresource \"aws_lambda_function\" \"options_handler\" {\n  filename         = data.archive_file.lambda_zip.output_path\n  function_name    = \"resume-visitor-counter-options-${var.environment}\"\n  role             = aws_iam_role.lambda_role.arn\n  handler          = \"visitor_counter.options_handler\"\n  source_code_hash = data.archive_file.lambda_zip.output_base64sha256\n  runtime          = \"python3.9\"\n  timeout          = 10\n  memory_size      = 128\n\n  environment {\n    variables = {\n      ALLOWED_ORIGIN = var.website_domain\n    }\n  }\n\n  tags = {\n    Name        = \"Resume Options Handler Lambda\"\n    Environment = var.environment\n    Project     = \"Cloud Resume Challenge\"\n  }\n}\n</code></pre> <p>I've implemented several security and operational improvements:</p> <ul> <li>Least privilege IAM policies</li> <li>X-Ray tracing for performance monitoring</li> <li>Proper CORS handling with a dedicated OPTIONS handler</li> <li>CloudWatch log group with retention policy</li> <li>Privacy-enhancing IP address hashing</li> </ul>","tags":["lambda","api-gateway","dynamodb","terraform","serverless"]},{"location":"blog/cloud-resume-challenge-with-terraform-building-the-backend-api/#3-api-gateway-for-exposing-the-lambda-function","title":"3. API Gateway for Exposing the Lambda Function \ud83d\udd17","text":"<p>Create a file at <code>modules/backend/api_gateway.tf</code>:</p> <pre><code># Create the API Gateway REST API\nresource \"aws_api_gateway_rest_api\" \"visitor_counter\" {\n  name        = \"resume-visitor-counter-${var.environment}\"\n  description = \"API for the resume visitor counter\"\n\n  endpoint_configuration {\n    types = [\"REGIONAL\"]\n  }\n\n  tags = {\n    Name        = \"Resume Visitor Counter API\"\n    Environment = var.environment\n    Project     = \"Cloud Resume Challenge\"\n  }\n}\n\n# Create a resource for the API\nresource \"aws_api_gateway_resource\" \"visitor_counter\" {\n  rest_api_id = aws_api_gateway_rest_api.visitor_counter.id\n  parent_id   = aws_api_gateway_rest_api.visitor_counter.root_resource_id\n  path_part   = \"count\"\n}\n\n# Create a GET method for the API\nresource \"aws_api_gateway_method\" \"get\" {\n  rest_api_id   = aws_api_gateway_rest_api.visitor_counter.id\n  resource_id   = aws_api_gateway_resource.visitor_counter.id\n  http_method   = \"GET\"\n  authorization_type = \"NONE\"\n\n  # Add API key requirement if needed\n  # api_key_required = true\n}\n\n# Create an OPTIONS method for the API (for CORS)\nresource \"aws_api_gateway_method\" \"options\" {\n  rest_api_id   = aws_api_gateway_rest_api.visitor_counter.id\n  resource_id   = aws_api_gateway_resource.visitor_counter.id\n  http_method   = \"OPTIONS\"\n  authorization_type = \"NONE\"\n}\n\n# Set up the GET method integration with Lambda\nresource \"aws_api_gateway_integration\" \"lambda_get\" {\n  rest_api_id = aws_api_gateway_rest_api.visitor_counter.id\n  resource_id = aws_api_gateway_resource.visitor_counter.id\n  http_method = aws_api_gateway_method.get.http_method\n\n  integration_http_method = \"POST\"\n  type                    = \"AWS_PROXY\"\n  uri                     = aws_lambda_function.visitor_counter.invoke_arn\n}\n\n# Set up the OPTIONS method integration with Lambda\nresource \"aws_api_gateway_integration\" \"lambda_options\" {\n  rest_api_id = aws_api_gateway_rest_api.visitor_counter.id\n  resource_id = aws_api_gateway_resource.visitor_counter.id\n  http_method = aws_api_gateway_method.options.http_method\n\n  integration_http_method = \"POST\"\n  type                    = \"AWS_PROXY\"\n  uri                     = aws_lambda_function.options_handler.invoke_arn\n}\n\n# Create a deployment for the API\nresource \"aws_api_gateway_deployment\" \"visitor_counter\" {\n  depends_on = [\n    aws_api_gateway_integration.lambda_get,\n    aws_api_gateway_integration.lambda_options\n  ]\n\n  rest_api_id = aws_api_gateway_rest_api.visitor_counter.id\n  stage_name  = var.environment\n\n  lifecycle {\n    create_before_destroy = true\n  }\n}\n\n# Add permission for API Gateway to invoke the Lambda function\nresource \"aws_lambda_permission\" \"api_gateway_lambda\" {\n  statement_id  = \"AllowAPIGatewayInvoke\"\n  action        = \"lambda:InvokeFunction\"\n  function_name = aws_lambda_function.visitor_counter.function_name\n  principal     = \"apigateway.amazonaws.com\"\n\n  # The /* part allows invocation from any stage, method and resource path\n  # within API Gateway\n  source_arn = \"${aws_api_gateway_rest_api.visitor_counter.execution_arn}/*/${aws_api_gateway_method.get.http_method}${aws_api_gateway_resource.visitor_counter.path}\"\n}\n\n# Add permission for API Gateway to invoke the OPTIONS Lambda function\nresource \"aws_lambda_permission\" \"api_gateway_options_lambda\" {\n  statement_id  = \"AllowAPIGatewayInvokeOptions\"\n  action        = \"lambda:InvokeFunction\"\n  function_name = aws_lambda_function.options_handler.function_name\n  principal     = \"apigateway.amazonaws.com\"\n\n  source_arn = \"${aws_api_gateway_rest_api.visitor_counter.execution_arn}/*/${aws_api_gateway_method.options.http_method}${aws_api_gateway_resource.visitor_counter.path}\"\n}\n\n# Enable CloudWatch logging for API Gateway\nresource \"aws_api_gateway_account\" \"main\" {\n  cloudwatch_role_arn = aws_iam_role.api_gateway_cloudwatch.arn\n}\n\nresource \"aws_iam_role\" \"api_gateway_cloudwatch\" {\n  name = \"api-gateway-cloudwatch-role-${var.environment}\"\n\n  assume_role_policy = jsonencode({\n    Version = \"2012-10-17\"\n    Statement = [\n      {\n        Action = \"sts:AssumeRole\"\n        Effect = \"Allow\"\n        Principal = {\n          Service = \"apigateway.amazonaws.com\"\n        }\n      }\n    ]\n  })\n}\n\nresource \"aws_iam_role_policy_attachment\" \"api_gateway_cloudwatch\" {\n  role       = aws_iam_role.api_gateway_cloudwatch.name\n  policy_arn = \"arn:aws:iam::aws:policy/service-role/AmazonAPIGatewayPushToCloudWatchLogs\"\n}\n\n# Set up method settings for logging and throttling\nresource \"aws_api_gateway_method_settings\" \"settings\" {\n  rest_api_id = aws_api_gateway_rest_api.visitor_counter.id\n  stage_name  = aws_api_gateway_deployment.visitor_counter.stage_name\n  method_path = \"*/*\"\n\n  settings {\n    metrics_enabled        = true\n    logging_level          = \"INFO\"\n    data_trace_enabled     = true\n    throttling_rate_limit  = 100\n    throttling_burst_limit = 50\n  }\n}\n\n# Create a custom domain for the API\nresource \"aws_api_gateway_domain_name\" \"api\" {\n  domain_name              = \"api.${var.domain_name}\"\n  regional_certificate_arn = var.certificate_arn\n\n  endpoint_configuration {\n    types = [\"REGIONAL\"]\n  }\n\n  tags = {\n    Name        = \"Resume API Domain\"\n    Environment = var.environment\n    Project     = \"Cloud Resume Challenge\"\n  }\n}\n\n# Create a base path mapping for the custom domain\nresource \"aws_api_gateway_base_path_mapping\" \"api\" {\n  api_id      = aws_api_gateway_rest_api.visitor_counter.id\n  stage_name  = aws_api_gateway_deployment.visitor_counter.stage_name\n  domain_name = aws_api_gateway_domain_name.api.domain_name\n}\n\n# Create a Route 53 record for the API domain\nresource \"aws_route53_record\" \"api\" {\n  name    = aws_api_gateway_domain_name.api.domain_name\n  type    = \"A\"\n  zone_id = var.hosted_zone_id\n\n  alias {\n    name                   = aws_api_gateway_domain_name.api.regional_domain_name\n    zone_id                = aws_api_gateway_domain_name.api.regional_zone_id\n    evaluate_target_health = false\n  }\n}\n</code></pre> <p>The API Gateway configuration includes several enhancements:</p> <ul> <li>CloudWatch logging and metrics</li> <li>Rate limiting and throttling to prevent abuse</li> <li>Custom domain for a professional API endpoint</li> <li>Proper Route 53 DNS configuration</li> </ul>","tags":["lambda","api-gateway","dynamodb","terraform","serverless"]},{"location":"blog/cloud-resume-challenge-with-terraform-building-the-backend-api/#4-variables-and-outputs","title":"4. Variables and Outputs \ud83d\udcdd","text":"<p>Create files at <code>modules/backend/variables.tf</code> and <code>modules/backend/outputs.tf</code>:</p> <p>variables.tf:</p> <pre><code>variable \"environment\" {\n  description = \"Deployment environment (e.g., dev, prod)\"\n  type        = string\n  default     = \"dev\"\n}\n\nvariable \"website_domain\" {\n  description = \"Domain of the resume website (for CORS)\"\n  type        = string\n}\n\nvariable \"domain_name\" {\n  description = \"Base domain name for custom API endpoint\"\n  type        = string\n}\n\nvariable \"hosted_zone_id\" {\n  description = \"Route 53 hosted zone ID\"\n  type        = string\n}\n\nvariable \"certificate_arn\" {\n  description = \"ARN of the ACM certificate for the API domain\"\n  type        = string\n}\n</code></pre> <p>outputs.tf:</p> <pre><code>output \"api_endpoint\" {\n  description = \"Endpoint URL of the API Gateway\"\n  value       = aws_api_gateway_deployment.visitor_counter.invoke_url\n}\n\noutput \"api_custom_domain\" {\n  description = \"Custom domain for the API\"\n  value       = aws_api_gateway_domain_name.api.domain_name\n}\n\noutput \"dynamodb_table_name\" {\n  description = \"Name of the DynamoDB table\"\n  value       = aws_dynamodb_table.visitor_counter.name\n}\n</code></pre>","tags":["lambda","api-gateway","dynamodb","terraform","serverless"]},{"location":"blog/cloud-resume-challenge-with-terraform-building-the-backend-api/#5-source-control-for-backend-code","title":"5. Source Control for Backend Code \ud83d\udcda","text":"<p>An important aspect of the Cloud Resume Challenge is using source control. We'll create a GitHub repository for our backend code. Here's how I organize my repository:</p> <pre><code>resume-backend/\n\u251c\u2500\u2500 .github/\n\u2502   \u2514\u2500\u2500 workflows/\n\u2502       \u2514\u2500\u2500 deploy.yml  # GitHub Actions workflow (we'll create this in next post)\n\u251c\u2500\u2500 lambda/\n\u2502   \u2514\u2500\u2500 visitor_counter.py\n\u251c\u2500\u2500 terraform/\n\u2502   \u251c\u2500\u2500 modules/\n\u2502   \u2502   \u251c\u2500\u2500 backend/\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 api_gateway.tf\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 dynamodb.tf\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 lambda.tf\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 variables.tf\n\u2502   \u2502   \u2502   \u2514\u2500\u2500 outputs.tf\n\u2502   \u251c\u2500\u2500 main.tf\n\u2502   \u251c\u2500\u2500 variables.tf\n\u2502   \u2514\u2500\u2500 outputs.tf\n\u251c\u2500\u2500 tests/\n\u2502   \u2514\u2500\u2500 test_visitor_counter.py  # Python unit tests\n\u2514\u2500\u2500 README.md\n</code></pre>","tags":["lambda","api-gateway","dynamodb","terraform","serverless"]},{"location":"blog/cloud-resume-challenge-with-terraform-building-the-backend-api/#implementing-python-tests","title":"Implementing Python Tests \ud83e\uddea","text":"<p>For step 11 of the Cloud Resume Challenge, we need to include tests for our Python code. Create a file at <code>tests/test_visitor_counter.py</code>:</p> <pre><code>import unittest\nimport json\nimport os\nimport sys\nfrom unittest.mock import patch, MagicMock\n\n# Add lambda directory to the path so we can import the function\nsys.path.append(os.path.join(os.path.dirname(__file__), '..', 'lambda'))\n\nimport visitor_counter\n\nclass TestVisitorCounter(unittest.TestCase):\n    \"\"\"Test cases for the visitor counter Lambda function.\"\"\"\n\n    @patch('visitor_counter.table')\n    def test_lambda_handler_success(self, mock_table):\n        \"\"\"Test successful execution of the lambda_handler function.\"\"\"\n        # Mock the DynamoDB responses\n        mock_put_response = MagicMock()\n        mock_update_response = {\n            'Attributes': {\n                'count': 42\n            }\n        }\n        mock_table.put_item.return_value = mock_put_response\n        mock_table.update_item.return_value = mock_update_response\n\n        # Set required environment variables\n        os.environ['DYNAMODB_TABLE'] = 'test-table'\n        os.environ['ALLOWED_ORIGIN'] = 'https://example.com'\n\n        # Create a test event\n        event = {\n            'httpMethod': 'GET',\n            'path': '/count',\n            'headers': {\n                'User-Agent': 'test-agent'\n            },\n            'requestContext': {\n                'identity': {\n                    'sourceIp': '127.0.0.1'\n                }\n            }\n        }\n\n        # Call the function\n        response = visitor_counter.lambda_handler(event, {})\n\n        # Assert response is correct\n        self.assertEqual(response['statusCode'], 200)\n        self.assertEqual(response['headers']['Content-Type'], 'application/json')\n        self.assertEqual(response['headers']['Access-Control-Allow-Origin'], 'https://example.com')\n\n        # Parse the body and check the count\n        body = json.loads(response['body'])\n        self.assertEqual(body['count'], 42)\n        self.assertEqual(body['message'], 'Visitor count updated successfully')\n\n        # Verify that DynamoDB was called correctly\n        mock_table.put_item.assert_called_once()\n        mock_table.update_item.assert_called_once_with(\n            Key={'visit_id': 'total'},\n            UpdateExpression='ADD #count :incr',\n            ExpressionAttributeNames={'#count': 'count'},\n            ExpressionAttributeValues={':incr': 1},\n            ReturnValues='UPDATED_NEW'\n        )\n\n    @patch('visitor_counter.table')\n    def test_lambda_handler_error(self, mock_table):\n        \"\"\"Test error handling in the lambda_handler function.\"\"\"\n        # Simulate a DynamoDB error\n        mock_table.update_item.side_effect = Exception(\"Test error\")\n\n        # Set required environment variables\n        os.environ['DYNAMODB_TABLE'] = 'test-table'\n        os.environ['ALLOWED_ORIGIN'] = 'https://example.com'\n\n        # Create a test event\n        event = {\n            'httpMethod': 'GET',\n            'path': '/count',\n            'headers': {\n                'User-Agent': 'test-agent'\n            },\n            'requestContext': {\n                'identity': {\n                    'sourceIp': '127.0.0.1'\n                }\n            }\n        }\n\n        # Call the function\n        response = visitor_counter.lambda_handler(event, {})\n\n        # Assert response indicates an error\n        self.assertEqual(response['statusCode'], 500)\n        self.assertEqual(response['headers']['Content-Type'], 'application/json')\n\n        # Parse the body and check the error message\n        body = json.loads(response['body'])\n        self.assertIn('error', body)\n        self.assertIn('message', body)\n\n    def test_options_handler(self):\n        \"\"\"Test the OPTIONS handler for CORS support.\"\"\"\n        # Set required environment variables\n        os.environ['ALLOWED_ORIGIN'] = 'https://example.com'\n\n        # Create a test event\n        event = {\n            'httpMethod': 'OPTIONS',\n            'path': '/count',\n            'headers': {\n                'Origin': 'https://example.com'\n            }\n        }\n\n        # Call the function\n        response = visitor_counter.options_handler(event, {})\n\n        # Assert response is correct for OPTIONS\n        self.assertEqual(response['statusCode'], 200)\n        self.assertEqual(response['headers']['Access-Control-Allow-Origin'], 'https://example.com')\n        self.assertEqual(response['headers']['Access-Control-Allow-Methods'], 'GET, OPTIONS')\n        self.assertEqual(response['headers']['Access-Control-Allow-Headers'], 'Content-Type')\n\nif __name__ == '__main__':\n    unittest.main()\n</code></pre> <p>This test suite covers:</p> <ul> <li>Successful API calls</li> <li>Error handling</li> <li>CORS OPTIONS request handling</li> </ul> <p>To run these tests, you would use the following command:</p> <pre><code>python -m unittest tests/test_visitor_counter.py\n</code></pre>","tags":["lambda","api-gateway","dynamodb","terraform","serverless"]},{"location":"blog/cloud-resume-challenge-with-terraform-building-the-backend-api/#testing-the-api-manually","title":"Testing the API Manually \ud83e\uddea","text":"<p>Once you've deployed the API, you can test it manually using tools like cURL or Postman. Here's how to test with cURL:</p> <pre><code># Get the current visitor count\ncurl -X GET https://api.yourdomain.com/count\n\n# Test CORS pre-flight request\ncurl -X OPTIONS https://api.yourdomain.com/count \\\n  -H \"Origin: https://yourdomain.com\" \\\n  -H \"Access-Control-Request-Method: GET\" \\\n  -H \"Access-Control-Request-Headers: Content-Type\"\n</code></pre> <p>For Postman:</p> <ol> <li>Create a new GET request to your API endpoint (<code>https://api.yourdomain.com/count</code>)</li> <li>Send the request and verify you get a 200 response with a JSON body</li> <li>Create a new OPTIONS request to test CORS</li> <li>Add headers: <code>Origin: https://yourdomain.com</code>, <code>Access-Control-Request-Method: GET</code></li> <li>Send the request and verify you get a 200 response with the correct CORS headers</li> </ol>","tags":["lambda","api-gateway","dynamodb","terraform","serverless"]},{"location":"blog/cloud-resume-challenge-with-terraform-building-the-backend-api/#setting-up-cloudwatch-monitoring-and-alarms","title":"Setting Up CloudWatch Monitoring and Alarms \u26a0\ufe0f","text":"<p>Adding monitoring and alerting is a critical part of any production-grade API. Let's add CloudWatch alarms to notify us if something goes wrong:</p> <pre><code># Add to modules/backend/monitoring.tf\n\n# Alarm for Lambda errors\nresource \"aws_cloudwatch_metric_alarm\" \"lambda_errors\" {\n  alarm_name          = \"lambda-visitor-counter-errors-${var.environment}\"\n  comparison_operator = \"GreaterThanThreshold\"\n  evaluation_periods  = 1\n  metric_name         = \"Errors\"\n  namespace           = \"AWS/Lambda\"\n  period              = 60\n  statistic           = \"Sum\"\n  threshold           = 0\n  alarm_description   = \"This alarm monitors for errors in the visitor counter Lambda function\"\n\n  dimensions = {\n    FunctionName = aws_lambda_function.visitor_counter.function_name\n  }\n\n  # Add SNS topic ARN if you want notifications\n  # alarm_actions     = [aws_sns_topic.alerts.arn]\n  # ok_actions        = [aws_sns_topic.alerts.arn]\n}\n\n# Alarm for API Gateway 5XX errors\nresource \"aws_cloudwatch_metric_alarm\" \"api_5xx_errors\" {\n  alarm_name          = \"api-visitor-counter-5xx-errors-${var.environment}\"\n  comparison_operator = \"GreaterThanThreshold\"\n  evaluation_periods  = 1\n  metric_name         = \"5XXError\"\n  namespace           = \"AWS/ApiGateway\"\n  period              = 60\n  statistic           = \"Sum\"\n  threshold           = 0\n  alarm_description   = \"This alarm monitors for 5XX errors in the visitor counter API\"\n\n  dimensions = {\n    ApiName = aws_api_gateway_rest_api.visitor_counter.name\n    Stage   = aws_api_gateway_deployment.visitor_counter.stage_name\n  }\n\n  # Add SNS topic ARN if you want notifications\n  # alarm_actions     = [aws_sns_topic.alerts.arn]\n  # ok_actions        = [aws_sns_topic.alerts.arn]\n}\n\n# Dashboard for monitoring the API\nresource \"aws_cloudwatch_dashboard\" \"api_dashboard\" {\n  dashboard_name = \"visitor-counter-dashboard-${var.environment}\"\n\n  dashboard_body = jsonencode({\n    widgets = [\n      {\n        type   = \"metric\"\n        x      = 0\n        y      = 0\n        width  = 12\n        height = 6\n        properties = {\n          metrics = [\n            [\"AWS/ApiGateway\", \"Count\", \"ApiName\", aws_api_gateway_rest_api.visitor_counter.name, \"Stage\", aws_api_gateway_deployment.visitor_counter.stage_name]\n          ]\n          period = 300\n          stat   = \"Sum\"\n          region = \"us-east-1\"\n          title  = \"API Requests\"\n        }\n      },\n      {\n        type   = \"metric\"\n        x      = 12\n        y      = 0\n        width  = 12\n        height = 6\n        properties = {\n          metrics = [\n            [\"AWS/ApiGateway\", \"4XXError\", \"ApiName\", aws_api_gateway_rest_api.visitor_counter.name, \"Stage\", aws_api_gateway_deployment.visitor_counter.stage_name],\n            [\"AWS/ApiGateway\", \"5XXError\", \"ApiName\", aws_api_gateway_rest_api.visitor_counter.name, \"Stage\", aws_api_gateway_deployment.visitor_counter.stage_name]\n          ]\n          period = 300\n          stat   = \"Sum\"\n          region = \"us-east-1\"\n          title  = \"API Errors\"\n        }\n      },\n      {\n        type   = \"metric\"\n        x      = 0\n        y      = 6\n        width  = 12\n        height = 6\n        properties = {\n          metrics = [\n            [\"AWS/Lambda\", \"Invocations\", \"FunctionName\", aws_lambda_function.visitor_counter.function_name],\n            [\"AWS/Lambda\", \"Errors\", \"FunctionName\", aws_lambda_function.visitor_counter.function_name]\n          ]\n          period = 300\n          stat   = \"Sum\"\n          region = \"us-east-1\"\n          title  = \"Lambda Invocations and Errors\"\n        }\n      },\n      {\n        type   = \"metric\"\n        x      = 12\n        y      = 6\n        width  = 12\n        height = 6\n        properties = {\n          metrics = [\n            [\"AWS/Lambda\", \"Duration\", \"FunctionName\", aws_lambda_function.visitor_counter.function_name]\n          ]\n          period = 300\n          stat   = \"Average\"\n          region = \"us-east-1\"\n          title  = \"Lambda Duration\"\n        }\n      }\n    ]\n  })\n}\n</code></pre>","tags":["lambda","api-gateway","dynamodb","terraform","serverless"]},{"location":"blog/cloud-resume-challenge-with-terraform-building-the-backend-api/#debugging-common-api-issues","title":"Debugging Common API Issues \ud83d\udc1b","text":"<p>During my implementation, I encountered several challenges:</p> <ol> <li> <p>CORS Issues: The most common problem was with CORS configuration. Make sure your API Gateway and Lambda function both return the proper CORS headers.</p> </li> <li> <p>IAM Permission Errors: Initially, I gave my Lambda function too many permissions, then too few. The policy shown above represents the minimal set of permissions needed.</p> </li> <li> <p>DynamoDB Initialization: The counter needs to be initialized with a value. I solved this by adding an item to the table during deployment.</p> </li> <li> <p>API Gateway Integration: Make sure your Lambda function and API Gateway are correctly integrated. Check for proper resource paths and method settings.</p> </li> </ol>","tags":["lambda","api-gateway","dynamodb","terraform","serverless"]},{"location":"blog/cloud-resume-challenge-with-terraform-building-the-backend-api/#lessons-learned","title":"Lessons Learned \ud83d\udca1","text":"<ol> <li> <p>DynamoDB Design: My initial design was too simple. Adding more fields like timestamp and user-agent provides valuable analytics data.</p> </li> <li> <p>Error Handling: Robust error handling is critical for serverless applications. Without proper logging, debugging becomes nearly impossible.</p> </li> <li> <p>Testing Strategy: Writing tests before implementing the Lambda function (test-driven development) helped me think through edge cases and error scenarios.</p> </li> <li> <p>Security Considerations: Privacy is important. Hashing IP addresses and implementing proper IAM policies ensures we protect user data.</p> </li> </ol>","tags":["lambda","api-gateway","dynamodb","terraform","serverless"]},{"location":"blog/cloud-resume-challenge-with-terraform-building-the-backend-api/#api-security-considerations","title":"API Security Considerations \ud83d\udd12","text":"<p>Security was a primary concern when building this API. Here are the key security measures I implemented:</p> <ol> <li> <p>Least Privilege IAM Policies: The Lambda function has only the minimal permissions needed.</p> </li> <li> <p>Input Validation: The Lambda function validates and sanitizes all input.</p> </li> <li> <p>Rate Limiting: API Gateway is configured with throttling to prevent abuse.</p> </li> <li> <p>HTTPS Only: All API endpoints use HTTPS with modern TLS settings.</p> </li> <li> <p>CORS Configuration: Only the resume website domain is allowed to make cross-origin requests.</p> </li> <li> <p>Privacy Protection: IP addresses are hashed to protect visitor privacy.</p> </li> </ol> <p>These measures help protect against common API vulnerabilities like injection attacks, denial of service, and data exposure.</p>","tags":["lambda","api-gateway","dynamodb","terraform","serverless"]},{"location":"blog/cloud-resume-challenge-with-terraform-building-the-backend-api/#enhancements-and-mods","title":"Enhancements and Mods \ud83d\ude80","text":"<p>Here are some ways to extend this part of the challenge:</p>","tags":["lambda","api-gateway","dynamodb","terraform","serverless"]},{"location":"blog/cloud-resume-challenge-with-terraform-building-the-backend-api/#developer-mod-schemas-and-dreamers","title":"Developer Mod: Schemas and Dreamers","text":"<p>Instead of using DynamoDB, consider implementing a relational database approach:</p> <pre><code>resource \"aws_db_subnet_group\" \"database\" {\n  name       = \"resume-database-subnet-group\"\n  subnet_ids = var.private_subnet_ids\n}\n\nresource \"aws_security_group\" \"database\" {\n  name        = \"resume-database-sg\"\n  description = \"Security group for the resume database\"\n  vpc_id      = var.vpc_id\n\n  ingress {\n    from_port       = 5432\n    to_port         = 5432\n    protocol        = \"tcp\"\n    security_groups = [aws_security_group.lambda.id]\n  }\n}\n\nresource \"aws_db_instance\" \"postgresql\" {\n  allocated_storage      = 20\n  storage_type           = \"gp2\"\n  engine                 = \"postgres\"\n  engine_version         = \"13.4\"\n  instance_class         = \"db.t3.micro\"\n  db_name                = \"resumedb\"\n  username               = \"postgres\"\n  password               = var.db_password\n  parameter_group_name   = \"default.postgres13\"\n  db_subnet_group_name   = aws_db_subnet_group.database.name\n  vpc_security_group_ids = [aws_security_group.database.id]\n  skip_final_snapshot    = true\n  multi_az               = false\n\n  tags = {\n    Name        = \"Resume Database\"\n    Environment = var.environment\n  }\n}\n</code></pre> <p>This approach introduces interesting networking challenges and requires modifications to your Lambda function to connect to PostgreSQL.</p>","tags":["lambda","api-gateway","dynamodb","terraform","serverless"]},{"location":"blog/cloud-resume-challenge-with-terraform-building-the-backend-api/#devops-mod-monitor-lizard","title":"DevOps Mod: Monitor Lizard","text":"<p>Enhance monitoring with X-Ray traces and custom CloudWatch metrics:</p> <pre><code># Add to Lambda function configuration\ntracing_config {\n  mode = \"Active\"\n}\n\n# Add X-Ray policy\nresource \"aws_iam_policy\" \"lambda_xray\" {\n  name        = \"lambda-xray-policy-${var.environment}\"\n  description = \"IAM policy for X-Ray tracing\"\n\n  policy = jsonencode({\n    Version = \"2012-10-17\"\n    Statement = [\n      {\n        Action = [\n          \"xray:PutTraceSegments\",\n          \"xray:PutTelemetryRecords\"\n        ]\n        Effect   = \"Allow\"\n        Resource = \"*\"\n      }\n    ]\n  })\n}\n\nresource \"aws_iam_role_policy_attachment\" \"lambda_xray\" {\n  role       = aws_iam_role.lambda_role.name\n  policy_arn = aws_iam_policy.lambda_xray.arn\n}\n</code></pre> <p>Then modify your Lambda function to emit custom metrics:</p> <pre><code>import boto3\nfrom aws_xray_sdk.core import xray_recorder\nfrom aws_xray_sdk.core import patch_all\n\n# Patch all supported libraries for X-Ray\npatch_all()\n\ncloudwatch = boto3.client('cloudwatch')\n\n# Inside lambda_handler\ncloudwatch.put_metric_data(\n    Namespace='ResumeMetrics',\n    MetricData=[\n        {\n            'MetricName': 'VisitorCount',\n            'Value': count,\n            'Unit': 'Count'\n        }\n    ]\n)\n</code></pre>","tags":["lambda","api-gateway","dynamodb","terraform","serverless"]},{"location":"blog/cloud-resume-challenge-with-terraform-building-the-backend-api/#security-mod-check-your-privilege","title":"Security Mod: Check Your Privilege","text":"<p>Implement AWS WAF to protect your API from common web attacks:</p> <pre><code>resource \"aws_wafv2_web_acl\" \"api\" {\n  name        = \"api-waf-${var.environment}\"\n  description = \"WAF for the resume API\"\n  scope       = \"REGIONAL\"\n\n  default_action {\n    allow {}\n  }\n\n  rule {\n    name     = \"AWSManagedRulesCommonRuleSet\"\n    priority = 0\n\n    override_action {\n      none {}\n    }\n\n    statement {\n      managed_rule_group_statement {\n        name        = \"AWSManagedRulesCommonRuleSet\"\n        vendor_name = \"AWS\"\n      }\n    }\n\n    visibility_config {\n      cloudwatch_metrics_enabled = true\n      metric_name                = \"AWSManagedRulesCommonRuleSetMetric\"\n      sampled_requests_enabled   = true\n    }\n  }\n\n  rule {\n    name     = \"RateLimit\"\n    priority = 1\n\n    action {\n      block {}\n    }\n\n    statement {\n      rate_based_statement {\n        limit              = 100\n        aggregate_key_type = \"IP\"\n      }\n    }\n\n    visibility_config {\n      cloudwatch_metrics_enabled = true\n      metric_name                = \"RateLimitMetric\"\n      sampled_requests_enabled   = true\n    }\n  }\n\n  visibility_config {\n    cloudwatch_metrics_enabled = true\n    metric_name                = \"APIWebACLMetric\"\n    sampled_requests_enabled   = true\n  }\n}\n\nresource \"aws_wafv2_web_acl_association\" \"api\" {\n  resource_arn = aws_api_gateway_stage.visitor_counter.arn\n  web_acl_arn  = aws_wafv2_web_acl.api.arn\n}\n</code></pre>","tags":["lambda","api-gateway","dynamodb","terraform","serverless"]},{"location":"blog/cloud-resume-challenge-with-terraform-building-the-backend-api/#next-steps","title":"Next Steps \u23ed\ufe0f","text":"<p>With our backend API completed, we're ready to connect it to our frontend in the next post. We'll integrate the JavaScript visitor counter with our API and then automate the deployment process using GitHub Actions.</p> <p>Stay tuned to see how we bring the full stack together!</p> <p>Up Next: [Cloud Resume Challenge with Terraform: Automating Deployments with GitHub Actions] \ud83d\udd17</p> <p>Share on  Share on </p>","tags":["lambda","api-gateway","dynamodb","terraform","serverless"]},{"location":"blog/cloud-resume-challenge-with-terraform-automating-deployments-with-github-actions/","title":"Cloud Resume Challenge with Terraform: Automating Deployments with GitHub Actions \u26a1","text":"<p>In our previous posts, we built the frontend and backend components of our cloud resume project. Now it's time to take our implementation to the next level by implementing continuous integration and deployment (CI/CD) with GitHub Actions.</p>","tags":["github-actions","terraform","automation","cloud","devops"]},{"location":"blog/cloud-resume-challenge-with-terraform-automating-deployments-with-github-actions/#why-cicd-is-critical-for-cloud-engineers","title":"Why CI/CD Is Critical for Cloud Engineers \ud83d\udee0\ufe0f","text":"<p>When I first started this challenge, I manually ran <code>terraform apply</code> every time I made a change. This quickly became tedious and error-prone. As a cloud engineer, I wanted to demonstrate a professional approach to infrastructure management by implementing proper CI/CD pipelines.</p> <p>Automating deployments offers several key benefits:</p> <ul> <li>Consistency: Every deployment follows the same process</li> <li>Efficiency: No more manual steps or waiting around</li> <li>Safety: Automated tests catch issues before they reach production</li> <li>Auditability: Each change is tracked with a commit and workflow run</li> </ul> <p>This approach mirrors how professional cloud teams work and is a crucial skill for any cloud engineer.</p>","tags":["github-actions","terraform","automation","cloud","devops"]},{"location":"blog/cloud-resume-challenge-with-terraform-automating-deployments-with-github-actions/#cicd-architecture-overview","title":"CI/CD Architecture Overview \ud83c\udfd7\ufe0f","text":"<p>Here's a visual representation of our CI/CD pipelines:</p> <pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510          \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510          \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502             \u2502          \u2502                 \u2502          \u2502             \u2502\n\u2502  Developer  \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u25ba\u2502  GitHub Actions \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u25ba\u2502  AWS Cloud  \u2502\n\u2502  Workstation\u2502          \u2502                 \u2502          \u2502             \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518          \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518          \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n       \u2502                          \u2502                          \u25b2\n       \u2502                          \u2502                          \u2502\n       \u25bc                          \u25bc                          \u2502\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510          \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510                 \u2502\n\u2502             \u2502          \u2502                 \u2502                 \u2502\n\u2502   GitHub    \u2502          \u2502  Terraform      \u2502                 \u2502\n\u2502 Repositories\u2502          \u2502  Plan &amp; Apply   \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n\u2502             \u2502          \u2502                 \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518          \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre> <p>We'll set up separate workflows for:</p> <ol> <li>Frontend deployment: Updates the S3 website content and invalidates CloudFront</li> <li>Backend deployment: Runs Terraform to update our API infrastructure</li> <li>Smoke tests: Verifies that both components are working correctly after deployment</li> </ol>","tags":["github-actions","terraform","automation","cloud","devops"]},{"location":"blog/cloud-resume-challenge-with-terraform-automating-deployments-with-github-actions/#setting-up-github-repositories","title":"Setting Up GitHub Repositories \ud83d\udcc1","text":"<p>For this challenge, I've created two repositories:</p> <ul> <li><code>cloud-resume-frontend</code>: Contains HTML, CSS, JavaScript, and frontend deployment workflows</li> <li><code>cloud-resume-backend</code>: Contains Terraform configuration, Lambda code, and backend deployment workflows</li> </ul>","tags":["github-actions","terraform","automation","cloud","devops"]},{"location":"blog/cloud-resume-challenge-with-terraform-automating-deployments-with-github-actions/#repository-structure","title":"Repository Structure","text":"<p>Here's how I've organized my repositories:</p> <p>Frontend Repository:</p> <pre><code>cloud-resume-frontend/\n\u251c\u2500\u2500 .github/\n\u2502   \u2514\u2500\u2500 workflows/\n\u2502       \u2514\u2500\u2500 deploy.yml\n\u251c\u2500\u2500 website/\n\u2502   \u251c\u2500\u2500 index.html\n\u2502   \u251c\u2500\u2500 styles.css\n\u2502   \u251c\u2500\u2500 counter.js\n\u2502   \u2514\u2500\u2500 error.html\n\u251c\u2500\u2500 tests/\n\u2502   \u2514\u2500\u2500 cypress/\n\u2502       \u2514\u2500\u2500 integration/\n\u2502           \u2514\u2500\u2500 counter.spec.js\n\u2514\u2500\u2500 README.md\n</code></pre> <p>Backend Repository:</p> <pre><code>cloud-resume-backend/\n\u251c\u2500\u2500 .github/\n\u2502   \u2514\u2500\u2500 workflows/\n\u2502       \u2514\u2500\u2500 deploy.yml\n\u251c\u2500\u2500 lambda/\n\u2502   \u2514\u2500\u2500 visitor_counter.py\n\u251c\u2500\u2500 terraform/\n\u2502   \u251c\u2500\u2500 modules/\n\u2502   \u2502   \u251c\u2500\u2500 backend/\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 api_gateway.tf\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 dynamodb.tf\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 lambda.tf\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 variables.tf\n\u2502   \u2502   \u2502   \u2514\u2500\u2500 outputs.tf\n\u2502   \u251c\u2500\u2500 environments/\n\u2502   \u2502   \u251c\u2500\u2500 dev/\n\u2502   \u2502   \u2502   \u2514\u2500\u2500 main.tf\n\u2502   \u2502   \u2514\u2500\u2500 prod/\n\u2502   \u2502       \u2514\u2500\u2500 main.tf\n\u2502   \u251c\u2500\u2500 main.tf\n\u2502   \u251c\u2500\u2500 variables.tf\n\u2502   \u2514\u2500\u2500 outputs.tf\n\u251c\u2500\u2500 tests/\n\u2502   \u2514\u2500\u2500 test_visitor_counter.py\n\u2514\u2500\u2500 README.md\n</code></pre>","tags":["github-actions","terraform","automation","cloud","devops"]},{"location":"blog/cloud-resume-challenge-with-terraform-automating-deployments-with-github-actions/#securing-aws-authentication-in-github-actions","title":"Securing AWS Authentication in GitHub Actions \ud83d\udd12","text":"<p>Before setting up our workflows, we need to address a critical security concern: how to securely authenticate GitHub Actions with AWS.</p> <p>In the past, many tutorials recommended storing AWS access keys as GitHub Secrets. This approach works but has significant security drawbacks:</p> <ul> <li>Long-lived credentials are a security risk</li> <li>Credential rotation is manual and error-prone</li> <li>Access is typically overly permissive</li> </ul> <p>Instead, I'll implement a more secure approach using OpenID Connect (OIDC) for keyless authentication between GitHub Actions and AWS.</p>","tags":["github-actions","terraform","automation","cloud","devops"]},{"location":"blog/cloud-resume-challenge-with-terraform-automating-deployments-with-github-actions/#setting-up-oidc-authentication","title":"Setting Up OIDC Authentication","text":"<p>First, create an IAM OIDC provider for GitHub in your AWS account:</p> <pre><code># oidc-provider.tf\nresource \"aws_iam_openid_connect_provider\" \"github\" {\n  url             = \"https://token.actions.githubusercontent.com\"\n  client_id_list  = [\"sts.amazonaws.com\"]\n  thumbprint_list = [\"6938fd4d98bab03faadb97b34396831e3780aea1\"]\n}\n</code></pre> <p>Then, create an IAM role that GitHub Actions can assume:</p> <pre><code># oidc-role.tf\nresource \"aws_iam_role\" \"github_actions\" {\n  name = \"github-actions-role\"\n\n  assume_role_policy = jsonencode({\n    Version = \"2012-10-17\"\n    Statement = [\n      {\n        Action = \"sts:AssumeRoleWithWebIdentity\"\n        Effect = \"Allow\"\n        Principal = {\n          Federated = aws_iam_openid_connect_provider.github.arn\n        }\n        Condition = {\n          StringEquals = {\n            \"token.actions.githubusercontent.com:aud\" = \"sts.amazonaws.com\"\n          }\n          StringLike = {\n            \"token.actions.githubusercontent.com:sub\" = \"repo:${var.github_org}/${var.github_repo}:*\"\n          }\n        }\n      }\n    ]\n  })\n}\n\n# Attach policies to the role\nresource \"aws_iam_role_policy_attachment\" \"terraform_permissions\" {\n  role       = aws_iam_role.github_actions.name\n  policy_arn = aws_iam_policy.terraform_permissions.arn\n}\n\nresource \"aws_iam_policy\" \"terraform_permissions\" {\n  name        = \"terraform-deployment-policy\"\n  description = \"Policy for Terraform deployments via GitHub Actions\"\n\n  policy = jsonencode({\n    Version = \"2012-10-17\"\n    Statement = [\n      {\n        Action = [\n          \"s3:*\",\n          \"cloudfront:*\",\n          \"route53:*\",\n          \"acm:*\",\n          \"lambda:*\",\n          \"apigateway:*\",\n          \"dynamodb:*\",\n          \"logs:*\",\n          \"iam:GetRole\",\n          \"iam:PassRole\",\n          \"iam:CreateRole\",\n          \"iam:DeleteRole\",\n          \"iam:PutRolePolicy\",\n          \"iam:DeleteRolePolicy\",\n          \"iam:AttachRolePolicy\",\n          \"iam:DetachRolePolicy\"\n        ]\n        Effect   = \"Allow\"\n        Resource = \"*\"\n      }\n    ]\n  })\n}\n</code></pre> <p>For a production environment, I would use more fine-grained permissions, but this policy works for our demonstration.</p>","tags":["github-actions","terraform","automation","cloud","devops"]},{"location":"blog/cloud-resume-challenge-with-terraform-automating-deployments-with-github-actions/#implementing-frontend-cicd-workflow","title":"Implementing Frontend CI/CD Workflow \ud83d\udd04","text":"<p>Let's create a GitHub Actions workflow for our frontend repository. Create a file at <code>.github/workflows/deploy.yml</code>:</p> <pre><code>name: Deploy Frontend\n\non:\n  push:\n    branches:\n      - main\n    paths:\n      - 'website/**'\n      - '.github/workflows/deploy.yml'\n\n  workflow_dispatch:\n\npermissions:\n  id-token: write\n  contents: read\n\njobs:\n  deploy:\n    name: 'Deploy to S3 and Invalidate CloudFront'\n    runs-on: ubuntu-latest\n\n    steps:\n      - name: Checkout Repository\n        uses: actions/checkout@v3\n\n      - name: Configure AWS Credentials\n        uses: aws-actions/configure-aws-credentials@v2\n        with:\n          role-to-assume: arn:aws:iam::${{ secrets.AWS_ACCOUNT_ID }}:role/github-actions-role\n          aws-region: us-east-1\n\n      - name: Deploy to S3\n        run: |\n          aws s3 sync website/ s3://${{ secrets.S3_BUCKET_NAME }} --delete\n\n      - name: Invalidate CloudFront Cache\n        run: |\n          aws cloudfront create-invalidation --distribution-id ${{ secrets.CLOUDFRONT_DISTRIBUTION_ID }} --paths \"/*\"\n\n  test:\n    name: 'Run Smoke Tests'\n    needs: deploy\n    runs-on: ubuntu-latest\n\n    steps:\n      - name: Checkout Repository\n        uses: actions/checkout@v3\n\n      - name: Install Cypress\n        uses: cypress-io/github-action@v5\n        with:\n          install-command: npm install\n\n      - name: Run Cypress Tests\n        uses: cypress-io/github-action@v5\n        with:\n          command: npx cypress run\n          config: baseUrl=${{ secrets.WEBSITE_URL }}\n</code></pre> <p>This workflow:</p> <ol> <li>Authenticates using OIDC</li> <li>Syncs website files to the S3 bucket</li> <li>Invalidates the CloudFront cache</li> <li>Runs Cypress tests to verify the site is working</li> </ol>","tags":["github-actions","terraform","automation","cloud","devops"]},{"location":"blog/cloud-resume-challenge-with-terraform-automating-deployments-with-github-actions/#creating-a-cypress-test-for-the-frontend","title":"Creating a Cypress Test for the Frontend","text":"<p>Let's create a simple Cypress test to verify that our visitor counter is working. First, create a <code>package.json</code> file in the root of your frontend repository:</p> <pre><code>{\n  \"name\": \"cloud-resume-frontend\",\n  \"version\": \"1.0.0\",\n  \"description\": \"Frontend for Cloud Resume Challenge\",\n  \"scripts\": {\n    \"test\": \"cypress open\",\n    \"test:ci\": \"cypress run\"\n  },\n  \"devDependencies\": {\n    \"cypress\": \"^12.0.0\"\n  }\n}\n</code></pre> <p>Then create a Cypress test at <code>tests/cypress/integration/counter.spec.js</code>:</p> <pre><code>describe('Resume Website Tests', () =&gt; {\n  beforeEach(() =&gt; {\n    // Visit the home page before each test\n    cy.visit('/');\n  });\n\n  it('should load the resume page', () =&gt; {\n    // Check that we have a title\n    cy.get('h1').should('be.visible');\n\n    // Check that key sections exist\n    cy.contains('Experience').should('be.visible');\n    cy.contains('Education').should('be.visible');\n    cy.contains('Skills').should('be.visible');\n  });\n\n  it('should load and display the visitor counter', () =&gt; {\n    // Check that the counter element exists\n    cy.get('#count').should('exist');\n\n    // Wait for the counter to update (should not remain at 0)\n    cy.get('#count', { timeout: 10000 })\n      .should('not.contain', '0')\n      .should('not.contain', 'Loading');\n\n    // Verify the counter shows a number\n    cy.get('#count').invoke('text').then(parseFloat)\n      .should('be.gt', 0);\n  });\n});\n</code></pre>","tags":["github-actions","terraform","automation","cloud","devops"]},{"location":"blog/cloud-resume-challenge-with-terraform-automating-deployments-with-github-actions/#implementing-backend-cicd-workflow","title":"Implementing Backend CI/CD Workflow \ud83d\udd04","text":"<p>Now, let's create a GitHub Actions workflow for our backend repository. Create a file at <code>.github/workflows/deploy.yml</code>:</p> <pre><code>name: Deploy Backend\n\non:\n  push:\n    branches:\n      - main\n    paths:\n      - 'lambda/**'\n      - 'terraform/**'\n      - '.github/workflows/deploy.yml'\n\n  pull_request:\n    branches:\n      - main\n\n  workflow_dispatch:\n\npermissions:\n  id-token: write\n  contents: read\n  pull-requests: write\n\njobs:\n  test:\n    name: 'Run Python Tests'\n    runs-on: ubuntu-latest\n\n    steps:\n      - name: Checkout Repository\n        uses: actions/checkout@v3\n\n      - name: Set up Python\n        uses: actions/setup-python@v4\n        with:\n          python-version: '3.9'\n\n      - name: Install Dependencies\n        run: |\n          python -m pip install --upgrade pip\n          pip install pytest boto3 moto\n\n      - name: Run Tests\n        run: |\n          python -m pytest tests/\n\n  validate:\n    name: 'Validate Terraform'\n    runs-on: ubuntu-latest\n    needs: test\n\n    steps:\n      - name: Checkout Repository\n        uses: actions/checkout@v3\n\n      - name: Setup Terraform\n        uses: hashicorp/setup-terraform@v2\n        with:\n          terraform_version: 1.2.0\n\n      - name: Terraform Format\n        working-directory: ./terraform\n        run: terraform fmt -check\n\n      - name: Terraform Init\n        working-directory: ./terraform\n        run: terraform init -backend=false\n\n      - name: Terraform Validate\n        working-directory: ./terraform\n        run: terraform validate\n\n  plan:\n    name: 'Terraform Plan'\n    runs-on: ubuntu-latest\n    needs: validate\n    if: github.event_name == 'pull_request' || github.event_name == 'push' || github.event_name == 'workflow_dispatch'\n    environment: dev\n\n    steps:\n      - name: Checkout Repository\n        uses: actions/checkout@v3\n\n      - name: Configure AWS Credentials\n        uses: aws-actions/configure-aws-credentials@v2\n        with:\n          role-to-assume: arn:aws:iam::${{ secrets.AWS_ACCOUNT_ID }}:role/github-actions-role\n          aws-region: us-east-1\n\n      - name: Setup Terraform\n        uses: hashicorp/setup-terraform@v2\n        with:\n          terraform_version: 1.2.0\n\n      - name: Terraform Init\n        working-directory: ./terraform\n        run: terraform init -backend-config=\"bucket=${{ secrets.TF_STATE_BUCKET }}\" -backend-config=\"key=${{ secrets.TF_STATE_KEY }}\" -backend-config=\"region=us-east-1\"\n\n      - name: Terraform Plan\n        working-directory: ./terraform\n        run: terraform plan -var=\"environment=dev\" -var=\"domain_name=${{ secrets.DOMAIN_NAME }}\" -out=tfplan\n\n      - name: Comment Plan on PR\n        uses: actions/github-script@v6\n        if: github.event_name == 'pull_request'\n        with:\n          github-token: ${{ secrets.GITHUB_TOKEN }}\n          script: |\n            const output = `#### Terraform Format and Style \ud83d\udd8c\\`${{ steps.fmt.outcome }}\\`\n            #### Terraform Plan \ud83d\udcd6\\`${{ steps.plan.outcome }}\\`\n\n            &lt;details&gt;&lt;summary&gt;Show Plan&lt;/summary&gt;\n\n            \\`\\`\\`terraform\n            ${{ steps.plan.outputs.stdout }}\n            \\`\\`\\`\n\n            &lt;/details&gt;`;\n            github.rest.issues.createComment({\n              issue_number: context.issue.number,\n              owner: context.repo.owner,\n              repo: context.repo.repo,\n              body: output\n            })\n\n      - name: Upload Plan Artifact\n        uses: actions/upload-artifact@v3\n        with:\n          name: tfplan\n          path: ./terraform/tfplan\n\n  apply:\n    name: 'Terraform Apply'\n    runs-on: ubuntu-latest\n    needs: plan\n    if: github.event_name == 'push' &amp;&amp; github.ref == 'refs/heads/main' || github.event_name == 'workflow_dispatch'\n    environment: dev\n\n    steps:\n      - name: Checkout Repository\n        uses: actions/checkout@v3\n\n      - name: Configure AWS Credentials\n        uses: aws-actions/configure-aws-credentials@v2\n        with:\n          role-to-assume: arn:aws:iam::${{ secrets.AWS_ACCOUNT_ID }}:role/github-actions-role\n          aws-region: us-east-1\n\n      - name: Setup Terraform\n        uses: hashicorp/setup-terraform@v2\n        with:\n          terraform_version: 1.2.0\n\n      - name: Terraform Init\n        working-directory: ./terraform\n        run: terraform init -backend-config=\"bucket=${{ secrets.TF_STATE_BUCKET }}\" -backend-config=\"key=${{ secrets.TF_STATE_KEY }}\" -backend-config=\"region=us-east-1\"\n\n      - name: Download Plan Artifact\n        uses: actions/download-artifact@v3\n        with:\n          name: tfplan\n          path: ./terraform\n\n      - name: Terraform Apply\n        working-directory: ./terraform\n        run: terraform apply -auto-approve tfplan\n\n  test-api:\n    name: 'Test API Deployment'\n    runs-on: ubuntu-latest\n    needs: apply\n    environment: dev\n\n    steps:\n      - name: Checkout Repository\n        uses: actions/checkout@v3\n\n      - name: Configure AWS Credentials\n        uses: aws-actions/configure-aws-credentials@v2\n        with:\n          role-to-assume: arn:aws:iam::${{ secrets.AWS_ACCOUNT_ID }}:role/github-actions-role\n          aws-region: us-east-1\n\n      - name: Fetch API Endpoint\n        run: |\n          API_ENDPOINT=$(aws cloudformation describe-stacks --stack-name resume-backend-dev --query \"Stacks[0].Outputs[?OutputKey=='ApiEndpoint'].OutputValue\" --output text)\n          echo \"API_ENDPOINT=$API_ENDPOINT\" &gt;&gt; $GITHUB_ENV\n\n      - name: Test API Response\n        run: |\n          response=$(curl -s \"$API_ENDPOINT/count\")\n          echo \"API Response: $response\"\n\n          # Check if the response contains a count field\n          echo $response | grep -q '\"count\":'\n          if [ $? -eq 0 ]; then\n            echo \"API test successful\"\n          else\n            echo \"API test failed\"\n            exit 1\n          fi\n</code></pre> <p>This workflow is more complex and includes:</p> <ol> <li>Running Python tests for the Lambda function</li> <li>Validating Terraform syntax and formatting</li> <li>Planning Terraform changes (with PR comments for review)</li> <li>Applying Terraform changes to the environment</li> <li>Testing the deployed API to ensure it's functioning</li> </ol>","tags":["github-actions","terraform","automation","cloud","devops"]},{"location":"blog/cloud-resume-challenge-with-terraform-automating-deployments-with-github-actions/#implementing-multi-environment-deployments","title":"Implementing Multi-Environment Deployments \ud83c\udf0d","text":"<p>One of the most valuable CI/CD patterns is deploying to multiple environments. Let's modify our backend workflow to support both development and production environments:</p> <pre><code># Additional job for production deployment after dev is successful\n  promote-to-prod:\n    name: 'Promote to Production'\n    runs-on: ubuntu-latest\n    needs: test-api\n    environment: production\n    if: github.event_name == 'workflow_dispatch'\n\n    steps:\n      - name: Checkout Repository\n        uses: actions/checkout@v3\n\n      - name: Configure AWS Credentials\n        uses: aws-actions/configure-aws-credentials@v2\n        with:\n          role-to-assume: arn:aws:iam::${{ secrets.AWS_ACCOUNT_ID }}:role/github-actions-role\n          aws-region: us-east-1\n\n      - name: Setup Terraform\n        uses: hashicorp/setup-terraform@v2\n        with:\n          terraform_version: 1.2.0\n\n      - name: Terraform Init\n        working-directory: ./terraform/environments/prod\n        run: terraform init -backend-config=\"bucket=${{ secrets.TF_STATE_BUCKET }}\" -backend-config=\"key=${{ secrets.TF_STATE_KEY_PROD }}\" -backend-config=\"region=us-east-1\"\n\n      - name: Terraform Plan\n        working-directory: ./terraform/environments/prod\n        run: terraform plan -var=\"environment=prod\" -var=\"domain_name=${{ secrets.DOMAIN_NAME_PROD }}\" -out=tfplan\n\n      - name: Terraform Apply\n        working-directory: ./terraform/environments/prod\n        run: terraform apply -auto-approve tfplan\n\n      - name: Test Production API\n        run: |\n          API_ENDPOINT=$(aws cloudformation describe-stacks --stack-name resume-backend-prod --query \"Stacks[0].Outputs[?OutputKey=='ApiEndpoint'].OutputValue\" --output text)\n          response=$(curl -s \"$API_ENDPOINT/count\")\n          echo \"API Response: $response\"\n\n          # Check if the response contains a count field\n          echo $response | grep -q '\"count\":'\n          if [ $? -eq 0 ]; then\n            echo \"Production API test successful\"\n          else\n            echo \"Production API test failed\"\n            exit 1\n          fi\n</code></pre>","tags":["github-actions","terraform","automation","cloud","devops"]},{"location":"blog/cloud-resume-challenge-with-terraform-automating-deployments-with-github-actions/#terraform-structure-for-multiple-environments","title":"Terraform Structure for Multiple Environments","text":"<p>To support multiple environments, I've reorganized my Terraform configuration:</p> <pre><code>terraform/\n\u251c\u2500\u2500 modules/\n\u2502   \u251c\u2500\u2500 backend/\n\u2502   \u2502   \u251c\u2500\u2500 api_gateway.tf\n\u2502   \u2502   \u251c\u2500\u2500 dynamodb.tf\n\u2502   \u2502   \u251c\u2500\u2500 lambda.tf\n\u2502   \u2502   \u251c\u2500\u2500 variables.tf\n\u2502   \u2502   \u2514\u2500\u2500 outputs.tf\n\u251c\u2500\u2500 environments/\n\u2502   \u251c\u2500\u2500 dev/\n\u2502   \u2502   \u251c\u2500\u2500 main.tf\n\u2502   \u2502   \u251c\u2500\u2500 variables.tf\n\u2502   \u2502   \u2514\u2500\u2500 outputs.tf\n\u2502   \u2514\u2500\u2500 prod/\n\u2502       \u251c\u2500\u2500 main.tf\n\u2502       \u251c\u2500\u2500 variables.tf\n\u2502       \u2514\u2500\u2500 outputs.tf\n</code></pre> <p>Each environment directory contains its own Terraform configuration that references the shared modules.</p>","tags":["github-actions","terraform","automation","cloud","devops"]},{"location":"blog/cloud-resume-challenge-with-terraform-automating-deployments-with-github-actions/#implementing-github-security-best-practices","title":"Implementing GitHub Security Best Practices \ud83d\udd12","text":"<p>To enhance the security of our CI/CD pipelines, I've implemented several additional measures:</p>","tags":["github-actions","terraform","automation","cloud","devops"]},{"location":"blog/cloud-resume-challenge-with-terraform-automating-deployments-with-github-actions/#1-supply-chain-security-with-dependabot","title":"1. Supply Chain Security with Dependabot","text":"<p>Create a file at <code>.github/dependabot.yml</code> in both repositories:</p> <pre><code>version: 2\nupdates:\n  - package-ecosystem: \"github-actions\"\n    directory: \"/\"\n    schedule:\n      interval: \"weekly\"\n    open-pull-requests-limit: 10\n\n  # For frontend\n  - package-ecosystem: \"npm\"\n    directory: \"/\"\n    schedule:\n      interval: \"weekly\"\n    open-pull-requests-limit: 10\n\n  # For backend\n  - package-ecosystem: \"pip\"\n    directory: \"/\"\n    schedule:\n      interval: \"weekly\"\n    open-pull-requests-limit: 10\n</code></pre> <p>This configuration automatically updates dependencies and identifies security vulnerabilities.</p>","tags":["github-actions","terraform","automation","cloud","devops"]},{"location":"blog/cloud-resume-challenge-with-terraform-automating-deployments-with-github-actions/#2-code-scanning-with-codeql","title":"2. Code Scanning with CodeQL","text":"<p>Create a file at <code>.github/workflows/codeql.yml</code> in the backend repository:</p> <pre><code>name: \"CodeQL\"\n\non:\n  push:\n    branches: [ main ]\n  pull_request:\n    branches: [ main ]\n  schedule:\n    - cron: '0 0 * * 0'  # Run weekly\n\njobs:\n  analyze:\n    name: Analyze\n    runs-on: ubuntu-latest\n    permissions:\n      actions: read\n      contents: read\n      security-events: write\n\n    strategy:\n      fail-fast: false\n      matrix:\n        language: [ 'python', 'javascript' ]\n\n    steps:\n    - name: Checkout repository\n      uses: actions/checkout@v3\n\n    - name: Initialize CodeQL\n      uses: github/codeql-action/init@v2\n      with:\n        languages: ${{ matrix.language }}\n\n    - name: Perform CodeQL Analysis\n      uses: github/codeql-action/analyze@v2\n</code></pre> <p>This workflow scans our code for security vulnerabilities and coding problems.</p>","tags":["github-actions","terraform","automation","cloud","devops"]},{"location":"blog/cloud-resume-challenge-with-terraform-automating-deployments-with-github-actions/#3-branch-protection-rules","title":"3. Branch Protection Rules","text":"<p>I've set up branch protection rules for the <code>main</code> branch in both repositories:</p> <ul> <li>Require pull request reviews before merging</li> <li>Require status checks to pass before merging</li> <li>Require signed commits</li> <li>Do not allow bypassing the above settings</li> </ul>","tags":["github-actions","terraform","automation","cloud","devops"]},{"location":"blog/cloud-resume-challenge-with-terraform-automating-deployments-with-github-actions/#adding-verification-tests-to-the-workflow","title":"Adding Verification Tests to the Workflow \ud83e\uddea","text":"<p>In addition to unit tests, I've added end-to-end integration tests to verify that the frontend and backend work together correctly:</p>","tags":["github-actions","terraform","automation","cloud","devops"]},{"location":"blog/cloud-resume-challenge-with-terraform-automating-deployments-with-github-actions/#1-frontend-backend-integration-test","title":"1. Frontend-Backend Integration Test","text":"<p>Create a file at <code>tests/integration-test.js</code> in the frontend repository:</p> <pre><code>const axios = require('axios');\nconst assert = require('assert');\n\n// URLs to test - these should be passed as environment variables\nconst WEBSITE_URL = process.env.WEBSITE_URL || 'https://resume.yourdomain.com';\nconst API_URL = process.env.API_URL || 'https://api.yourdomain.com/count';\n\n// Test that the API returns a valid response\nasync function testAPI() {\n  try {\n    console.log(`Testing API at ${API_URL}`);\n    const response = await axios.get(API_URL);\n\n    // Verify the API response contains a count\n    assert(response.status === 200, `API returned status ${response.status}`);\n    assert(response.data.count !== undefined, 'API response missing count field');\n    assert(typeof response.data.count === 'number', 'Count is not a number');\n\n    console.log(`API test successful. Count: ${response.data.count}`);\n    return true;\n  } catch (error) {\n    console.error('API test failed:', error.message);\n    return false;\n  }\n}\n\n// Test that the website loads and contains necessary elements\nasync function testWebsite() {\n  try {\n    console.log(`Testing website at ${WEBSITE_URL}`);\n    const response = await axios.get(WEBSITE_URL);\n\n    // Verify the website loads\n    assert(response.status === 200, `Website returned status ${response.status}`);\n\n    // Check that the page contains some expected content\n    assert(response.data.includes('&lt;html'), 'Response is not HTML');\n    assert(response.data.includes('id=\"count\"'), 'Counter element not found');\n\n    console.log('Website test successful');\n    return true;\n  } catch (error) {\n    console.error('Website test failed:', error.message);\n    return false;\n  }\n}\n\n// Run all tests\nasync function runTests() {\n  const apiResult = await testAPI();\n  const websiteResult = await testWebsite();\n\n  if (apiResult &amp;&amp; websiteResult) {\n    console.log('All integration tests passed!');\n    process.exit(0);\n  } else {\n    console.error('Some integration tests failed');\n    process.exit(1);\n  }\n}\n\n// Run the tests\nrunTests();\n</code></pre> <p>Then add a step to the workflow:</p> <pre><code>- name: Run Integration Tests\n  run: |\n    npm install axios\n    node tests/integration-test.js\n  env:\n    WEBSITE_URL: ${{ secrets.WEBSITE_URL }}\n    API_URL: ${{ secrets.API_URL }}\n</code></pre>","tags":["github-actions","terraform","automation","cloud","devops"]},{"location":"blog/cloud-resume-challenge-with-terraform-automating-deployments-with-github-actions/#implementing-secure-github-action-secrets","title":"Implementing Secure GitHub Action Secrets \ud83d\udd10","text":"<p>For our GitHub Actions workflows, I've set up the following repository secrets:</p> <ul> <li><code>AWS_ACCOUNT_ID</code>: The AWS account ID used for OIDC authentication</li> <li><code>S3_BUCKET_NAME</code>: The name of the S3 bucket for the website</li> <li><code>CLOUDFRONT_DISTRIBUTION_ID</code>: The ID of the CloudFront distribution</li> <li><code>WEBSITE_URL</code>: The URL of the deployed website</li> <li><code>API_URL</code>: The URL of the deployed API</li> <li><code>TF_STATE_BUCKET</code>: The bucket for Terraform state</li> <li><code>TF_STATE_KEY</code>: The key for Terraform state (dev)</li> <li><code>TF_STATE_KEY_PROD</code>: The key for Terraform state (prod)</li> <li><code>DOMAIN_NAME</code>: The domain name for the dev environment</li> <li><code>DOMAIN_NAME_PROD</code>: The domain name for the prod environment</li> </ul> <p>These secrets are protected by GitHub and only exposed to authorized workflow runs.</p>","tags":["github-actions","terraform","automation","cloud","devops"]},{"location":"blog/cloud-resume-challenge-with-terraform-automating-deployments-with-github-actions/#managing-manual-approvals-for-production-deployments","title":"Managing Manual Approvals for Production Deployments \ud83d\udea6","text":"<p>For production deployments, I've added a manual approval step using GitHub Environments:</p> <ol> <li>Go to your repository settings</li> <li>Navigate to Environments</li> <li>Create a new environment called \"production\"</li> <li>Enable \"Required reviewers\" and add yourself</li> <li>Configure \"Deployment branches\" to limit deployments to specific branches</li> </ol> <p>Now, production deployments will require explicit approval from an authorized reviewer.</p>","tags":["github-actions","terraform","automation","cloud","devops"]},{"location":"blog/cloud-resume-challenge-with-terraform-automating-deployments-with-github-actions/#monitoring-deployment-status-and-notifications","title":"Monitoring Deployment Status and Notifications \ud83d\udcca","text":"<p>To stay informed about deployment status, I've added notifications to the workflow:</p> <pre><code>- name: Notify Deployment Success\n  if: success()\n  uses: rtCamp/action-slack-notify@v2\n  env:\n    SLACK_WEBHOOK: ${{ secrets.SLACK_WEBHOOK }}\n    SLACK_TITLE: Deployment Successful\n    SLACK_MESSAGE: \"\u2705 Deployment to ${{ github.workflow }} was successful!\"\n    SLACK_COLOR: good\n\n- name: Notify Deployment Failure\n  if: failure()\n  uses: rtCamp/action-slack-notify@v2\n  env:\n    SLACK_WEBHOOK: ${{ secrets.SLACK_WEBHOOK }}\n    SLACK_TITLE: Deployment Failed\n    SLACK_MESSAGE: \"\u274c Deployment to ${{ github.workflow }} failed!\"\n    SLACK_COLOR: danger\n</code></pre> <p>This sends notifications to a Slack channel when deployments succeed or fail.</p>","tags":["github-actions","terraform","automation","cloud","devops"]},{"location":"blog/cloud-resume-challenge-with-terraform-automating-deployments-with-github-actions/#implementing-additional-security-for-aws-cloudfront","title":"Implementing Additional Security for AWS CloudFront \ud83d\udd12","text":"<p>To enhance the security of our CloudFront distribution, I've added a custom response headers policy:</p> <pre><code>resource \"aws_cloudfront_response_headers_policy\" \"security_headers\" {\n  name = \"security-headers-policy\"\n\n  security_headers_config {\n    content_security_policy {\n      content_security_policy = \"default-src 'self'; img-src 'self'; script-src 'self'; style-src 'self'; object-src 'none';\"\n      override = true\n    }\n\n    content_type_options {\n      override = true\n    }\n\n    frame_options {\n      frame_option = \"DENY\"\n      override = true\n    }\n\n    referrer_policy {\n      referrer_policy = \"same-origin\"\n      override = true\n    }\n\n    strict_transport_security {\n      access_control_max_age_sec = 31536000\n      include_subdomains = true\n      preload = true\n      override = true\n    }\n\n    xss_protection {\n      mode_block = true\n      protection = true\n      override = true\n    }\n  }\n}\n</code></pre> <p>Then reference this policy in the CloudFront distribution:</p> <pre><code>resource \"aws_cloudfront_distribution\" \"website\" {\n  # ... other configuration ...\n\n  default_cache_behavior {\n    # ... other configuration ...\n    response_headers_policy_id = aws_cloudfront_response_headers_policy.security_headers.id\n  }\n}\n</code></pre>","tags":["github-actions","terraform","automation","cloud","devops"]},{"location":"blog/cloud-resume-challenge-with-terraform-automating-deployments-with-github-actions/#lessons-learned","title":"Lessons Learned \ud83d\udca1","text":"<p>Implementing CI/CD for this project taught me several valuable lessons:</p> <ol> <li> <p>Start Simple, Then Iterate: My first workflow was basic - just syncing files to S3. As I gained confidence, I added testing, multiple environments, and security features.</p> </li> <li> <p>Security Is Non-Negotiable: Using OIDC for authentication instead of long-lived credentials was a game-changer for security. This approach follows AWS best practices and eliminates credential management headaches.</p> </li> <li> <p>Test Everything: Automated tests at every level (unit, integration, end-to-end) catch issues early. The time invested in writing tests paid off with more reliable deployments.</p> </li> <li> <p>Environment Separation: Keeping development and production environments separate allowed me to test changes safely before affecting the live site.</p> </li> <li> <p>Infrastructure as Code Works: Using Terraform to define all infrastructure components made the CI/CD process much more reliable. Everything is tracked, versioned, and repeatable.</p> </li> </ol>","tags":["github-actions","terraform","automation","cloud","devops"]},{"location":"blog/cloud-resume-challenge-with-terraform-automating-deployments-with-github-actions/#my-integration-challenges-and-solutions","title":"My Integration Challenges and Solutions \ud83e\udde9","text":"<p>During implementation, I encountered several challenges:</p> <ol> <li> <p>CORS Issues: The API and website needed proper CORS configuration to work together. Adding the correct headers in both Lambda and API Gateway fixed this.</p> </li> <li> <p>Environment Variables: Managing different configurations for dev and prod was tricky. I solved this by using GitHub environment variables and separate Terraform workspaces.</p> </li> <li> <p>Cache Invalidation Delays: Changes to the website sometimes weren't visible immediately due to CloudFront caching. Adding proper cache invalidation to the workflow fixed this.</p> </li> <li> <p>State Locking: When multiple workflow runs executed simultaneously, they occasionally conflicted on Terraform state. Using DynamoDB for state locking resolved this issue.</p> </li> </ol>","tags":["github-actions","terraform","automation","cloud","devops"]},{"location":"blog/cloud-resume-challenge-with-terraform-automating-deployments-with-github-actions/#devops-mod-multi-stage-pipeline-with-pull-request-environments","title":"DevOps Mod: Multi-Stage Pipeline with Pull Request Environments \ud83d\ude80","text":"<p>To extend this challenge further, I implemented a feature that creates temporary preview environments for pull requests:</p> <pre><code>  create_preview:\n    name: 'Create Preview Environment'\n    runs-on: ubuntu-latest\n    if: github.event_name == 'pull_request'\n\n    steps:\n      - name: Checkout Repository\n        uses: actions/checkout@v3\n\n      - name: Configure AWS Credentials\n        uses: aws-actions/configure-aws-credentials@v2\n        with:\n          role-to-assume: arn:aws:iam::${{ secrets.AWS_ACCOUNT_ID }}:role/github-actions-role\n          aws-region: us-east-1\n\n      - name: Setup Terraform\n        uses: hashicorp/setup-terraform@v2\n        with:\n          terraform_version: 1.2.0\n\n      - name: Generate Unique Environment Name\n        run: |\n          PR_NUMBER=${{ github.event.pull_request.number }}\n          BRANCH_NAME=$(echo ${{ github.head_ref }} | tr -cd '[:alnum:]' | tr '[:upper:]' '[:lower:]')\n          ENV_NAME=\"pr-${PR_NUMBER}-${BRANCH_NAME}\"\n          echo \"ENV_NAME=${ENV_NAME}\" &gt;&gt; $GITHUB_ENV\n\n      - name: Terraform Init\n        working-directory: ./terraform\n        run: terraform init -backend-config=\"bucket=${{ secrets.TF_STATE_BUCKET }}\" -backend-config=\"key=preview/${{ env.ENV_NAME }}/terraform.tfstate\" -backend-config=\"region=us-east-1\"\n\n      - name: Terraform Apply\n        working-directory: ./terraform\n        run: |\n          terraform apply -auto-approve \\\n            -var=\"environment=${{ env.ENV_NAME }}\" \\\n            -var=\"domain_name=pr-${{ github.event.pull_request.number }}.${{ secrets.DOMAIN_NAME }}\"\n\n      - name: Comment Preview URL\n        uses: actions/github-script@v6\n        with:\n          github-token: ${{ secrets.GITHUB_TOKEN }}\n          script: |\n            const output = `## \ud83d\ude80 Preview Environment Deployed\n\n            Preview URL: https://pr-${{ github.event.pull_request.number }}.${{ secrets.DOMAIN_NAME }}\n\n            API Endpoint: https://api-pr-${{ github.event.pull_request.number }}.${{ secrets.DOMAIN_NAME }}/count\n\n            This environment will be automatically deleted when the PR is closed.`;\n\n            github.rest.issues.createComment({\n              issue_number: context.issue.number,\n              owner: context.repo.owner,\n              repo: context.repo.repo,\n              body: output\n            })\n</code></pre> <p>And add a cleanup job to delete the preview environment when the PR is closed:</p> <pre><code>  cleanup_preview:\n    name: 'Cleanup Preview Environment'\n    runs-on: ubuntu-latest\n    if: github.event_name == 'pull_request' &amp;&amp; github.event.action == 'closed'\n\n    steps:\n      # Similar to create_preview but with terraform destroy\n</code></pre>","tags":["github-actions","terraform","automation","cloud","devops"]},{"location":"blog/cloud-resume-challenge-with-terraform-automating-deployments-with-github-actions/#security-mod-implementing-aws-secrets-manager-for-api-keys","title":"Security Mod: Implementing AWS Secrets Manager for API Keys \ud83d\udd10","text":"<p>To enhance the security of our API, I added API key authentication using AWS Secrets Manager:</p> <pre><code># Create a secret to store the API key\nresource \"aws_secretsmanager_secret\" \"api_key\" {\n  name        = \"resume-api-key-${var.environment}\"\n  description = \"API key for the Resume API\"\n}\n\n# Generate a random API key\nresource \"random_password\" \"api_key\" {\n  length  = 32\n  special = false\n}\n\n# Store the API key in Secrets Manager\nresource \"aws_secretsmanager_secret_version\" \"api_key\" {\n  secret_id     = aws_secretsmanager_secret.api_key.id\n  secret_string = random_password.api_key.result\n}\n\n# Add API key to API Gateway\nresource \"aws_api_gateway_api_key\" \"visitor_counter\" {\n  name = \"visitor-counter-key-${var.environment}\"\n}\n\nresource \"aws_api_gateway_usage_plan\" \"visitor_counter\" {\n  name = \"visitor-counter-usage-plan-${var.environment}\"\n\n  api_stages {\n    api_id = aws_api_gateway_rest_api.visitor_counter.id\n    stage  = aws_api_gateway_deployment.visitor_counter.stage_name\n  }\n\n  quota_settings {\n    limit  = 1000\n    period = \"DAY\"\n  }\n\n  throttle_settings {\n    burst_limit = 10\n    rate_limit  = 5\n  }\n}\n\nresource \"aws_api_gateway_usage_plan_key\" \"visitor_counter\" {\n  key_id        = aws_api_gateway_api_key.visitor_counter.id\n  key_type      = \"API_KEY\"\n  usage_plan_id = aws_api_gateway_usage_plan.visitor_counter.id\n}\n\n# Update the Lambda function to verify the API key\nresource \"aws_lambda_function\" \"visitor_counter\" {\n  # ... existing configuration ...\n\n  environment {\n    variables = {\n      DYNAMODB_TABLE = aws_dynamodb_table.visitor_counter.name\n      ALLOWED_ORIGIN = var.website_domain\n      API_KEY_SECRET = aws_secretsmanager_secret.api_key.name\n    }\n  }\n}\n</code></pre> <p>Then, modify the Lambda function to retrieve and validate the API key:</p> <pre><code>import boto3\nimport json\nimport os\n\n# Initialize Secrets Manager client\nsecretsmanager = boto3.client('secretsmanager')\n\ndef get_api_key():\n    \"\"\"Retrieve the API key from Secrets Manager\"\"\"\n    secret_name = os.environ['API_KEY_SECRET']\n    response = secretsmanager.get_secret_value(SecretId=secret_name)\n    return response['SecretString']\n\ndef lambda_handler(event, context):\n    # Verify API key\n    api_key = event.get('headers', {}).get('x-api-key')\n    expected_api_key = get_api_key()\n\n    if api_key != expected_api_key:\n        return {\n            'statusCode': 403,\n            'headers': {\n                'Content-Type': 'application/json'\n            },\n            'body': json.dumps({\n                'error': 'Forbidden',\n                'message': 'Invalid API key'\n            })\n        }\n\n    # Rest of the function...\n</code></pre>","tags":["github-actions","terraform","automation","cloud","devops"]},{"location":"blog/cloud-resume-challenge-with-terraform-automating-deployments-with-github-actions/#next-steps","title":"Next Steps \u23ed\ufe0f","text":"<p>With our CI/CD pipelines in place, our Cloud Resume Challenge implementation is complete! In the final post, we'll reflect on the project as a whole, discuss lessons learned, and explore potential future enhancements.</p> <p>Up Next: [Cloud Resume Challenge with Terraform: Final Thoughts &amp; Lessons Learned] \ud83d\udd17</p> <p>Share on  Share on </p>","tags":["github-actions","terraform","automation","cloud","devops"]},{"location":"blog/gitops-project-automating-aws-infrastructure-deployment-with-terraform-and-github-actions/","title":"GitOps Project: Automating AWS infrastructure deployment with Terraform and GitHub Actions","text":"","tags":["technology","iac","terraform","github"]},{"location":"blog/gitops-project-automating-aws-infrastructure-deployment-with-terraform-and-github-actions/#overview","title":"Overview","text":"<p>As infrastructure becomes more complex, ensuring automation, security, and compliance is crucial. This blog post walks through setting up a GitOps pipeline that simplifies AWS infrastructure management using Terraform and GitHub Actions. Whether you're an engineer looking to automate deployments or a security-conscious DevOps professional, this guide provides a structured approach to implementing Infrastructure as Code (IaC) best practices.</p> <p>This blog post documents my experience with the MoreThanCertified GitOps Minicamp, where students set up a fully automated GitOps pipeline to manage AWS infrastructure using Terraform and GitHub Actions. The goal is to implement Infrastructure as Code (IaC) best practices, enforce security policies, and automate infrastructure deployment.</p> <p>This tutorial is meant as an overview \"at-a-glance\" summary of the steps I took to implement the pipeline from project start, to resource deployment. The MoreThanCertified GitOps Minicamp goes into a lot more detail and covers all the topics in this post in more depth. I would highly recommend checking the course out if you haven't already.</p> <p>In this tutorial, I will guide you through:</p> <ul> <li> <p>Setting up GitHub Codespaces for development</p> </li> <li> <p>Configuring a Terraform backend in AWS with CloudFormation</p> </li> <li> <p>Deploying AWS resources with Terraform</p> </li> <li> <p>Running security, cost, and policy checks using GitHub Actions</p> </li> <li> <p>Implementing a CI/CD pipeline with GitHub Actions</p> </li> </ul>","tags":["technology","iac","terraform","github"]},{"location":"blog/gitops-project-automating-aws-infrastructure-deployment-with-terraform-and-github-actions/#visualizing-the-pipeline-architecture","title":"Visualizing the Pipeline Architecture","text":"<p>The diagram below illustrates the end-to-end GitOps workflow:  </p> <ul> <li>GitHub Actions automates security scans, cost checks, and Terraform execution.  </li> <li>Terraform manages AWS infrastructure, storing state in an S3 backend.  </li> <li>OpenID Connect (OIDC) ensures secure authentication to AWS.</li> </ul> <p></p>","tags":["technology","iac","terraform","github"]},{"location":"blog/gitops-project-automating-aws-infrastructure-deployment-with-terraform-and-github-actions/#setting-up-github-codespaces-for-development","title":"Setting Up GitHub Codespaces for Development","text":"<p>Prerequisites</p> <ul> <li> <p>Previous knowledge and use of Github is assumed.</p> </li> <li> <p>A GitHub repository needs to be created where the Terraform code will be stored.</p> </li> <li> <p>GitHub Codespaces enabled in your repository settings.</p> </li> </ul>","tags":["technology","iac","terraform","github"]},{"location":"blog/gitops-project-automating-aws-infrastructure-deployment-with-terraform-and-github-actions/#enable-codespaces-in-github","title":"Enable Codespaces in GitHub","text":"<ol> <li> <p>Navigate to <code>Settings &gt; Codespaces</code> in your repository.</p> </li> <li> <p>Ensure that Codespaces is enabled for your repository.</p> </li> <li> <p>Create a new Codespace and open it.</p> </li> <li> <p>The Codespace should open the browser based IDE, in the root of the repo you chose.</p> </li> </ol>","tags":["technology","iac","terraform","github"]},{"location":"blog/gitops-project-automating-aws-infrastructure-deployment-with-terraform-and-github-actions/#configure-development-environment","title":"Configure Development Environment","text":"<p>Install Terraform in Codespaces</p> <ul> <li>Copy, paste, and then run the following in your terminal.</li> </ul> Terraform install script<pre><code># Update package list and install dependencies\nsudo apt-get update &amp;&amp; sudo apt-get install -y gnupg software-properties-common\n\n# Add HashiCorp\u2019s GPG key\nwget -O- https://apt.releases.hashicorp.com/gpg | sudo gpg --dearmor -o /usr/share/keyrings/hashicorp-archive-keyring.gpg\n\n# Add the Terraform repository\necho \"deb [signed-by=/usr/share/keyrings/hashicorp-archive-keyring.gpg] https://apt.releases.hashicorp.com $(lsb_release -cs) main\" | sudo tee /etc/apt/sources.list.d/hashicorp.list\n\n# Update and install Terraform\nsudo apt-get update &amp;&amp; sudo apt-get install -y terraform\n\n# Verify installation\nterraform version\n</code></pre> <p>Verify Terraform Installation</p> <pre><code>terraform -version\n</code></pre>","tags":["technology","iac","terraform","github"]},{"location":"blog/gitops-project-automating-aws-infrastructure-deployment-with-terraform-and-github-actions/#configuring-a-terraform-backend-in-aws-using-cloudformation","title":"Configuring a Terraform Backend in AWS Using CloudFormation","text":"<p>Deploying an OIDC Role for GitHub Actions</p> <p>To allow GitHub Actions to authenticate securely with AWS, we use an OIDC (OpenID Connect) role. This CloudFormation template sets up the necessary IAM role and OIDC provider.</p> <p>Add the following code to a new template file <code>cfn &gt; oidc-role.yaml</code></p> oidc-role.yaml<pre><code>Parameters:\n  Repo:\n    Description: The GitHub organization/repo for which the OIDC provider is set up\n    Type: String \nResources:\n  MyOIDCProvider:\n    Type: 'AWS::IAM::OIDCProvider'\n    Properties:\n      Url: 'https://token.actions.githubusercontent.com'\n      ClientIdList:\n        - sts.amazonaws.com\n      ThumbprintList:\n        - 6938fd4d98bab03faadb97b34396831e3780aea1\n        - 1c58a3a8518e8759bf075b76b750d4f2df264fcd\n  gitops2024Role:\n    Type: 'AWS::IAM::Role'\n    Properties:\n      AssumeRolePolicyDocument:\n        Version: 2012-10-17\n        Statement:\n          - Effect: Allow\n            Principal:\n              Federated: !Sub &gt;-\n                arn:aws:iam::${AWS::AccountId}:oidc-provider/token.actions.githubusercontent.com\n            Action: 'sts:AssumeRoleWithWebIdentity'\n            Condition:\n              StringLike:\n                'token.actions.githubusercontent.com:sub': !Sub 'repo:${Repo}:*'\n              StringEquals:\n                'token.actions.githubusercontent.com:aud': sts.amazonaws.com\n      ManagedPolicyArns:\n        - 'arn:aws:iam::aws:policy/PowerUserAccess'\nOutputs:\n  RoleName:\n    Description: 'The name of the IAM role for GitHub Actions'\n    Value:\n      Ref: gitops2024Role\n    Export:\n      Name:\n        Fn::Sub: '${AWS::StackName}-RoleName'\n</code></pre>","tags":["technology","iac","terraform","github"]},{"location":"blog/gitops-project-automating-aws-infrastructure-deployment-with-terraform-and-github-actions/#prerequisites","title":"Prerequisites","text":"<ul> <li> <p>You must have an AWS account set up in advance.</p> </li> <li> <p>An IAM user with sufficient permissions to create S3 buckets, DynamoDB tables, and IAM roles.</p> </li> <li> <p>A remote Terraform backend ensures state consistency and allows multiple users to collaborate.</p> </li> </ul>","tags":["technology","iac","terraform","github"]},{"location":"blog/gitops-project-automating-aws-infrastructure-deployment-with-terraform-and-github-actions/#why-cloudformation","title":"Why CloudFormation?","text":"<p>CloudFormation is used to create the backend infrastructure as a best practice. This ensures automation and easy re-deployment.</p> <p>Steps (CodeSpaces)</p> <p>Add the following code to a new template file <code>cfn &gt; backend-resources.yaml</code></p> backend-resources.yml<pre><code>AWSTemplateFormatVersion: '2010-09-09'\nDescription: CloudFormation template to create S3 and DynamoDB for Terraform Backend\n\nParameters:\n  S3BucketName:\n    Type: String\n    Description: The name of the S3 bucket to be created for storing Terraform state files.\n    Default: gitops-tf-backend\n\n  DynamoDBTableName:\n    Type: String\n    Description: The name of the DynamoDB table to be created for Terraform state locking.\n    Default: GitopsTerraformLocks\n\nResources:\n  TerraformBackendBucket:\n    Type: 'AWS::S3::Bucket'\n    Properties:\n      BucketName: !Ref S3BucketName\n      VersioningConfiguration:\n        Status: Enabled\n\n  TerraformBackendDynamoDBTable:\n    Type: 'AWS::DynamoDB::Table'\n    Properties:\n      TableName: !Ref DynamoDBTableName\n      AttributeDefinitions:\n        - AttributeName: LockID\n          AttributeType: S\n      KeySchema:\n        - AttributeName: LockID\n          KeyType: HASH\n      ProvisionedThroughput:\n        ReadCapacityUnits: 5\n        WriteCapacityUnits: 5\n      SSESpecification:\n        SSEEnabled: true\n\nOutputs:\n  TerraformBackendBucketName:\n    Description: \"S3 bucket name for the Terraform backend.\"\n    Value: !Ref TerraformBackendBucket\n    Export:\n      Name: !Sub \"${AWS::StackName}-TerraformBackendBucketName\"\n\n  TerraformBackendDynamoDBTableName:\n    Description: \"DynamoDB table name for the Terraform backend.\"\n    Value: !Ref TerraformBackendDynamoDBTable\n    Export:\n      Name: !Sub \"${AWS::StackName}-TerraformBackendDynamoDBTableName\"\n</code></pre> <p>Steps (AWS Console)</p> <ol> <li> <p>Log in to the AWS Management Console and navigate to the CloudFormation service.</p> </li> <li> <p>Click on Create Stack and select With new resources (standard).</p> </li> <li> <p>In the Specify template section, select Upload a template file and upload the backend-resources.yml file.</p> </li> <li> <p>Click Next, enter a Stack name (e.g., <code>TerraformBackend</code>), and proceed.</p> </li> <li> <p>Click Next through the stack options, ensuring the correct IAM permissions are set.</p> </li> <li> <p>Click Create stack and wait for the deployment to complete.</p> </li> </ol> <p>Once the stack is successfully created, go to Resources in the CloudFormation console to confirm that the S3 bucket and DynamoDB table have been provisioned.</p> <p>Once the stack is deployed, navigate to the AWS CloudFormation console (https://console.aws.amazon.com/cloudformation) and check that:</p> <ul> <li> <p>Navigate to the AWS IAM Console.</p> </li> <li> <p>The S3 bucket is listed under AWS S3.</p> </li> <li> <p>The DynamoDB table is available in AWS DynamoDB.</p> </li> <li> <p>Check that a new IAM Role (gitops2024Role) has been created.</p> </li> <li> <p>Confirm that an OIDC Provider exists under IAM.</p> </li> </ul>","tags":["technology","iac","terraform","github"]},{"location":"blog/gitops-project-automating-aws-infrastructure-deployment-with-terraform-and-github-actions/#github-actions-workflows-automating-cicd-for-terraform","title":"GitHub Actions Workflows: Automating CI/CD for Terraform","text":"","tags":["technology","iac","terraform","github"]},{"location":"blog/gitops-project-automating-aws-infrastructure-deployment-with-terraform-and-github-actions/#example-mermaid-diagram","title":"Example Mermaid Diagram:","text":"<pre><code>graph TD;\n    Developer --&gt;|Push to GitHub| GitHub_Actions;\n    GitHub_Actions --&gt;|Run Formatting Checks| TFLint;\n    TFLint --&gt;|Run Security Checks| Trivy;\n    GitHub_Actions --&gt;|Run Terraform Plan| Terraform_Plan;\n    GitHub_Actions --&gt;|Run Cost Analysis| Infracost;\n    Terraform_Plan --&gt;|Approval Needed| Manual_Approval;\n    Manual_Approval --&gt;|Deploy Resources| Terraform_Apply;\n    Terraform_Apply --&gt;|Provision AWS Resources| AWS;\n    AWS --&gt;|Destroy if needed| Terraform_Destroy;</code></pre> <p>\ud83d\udccc GitHub Actions Workflow Execution Order:</p> <p>1\ufe0f\u20e3 TFLint &amp; Trivy Security Scan \u2013 Ensures best practices &amp; security compliance 2\ufe0f\u20e3 Terraform Plan \u2013 Generates a preview of infrastructure changes 3\ufe0f\u20e3 OPA Policy Checks &amp; Infracost Analysis \u2013 Ensures compliance &amp; cost awareness 4\ufe0f\u20e3 Terraform Apply (manual trigger) \u2013 Deploys the infrastructure 5\ufe0f\u20e3 Terraform Destroy (manual trigger) \u2013 Cleans up resources when no longer needed  </p> <p>Trivy Security Scan</p> <ul> <li>This workflow scans the Terraform configuration for security vulnerabilities using Trivy.</li> </ul> <p>TFLint Code Linter</p> <ul> <li>Lints Terraform code for syntax errors and best practices.</li> </ul> <p>Terraform Plan</p> <ul> <li>Generates a Terraform execution plan and performs OPA policy checks.</li> </ul> <p>Infracost Cost Analysis</p> <ul> <li>Estimates the cost impact of Terraform changes.</li> </ul> <p>Terraform Apply</p> <ul> <li>Deploys infrastructure changes to AWS.</li> </ul> <p>Terraform Destroy</p> <ul> <li>Destroys all infrastructure deployed by Terraform.</li> </ul> <p>Each of these workflows is triggered based on specific events and plays a key role in the CI/CD pipeline. Below is a breakdown of each workflow file, its purpose, how it works, and how it is triggered:</p>","tags":["technology","iac","terraform","github"]},{"location":"blog/gitops-project-automating-aws-infrastructure-deployment-with-terraform-and-github-actions/#trivy-security-scan","title":"Trivy Security Scan","text":"<pre><code>name: Trivy Security Scan\n\non:\n  push:\n    branches:\n      - main\n  pull_request:\n    branches:\n      - main\n      - feature\n  workflow_dispatch:\n\npermissions:\n  contents: read\n  pull-requests: write\n\njobs:\n  security-scan:\n    runs-on: ubuntu-latest\n    steps:\n      - name: Checkout repository\n        uses: actions/checkout@v4\n      - name: Install Trivy\n        uses: aquasecurity/setup-trivy@v0.2.2\n      - name: Run Trivy Terraform Security Scan\n        run: |\n          trivy fs --scanners misconfig --severity HIGH,CRITICAL --format table --exit-code 1 --ignorefile .trivyignore ./terraform | tee trivy-report.txt\n      - name: Display Scan Report\n        if: always()\n        run: cat trivy-report.txt\n      - name: Upload Scan Report\n        if: always()\n        uses: actions/upload-artifact@v4\n        with:\n          name: tfsec-report\n          path: trivy-report.txt\n      - name: Post Scan Results as PR Comment\n        if: always()\n        uses: mshick/add-pr-comment@v2\n        with:\n          message: \"\ud83d\udea8 **Terraform Security Scan Results** \ud83d\udea8\n\n``\n$(cat trivy-report.txt)\n``\n\n\ud83d\udccc **Severity Levels:** `HIGH`, `CRITICAL`\n\ud83d\udd0d **Ignored Findings:** Defined in `.trivyignore`\n\ud83d\udcc4 **Full Report:** Check [tfsec-report](https://github.com/${{ github.repository }}/actions/runs/${{ github.run_id }})\"\n          repo-token: ${{ secrets.GITHUB_TOKEN }}\n</code></pre> <p>Purpose: Scans Terraform configuration for security vulnerabilities.</p> <p>Triggers: Runs on push to main, pull_request to main or feature branches, and can be triggered manually via workflow_dispatch.</p> <p>Key Steps:</p> <ul> <li> <p>Checks out the repository.</p> </li> <li> <p>Installs Trivy security scanner.</p> </li> <li> <p>Runs a scan for HIGH and CRITICAL misconfigurations.</p> </li> <li> <p>Uploads scan results as an artifact and comments on PRs if issues are found.</p> </li> </ul>","tags":["technology","iac","terraform","github"]},{"location":"blog/gitops-project-automating-aws-infrastructure-deployment-with-terraform-and-github-actions/#tflint-code-linter","title":"TFLint Code Linter","text":"<pre><code>name: Lint\non:\n  push:\n    branches: [ main ]\n  pull_request:\n\njobs:\n  tflint:\n    runs-on: ${{ matrix.os }}\n    defaults:\n        run:\n            working-directory: ./terraform\n\n    strategy:\n      matrix:\n        os: [ubuntu-latest]\n\n    steps:\n    - uses: actions/checkout@v4\n      name: Checkout source code\n\n    - uses: actions/cache@v4\n      name: Cache plugin dir\n      with:\n        path: ~/.tflint.d/plugins\n        key: ${{ matrix.os }}-tflint-${{ hashFiles('.tflint.hcl') }}\n\n    - uses: terraform-linters/setup-tflint@v4\n      name: Setup TFLint\n      with:\n        tflint_version: v0.52.0\n    - name: Show version\n      run: tflint --version\n\n    - name: Init TFLint\n      run: tflint --init\n      env:\n        # https://github.com/terraform-linters/tflint/blob/master/docs/user-guide/plugins.md#avoiding-rate-limiting\n        GITHUB_TOKEN: ${{ github.token }}\n\n    - name: Run TFLint\n      run: tflint -f compact\n</code></pre> <p>Purpose: Ensures Terraform code follows best practices and is formatted correctly.</p> <p>Triggers: Runs on push to main and all pull_request events.</p> <p>Key Steps:</p> <ul> <li> <p>Checks out the repository.</p> </li> <li> <p>Caches TFLint plugins to optimize runs.</p> </li> <li> <p>Initializes and runs TFLint to detect formatting and best-practice issues.</p> </li> </ul>","tags":["technology","iac","terraform","github"]},{"location":"blog/gitops-project-automating-aws-infrastructure-deployment-with-terraform-and-github-actions/#terraform-plan","title":"Terraform Plan","text":"<pre><code>name: 'Plan'\n\non:\n  push:\n    branches: [ 'main' ]\n  pull_request:\n  workflow_dispatch:\n\npermissions:\n  contents: read\n  id-token: write\n\njobs:\n\n  terraform:\n    name: 'Terraform'\n    runs-on: ubuntu-latest\n    environment: production\n    defaults:\n      run:\n        shell: bash\n        working-directory: ./terraform\n    env:\n      GH_TOKEN: ${{ secrets.GITHUB_TOKEN }}\n\n\n    steps:\n    # Checkout the repository to the GitHub Actions runner\n    - name: Checkout\n      uses: actions/checkout@v4\n\n    - name: Configure AWS Credentials\n      uses: aws-actions/configure-aws-credentials@v4\n      with:\n        role-to-assume: ${{ secrets.ROLE_TO_ASSUME }}\n        aws-region: eu-west-2\n\n    # Install the latest version of Terraform CLI and configure the Terraform CLI configuration file with a Terraform Cloud user API token\n    - name: Setup Terraform\n      uses: hashicorp/setup-terraform@v3\n\n    # Initialize a new or existing Terraform working directory by creating initial files, loading any remote state, downloading modules, etc.\n    - name: Terraform Init\n      run: terraform init\n\n    # Checks that all Terraform configuration files adhere to a canonical format\n    - name: Terraform Format\n      run: terraform fmt -check\n\n    # Terraform Plan\n    - name: Terraform Plan\n      id: plan\n      run: |\n        terraform plan -out=plan.tfplan\n        terraform show -json plan.tfplan &gt; /tmp/plan.json\n        cat /tmp/plan.json\n\n    - name: Setup OPA\n      uses: open-policy-agent/setup-opa@v2\n      with:\n        version: latest\n\n    - name: Run OPA Tests\n      run: |\n        opaout=$(opa eval --data ../policies/instance-policy.rego --input /tmp/plan.json \"data.terraform.deny\" | jq -r '.result[].expressions[].value[]')\n        [ -z \"$opaout\" ] &amp;&amp; exit 0 || echo \"$opaout\" &amp;&amp; gh pr comment ${{ github.event.pull_request.number }} --body \"### $opaout\" &amp;&amp; exit 1\n</code></pre> <p>Purpose: Generates and evaluates a Terraform execution plan before applying changes.</p> <p>Triggers: Runs on push to main, pull_request, and manually via workflow_dispatch.</p> <p>Key Steps:</p> <ul> <li> <p>Checks out the repository.</p> </li> <li> <p>Configures AWS credentials using OIDC.</p> </li> <li> <p>Initializes Terraform and runs terraform plan, storing the output for later review.</p> </li> <li> <p>Runs OPA (Open Policy Agent) tests against the Terraform plan to enforce security policies.</p> </li> </ul>","tags":["technology","iac","terraform","github"]},{"location":"blog/gitops-project-automating-aws-infrastructure-deployment-with-terraform-and-github-actions/#infracost-cost-analysis","title":"Infracost Cost Analysis","text":"<pre><code>name: 'Run Infracost'\non:\n  pull_request:\n    types: [opened, synchronize, closed]\njobs:\n  infracost-pull-request-checks:\n    name: Infracost Pull Request Checks\n    if: github.event_name == 'pull_request' &amp;&amp; (github.event.action == 'opened' || github.event.action == 'synchronize')\n    runs-on: ubuntu-latest\n    environment: production\n    permissions:\n      contents: read\n      pull-requests: write # Required to post comments\n    steps:\n      - name: Setup Infracost\n        uses: infracost/actions/setup@v3\n        with:\n          api-key: ${{ secrets.INFRACOST_API_KEY }}\n      - name: Checkout base branch\n        uses: actions/checkout@v4\n        with:\n          ref: '${{ github.event.pull_request.base.ref }}'\n      - name: Generate Infracost cost estimate baseline\n        run: |\n          infracost breakdown --path=. \\\n                              --format=json \\\n                              --out-file=/tmp/infracost-base.json\n      - name: Checkout PR branch\n        uses: actions/checkout@v4\n      - name: Generate Infracost diff\n        run: |\n          infracost diff --path=. \\\n                          --format=json \\\n                          --compare-to=/tmp/infracost-base.json \\\n                          --out-file=/tmp/infracost.json\n      - name: Post Infracost comment\n        run: |\n            infracost comment github --path=/tmp/infracost.json \\\n                                     --repo=$GITHUB_REPOSITORY \\\n                                     --github-token=${{ github.token }} \\\n                                     --pull-request=${{ github.event.pull_request.number }} \\\n                                     --behavior=update \\\n                                     --policy-path ./policies/cost.rego\n</code></pre> <p>Purpose: Estimates the cost impact of Terraform changes before they are applied.</p> <p>Triggers: Runs on pull_request when a PR is opened, updated, or closed.</p> <p>Key Steps:</p> <ul> <li> <p>Sets up Infracost with an API key.</p> </li> <li> <p>Runs cost analysis for the current branch and compares it with the base branch.</p> </li> <li> <p>Posts a cost breakdown as a comment on the PR.</p> </li> </ul>","tags":["technology","iac","terraform","github"]},{"location":"blog/gitops-project-automating-aws-infrastructure-deployment-with-terraform-and-github-actions/#example-output","title":"Example Output:","text":"Name Quantity Unit Cost Monthly Cost aws_instance.grafana 1 $8.32 $8.32 aws_s3_bucket.gitops-tf 1 $0.03 $0.03","tags":["technology","iac","terraform","github"]},{"location":"blog/gitops-project-automating-aws-infrastructure-deployment-with-terraform-and-github-actions/#terraform-apply","title":"Terraform Apply","text":"<pre><code>name: 'Apply'\n\non: workflow_dispatch\n\npermissions:\n  contents: read\n  id-token: write\n\njobs:\n\n  terraform:\n    name: 'Terraform'\n    runs-on: ubuntu-latest\n\n    defaults:\n      run:\n        shell: bash\n\n    environment: production\n\n    steps:\n    # Checkout the repository to the GitHub Actions runner\n    - name: Configure AWS Credentials\n      uses: aws-actions/configure-aws-credentials@v4\n      with:\n        role-to-assume: ${{ secrets.ROLE_TO_ASSUME }}\n        aws-region: eu-west-2\n    - name: Checkout\n      uses: actions/checkout@v4\n\n    # Install the latest version of Terraform CLI and configure the Terraform CLI configuration file with a Terraform Cloud user API token\n    - name: Setup Terraform\n      uses: hashicorp/setup-terraform@v3\n\n    # Initialize a new or existing Terraform working directory by creating initial files, loading any remote state, downloading modules, etc.\n    - name: Terraform Init\n      run: terraform -chdir=\"./terraform\" init\n\n    # Checks that all Terraform configuration files adhere to a canonical format\n    - name: Terraform Format\n      run: terraform -chdir=\"./terraform\" fmt -check\n\n    # Generates an execution plan for Terraform\n    - name: Terraform Plan\n      run: terraform -chdir=\"./terraform\" plan -input=false\n\n    # Apply the Configuration\n    - name: Terraform Apply\n      run: terraform -chdir=\"./terraform\" apply -input=false -auto-approve\n</code></pre> <p>Purpose: Applies Terraform changes to deploy the infrastructure.</p> <p>Triggers: Runs only when manually triggered via workflow_dispatch.</p> <p>Key Steps:</p> <ul> <li> <p>Checks out the repository.</p> </li> <li> <p>Configures AWS credentials.</p> </li> <li> <p>Initializes Terraform.</p> </li> <li> <p>Runs terraform apply to deploy resources.</p> </li> </ul>","tags":["technology","iac","terraform","github"]},{"location":"blog/gitops-project-automating-aws-infrastructure-deployment-with-terraform-and-github-actions/#terraform-destroy","title":"Terraform Destroy","text":"<pre><code>name: 'Destroy'\n\non: workflow_dispatch\n\npermissions:\n  contents: read\n  id-token: write\n\njobs:\n\n  terraform:\n    name: 'Terraform'\n    runs-on: ubuntu-latest\n\n    defaults:\n      run:\n        shell: bash\n\n    environment: production\n\n    steps:\n    # Checkout the repository to the GitHub Actions runner\n    - name: Configure AWS Credentials\n      uses: aws-actions/configure-aws-credentials@v4\n      with:\n        role-to-assume: ${{ secrets.ROLE_TO_ASSUME }}\n        aws-region: eu-west-2\n    - name: Checkout\n      uses: actions/checkout@v4\n\n    # Install the latest version of Terraform CLI and configure the Terraform CLI configuration file with a Terraform Cloud user API token\n    - name: Setup Terraform\n      uses: hashicorp/setup-terraform@v3\n\n    # Initialize a new or existing Terraform working directory by creating initial files, loading any remote state, downloading modules, etc.\n    - name: Terraform Init\n      run: terraform -chdir=\"./terraform\" init\n\n    # Checks that all Terraform configuration files adhere to a canonical format\n    - name: Terraform Format\n      run: terraform -chdir=\"./terraform\" fmt -check\n\n    # Generates an execution plan for Terraform\n    - name: Terraform Plan\n      run: terraform -chdir=\"./terraform\" plan -input=false\n\n    # Apply the Configuration\n    - name: Terraform Destroy\n      run: terraform -chdir=\"./terraform\" destroy -input=false -auto-approve\n</code></pre> <p>Purpose: Destroys deployed infrastructure when it's no longer needed.</p> <p>Triggers: Runs only when manually triggered via workflow_dispatch.</p> <p>Key Steps:</p> <ul> <li> <p>Checks out the repository.</p> </li> <li> <p>Configures AWS credentials.</p> </li> <li> <p>Initializes Terraform.</p> </li> <li> <p>Runs terraform destroy to remove all resources.</p> </li> </ul>","tags":["technology","iac","terraform","github"]},{"location":"blog/gitops-project-automating-aws-infrastructure-deployment-with-terraform-and-github-actions/#push-changes-to-a-feature-branch-in-github-codespaces","title":"Push Changes to a Feature Branch (in GitHub Codespaces)","text":"<p>Run the following commands inside your cloned GitHub repository:</p> <pre><code># Create and switch to a new feature branch\ngit checkout -b feature-branch \n\n# Stage all modified files\ngit add . \n\n# Commit the changes with a meaningful message\ngit commit -m \"Testing CI/CD\"\n\n# Push the feature branch to GitHub\ngit push origin feature-branch\n</code></pre>","tags":["technology","iac","terraform","github"]},{"location":"blog/gitops-project-automating-aws-infrastructure-deployment-with-terraform-and-github-actions/#deploying-aws-resources-with-terraform","title":"Deploying AWS Resources with Terraform","text":"<p>Terraform Configuration Breakdown</p> <p>The following Terraform files define the infrastructure to be deployed. Below, we explain each file and its role in the deployment process.</p>","tags":["technology","iac","terraform","github"]},{"location":"blog/gitops-project-automating-aws-infrastructure-deployment-with-terraform-and-github-actions/#versionstf","title":"versions.tf","text":"<p>Defines the required Terraform version and provider constraints to ensure compatibility.</p> <pre><code>terraform {\n  required_version = \"&gt;= 1.3.0\"\n\n  backend \"s3\" {\n    bucket         = \"gitops-tf-backend-mpcloudlab\"\n    key            = \"terraform.tfstate\"\n    region         = \"eu-west-2\"\n    dynamodb_table = \"GitopsTerraformLocks\"\n  }\n}\n</code></pre>","tags":["technology","iac","terraform","github"]},{"location":"blog/gitops-project-automating-aws-infrastructure-deployment-with-terraform-and-github-actions/#providerstf","title":"providers.tf","text":"<p>Configures the AWS provider and region settings.</p> <pre><code>terraform {\n  required_providers {\n    aws = {\n      source  = \"hashicorp/aws\"\n      version = \"5.69.0\"\n    }\n    http = {\n      source  = \"hashicorp/http\"\n      version = \"3.4.5\"\n    }\n  }\n}\n\nprovider \"aws\" {\n  region = var.region\n}\n</code></pre>","tags":["technology","iac","terraform","github"]},{"location":"blog/gitops-project-automating-aws-infrastructure-deployment-with-terraform-and-github-actions/#variablestf","title":"variables.tf","text":"<p>Declares input variables used throughout the Terraform configuration.</p> <pre><code>variable \"region\" {\n  description = \"AWS region where resources will be deployed\"\n  type        = string\n  default     = \"eu-west-2\"\n}\n\nvariable \"instance_type\" {\n  description = \"EC2 instance type\"\n  type        = string\n  default     = \"t3.micro\"\n}\n</code></pre>","tags":["technology","iac","terraform","github"]},{"location":"blog/gitops-project-automating-aws-infrastructure-deployment-with-terraform-and-github-actions/#terraformtfvars","title":"terraform.tfvars","text":"<p>Defines default values for input variables.</p> <pre><code>region        = \"eu-west-2\"\ninstance_type = \"t3.micro\"\n</code></pre>","tags":["technology","iac","terraform","github"]},{"location":"blog/gitops-project-automating-aws-infrastructure-deployment-with-terraform-and-github-actions/#maintf","title":"main.tf","text":"<p>Defines the main infrastructure resources to be deployed.</p> <pre><code>data \"aws_ami\" \"ubuntu\" {\n  most_recent = true\n\n  filter {\n    name   = \"name\"\n    values = [\"ubuntu/images/hvm-ssd/ubuntu-jammy-22.04-amd64-server-*\"]\n  }\n\n  filter {\n    name   = \"virtualization-type\"\n    values = [\"hvm\"]\n  }\n\n  owners = [\"099720109477\"] # Canonical\n}\n\nresource \"aws_vpc\" \"gitops_vpc\" {\n  cidr_block           = \"10.0.0.0/16\"\n  enable_dns_support   = true\n  enable_dns_hostnames = true\n\n  tags = {\n    Name = \"gitops-vpc\"\n  }\n}\n\nresource \"aws_internet_gateway\" \"gitops_igw\" {\n  vpc_id = aws_vpc.gitops_vpc.id\n\n  tags = {\n    Name = \"gitops-igw\"\n  }\n}\n\nresource \"aws_route_table\" \"gitops_rt\" {\n  vpc_id = aws_vpc.gitops_vpc.id\n\n  route {\n    cidr_block = \"0.0.0.0/0\"\n    gateway_id = aws_internet_gateway.gitops_igw.id\n  }\n\n  tags = {\n    Name = \"gitops-rt\"\n  }\n}\n\nresource \"aws_subnet\" \"gitops_subnet\" {\n  vpc_id                  = aws_vpc.gitops_vpc.id\n  cidr_block              = \"10.0.1.0/24\"\n  map_public_ip_on_launch = true\n\n  tags = {\n    Name = \"gitops-subnet\"\n  }\n}\n\nresource \"aws_route_table_association\" \"gitops_rta\" {\n  subnet_id      = aws_subnet.gitops_subnet.id\n  route_table_id = aws_route_table.gitops_rt.id\n}\n\nresource \"aws_security_group\" \"gitops_sg\" {\n  name        = \"gitops_sg\"\n  description = \"Allow port 3000\"\n  vpc_id      = aws_vpc.gitops_vpc.id\n\n  ingress {\n    from_port   = 3000\n    to_port     = 3000\n    protocol    = \"tcp\"\n    cidr_blocks = [\"0.0.0.0/0\"]\n  }\n\n  egress {\n    from_port   = 0\n    to_port     = 0\n    protocol    = \"-1\"\n    cidr_blocks = [\"0.0.0.0/0\"]\n  }\n\n  tags = {\n    Name = \"gitops-sg\"\n  }\n}\n\nresource \"aws_instance\" \"grafana_server\" {\n  ami                    = data.aws_ami.ubuntu.id\n  instance_type          = var.instance_type\n  subnet_id              = aws_subnet.gitops_subnet.id\n  vpc_security_group_ids = [aws_security_group.gitops_sg.id]\n  user_data              = file(\"userdata.tftpl\")\n\n  root_block_device {\n    encrypted = true\n  }\n\n  metadata_options {\n    http_tokens = \"required\"\n  }\n\n  tags = {\n    Name = \"grafana-server\"\n  }\n}\n\ncheck \"grafana_health_check\" {\n  data \"http\" \"test\" {\n    url = \"http://${aws_instance.grafana_server.public_ip}:3000\"\n    retry {\n      attempts = 10\n    }\n  }\n  assert {\n    condition     = data.http.test.status_code == 200\n    error_message = \"Grafana is inaccessible on port 3000.\"\n  }\n}\n</code></pre>","tags":["technology","iac","terraform","github"]},{"location":"blog/gitops-project-automating-aws-infrastructure-deployment-with-terraform-and-github-actions/#userdatatftpl","title":"userdata.tftpl","text":"<p>Contains startup scripts that run when the EC2 instance is launched.</p> <pre><code>#!/bin/bash\nsudo apt-get install -y apt-transport-https software-properties-common wget &amp;&amp;\nsudo mkdir -p /etc/apt/keyrings/ &amp;&amp;\nwget -q -O - https://apt.grafana.com/gpg.key | gpg --dearmor | sudo tee /etc/apt/keyrings/grafana.gpg &gt; /dev/null &amp;&amp;\necho \"deb [signed-by=/etc/apt/keyrings/grafana.gpg] https://apt.grafana.com stable main\" | sudo tee -a /etc/apt/sources.list.d/grafana.list &amp;&amp;\nsudo apt-get update &amp;&amp;\nsudo apt-get install -y grafana &amp;&amp;\nsudo systemctl start grafana-server &amp;&amp;\nsudo systemctl enable grafana-server\n</code></pre>","tags":["technology","iac","terraform","github"]},{"location":"blog/gitops-project-automating-aws-infrastructure-deployment-with-terraform-and-github-actions/#outputstf","title":"outputs.tf","text":"<p>Defines output values to retrieve important details after deployment.</p> <pre><code>output \"grafana_ip\" {\n  value = \"http://${aws_instance.grafana_server.public_ip}:3000\"\n}\n</code></pre>","tags":["technology","iac","terraform","github"]},{"location":"blog/gitops-project-automating-aws-infrastructure-deployment-with-terraform-and-github-actions/#steps-to-deploy-terraform-resources","title":"Steps to Deploy Terraform Resources","text":"<p>Initialize Terraform</p> <pre><code>terraform init\n</code></pre> <p>Validate the configuration</p> <pre><code>terraform validate\n</code></pre> <p>Generate an execution plan</p> <pre><code>terraform plan\n</code></pre> <p>Apply the configuration</p> <pre><code>terraform apply -auto-approve\n</code></pre> <p>Retrieve outputs</p> <pre><code>terraform output\n</code></pre>","tags":["technology","iac","terraform","github"]},{"location":"blog/gitops-project-automating-aws-infrastructure-deployment-with-terraform-and-github-actions/#verifying-the-deployment","title":"Verifying the Deployment","text":"<ul> <li> <p>Check the AWS Console to confirm that resources have been created.</p> </li> <li> <p>Use SSH or a browser to access the deployed EC2 instance.</p> </li> </ul>","tags":["technology","iac","terraform","github"]},{"location":"blog/gitops-project-automating-aws-infrastructure-deployment-with-terraform-and-github-actions/#destroying-the-infrastructure","title":"Destroying the Infrastructure","text":"<p>When no longer needed, remove all resources:</p> <pre><code>terraform destroy -auto-approve\n</code></pre> <p>or use the <code>Destroy</code> GitHub Actions workflow</p>","tags":["technology","iac","terraform","github"]},{"location":"blog/gitops-project-automating-aws-infrastructure-deployment-with-terraform-and-github-actions/#running-and-testing-the-pipeline","title":"Running and Testing the Pipeline","text":"<p>How to Trigger Workflows</p>","tags":["technology","iac","terraform","github"]},{"location":"blog/gitops-project-automating-aws-infrastructure-deployment-with-terraform-and-github-actions/#automatic-triggers","title":"Automatic Triggers","text":"<ul> <li> <p>Pushes and pull requests to main trigger the Plan, Security Scan, and Linter.</p> </li> <li> <p>Pull requests trigger Infracost Cost Analysis.</p> </li> </ul>","tags":["technology","iac","terraform","github"]},{"location":"blog/gitops-project-automating-aws-infrastructure-deployment-with-terraform-and-github-actions/#manual-triggers","title":"Manual Triggers","text":"<ul> <li> <p><code>terraform apply</code> and <code>terraform destroy</code> require workflow_dispatch (manual execution via GitHub UI).</p> </li> <li> <p>Manually trigger <code>Apply</code> workflow to deploy</p> </li> <li> <p>Manually trigger <code>Destroy</code> workflow to clean up resources</p> </li> </ul>","tags":["technology","iac","terraform","github"]},{"location":"blog/gitops-project-automating-aws-infrastructure-deployment-with-terraform-and-github-actions/#step-by-step-execution","title":"Step-by-Step Execution","text":"","tags":["technology","iac","terraform","github"]},{"location":"blog/gitops-project-automating-aws-infrastructure-deployment-with-terraform-and-github-actions/#push-changes-to-a-new-feature-branch","title":"Push Changes to a New Feature Branch","text":"<pre><code>git checkout -b feature-branch\ngit add .\ngit commit -m \"Testing CI/CD\"\ngit push origin feature-branch\n</code></pre>","tags":["technology","iac","terraform","github"]},{"location":"blog/gitops-project-automating-aws-infrastructure-deployment-with-terraform-and-github-actions/#open-a-pull-request","title":"Open a Pull Request","text":"<ul> <li> <p>This will trigger security scans, cost analysis, and Terraform plan.</p> </li> <li> <p>Review GitHub Actions Results</p> </li> <li> <p>Check logs for security, cost, and linting errors.</p> </li> <li> <p>Merge to branch when approved</p> </li> </ul>","tags":["technology","iac","terraform","github"]},{"location":"blog/gitops-project-automating-aws-infrastructure-deployment-with-terraform-and-github-actions/#project-folder-structure","title":"Project Folder Structure","text":"<pre><code>\u251c\u2500\u2500 .github                  # GitHub Actions workflows\n\u2502   \u2514\u2500\u2500 workflows\n\u2502       \u251c\u2500\u2500 apply.yml       # GitHub Actions workflow for applying Terraform changes\n\u2502       \u251c\u2500\u2500 destroy.yml     # GitHub Actions workflow for destroying Terraform resources\n\u2502       \u251c\u2500\u2500 infracost.yml   # Workflow for running Infracost to estimate Terraform costs\n\u2502       \u251c\u2500\u2500 plan.yml        # Workflow for running Terraform Plan\n\u2502       \u251c\u2500\u2500 tflint.yml      # Workflow for running TFLint to check Terraform syntax\n\u2502       \u2514\u2500\u2500 tfsec.yml       # Workflow for running tfsec to check Terraform security\n\u251c\u2500\u2500 .gitignore              # Specifies files and directories to ignore in Git\n\u251c\u2500\u2500 .trivyignore            # Ignore file for Trivy security scanning\n\u251c\u2500\u2500 README.md               # Project documentation and setup instructions\n\u251c\u2500\u2500 cfn                     # CloudFormation templates for infrastructure\n\u2502   \u251c\u2500\u2500 backend-resources.yaml  # Defines S3 and DynamoDB resources for Terraform backend\n\u2502   \u2514\u2500\u2500 oidc-role.yaml      # CloudFormation template to create OIDC role for GitHub Actions\n\u251c\u2500\u2500 install terraform.txt   # Instructions for installing Terraform\n\u251c\u2500\u2500 policies                # Policies for security and cost analysis\n\u2502   \u251c\u2500\u2500 cost.rego           # OPA policy for Infracost cost enforcement\n\u2502   \u251c\u2500\u2500 instance-policy.rego # OPA policy for Terraform instance compliance\n\u2502   \u2514\u2500\u2500 plan.json           # JSON representation of Terraform Plan for policy validation\n\u2514\u2500\u2500 terraform               # Terraform configuration files\n    \u251c\u2500\u2500 main.tf             # Defines core infrastructure (VPC, EC2, Security Groups, etc.)\n    \u251c\u2500\u2500 outputs.tf          # Specifies Terraform output values\n    \u251c\u2500\u2500 providers.tf        # Configures Terraform providers (AWS, HTTP, etc.)\n    \u251c\u2500\u2500 terraform.tfvars    # Defines Terraform input variable values\n    \u251c\u2500\u2500 userdata.tftpl      # Cloud-init script for configuring EC2 instances\n    \u251c\u2500\u2500 variables.tf        # Declares Terraform input variables\n    \u2514\u2500\u2500 versions.tf         # Specifies required Terraform and provider versions\n</code></pre>","tags":["technology","iac","terraform","github"]},{"location":"blog/gitops-project-automating-aws-infrastructure-deployment-with-terraform-and-github-actions/#conclusion","title":"Conclusion","text":"<p>By implementing this GitOps pipeline, we achieve:</p> <p>\ud83d\ude80 Automation \u2013 Eliminates manual deployments \ud83d\udd12 Security \u2013 Enforces compliance using OPA &amp; Trivy \ud83d\udcb0 Cost Awareness \u2013 Monitors infrastructure costs via Infracost</p> <p>This approach provides scalability, consistency, and security for managing AWS infrastructure.</p> <p>If you have questions, feedback, or suggestions, feel free to reach out!</p> <p>Share on  Share on </p>","tags":["technology","iac","terraform","github"]},{"location":"blog/uninstalling-papercut-mf-client-via-intune--a-step-by-step-guide/","title":"\ud83d\udce2 Uninstalling PaperCut MF Client via Intune \u2013 A Step-by-Step Guide \ud83d\ude80","text":"","tags":["intune","powershell","automation"]},{"location":"blog/uninstalling-papercut-mf-client-via-intune--a-step-by-step-guide/#scenario-overview","title":"\ud83d\udd0d Scenario Overview","text":"<p>Managing software across an enterprise can be a headache, especially when it comes to removing outdated applications. Recently, I needed to uninstall the PaperCut MF Client from multiple Windows PCs in my environment. The challenge? Ensuring a clean removal without user intervention and no leftover files.</p> <p>Rather than relying on manual uninstallation, we used Microsoft Intune to deploy a PowerShell script that handles the removal automatically. This blog post details the full process, from script development to deployment and testing.</p>","tags":["intune","powershell","automation"]},{"location":"blog/uninstalling-papercut-mf-client-via-intune--a-step-by-step-guide/#the-goal","title":"\ud83c\udfaf The Goal","text":"<p>\u2705 Uninstall the PaperCut MF Client silently \u2705 Ensure no residual files are left behind \u2705 Deploy the solution via Intune as a PowerShell script (NOT as a Win32 app) \u2705 Test both locally and remotely before large-scale deployment  </p>","tags":["intune","powershell","automation"]},{"location":"blog/uninstalling-papercut-mf-client-via-intune--a-step-by-step-guide/#step-1-writing-the-uninstall-script","title":"\ud83d\udee0 Step 1: Writing the Uninstall Script","text":"<p>We first created a PowerShell script to:</p> <ol> <li>Stop PaperCut-related processes</li> <li>Run the built-in uninstaller (<code>unins000.exe</code>) if present</li> <li>Use MSIEXEC to remove the MSI-based install</li> <li>Forcefully delete any remaining files and registry entries</li> </ol>","tags":["intune","powershell","automation"]},{"location":"blog/uninstalling-papercut-mf-client-via-intune--a-step-by-step-guide/#the-uninstall-script","title":"\ud83d\udcdd The Uninstall Script","text":"<pre><code># Define variables\n$UninstallExePath = \"C:\\Program Files (x86)\\PaperCut MF Client\\unins000.exe\"\n$MsiProductCode = \"{5B4B80DE-34C4-11E9-9CA9-F53BB8A68831}\"  # Replace with actual Product Code\n$LogFile = \"C:\\ProgramData\\Custom-Intune-Scripts\\Papercut-Uninstall.log\"\n$InstallPath = \"C:\\Program Files (x86)\\PaperCut MF Client\"\n\n# Function to log output\nFunction Write-Log {\n    param ([string]$Message)\n    $TimeStamp = Get-Date -Format \"yyyy-MM-dd HH:mm:ss\"\n    \"$TimeStamp - $Message\" | Out-File -Append -FilePath $LogFile\n}\n\nWrite-Log \"Starting PaperCut MF Client uninstallation process.\"\n\n# Stop any running PaperCut processes before uninstalling\n$Processes = @(\"pc-client\", \"pc-client-java\", \"pc-client-local-cache\")  # Common PaperCut processes\nforeach ($Process in $Processes) {\n    if (Get-Process -Name $Process -ErrorAction SilentlyContinue) {\n        Write-Log \"Stopping process: $Process\"\n        Stop-Process -Name $Process -Force -ErrorAction SilentlyContinue\n    }\n}\n\n# Check if unins000.exe exists\nif (Test-Path $UninstallExePath) {\n    Write-Log \"Found unins000.exe at $UninstallExePath. Initiating uninstallation.\"\n    Start-Process -FilePath $UninstallExePath -ArgumentList \"/SILENT\" -NoNewWindow -Wait\n    Write-Log \"Uninstallation process completed using unins000.exe.\"\n} else {\n    Write-Log \"unins000.exe not found. Attempting MSI uninstallation using Product Code $MsiProductCode.\"\n    Start-Process -FilePath \"msiexec.exe\" -ArgumentList \"/x $MsiProductCode /qn /norestart\" -NoNewWindow -Wait\n}\n\n# Forcefully delete the remaining installation folder\nif (Test-Path $InstallPath) {\n    Write-Log \"Residual files found at $InstallPath. Attempting to remove forcefully.\"\n    takeown /F \"$InstallPath\" /R /D Y | Out-Null\n    icacls \"$InstallPath\" /grant Administrators:F /T /C /Q | Out-Null\n    Remove-Item -Path $InstallPath -Recurse -Force -ErrorAction SilentlyContinue\n    if (-not (Test-Path $InstallPath)) {\n        Write-Log \"SUCCESS: Residual files successfully removed.\"\n    } else {\n        Write-Log \"ERROR: Failed to remove residual files. Manual intervention may be required.\"\n    }\n} else {\n    Write-Log \"No residual files found.\"\n}\n\nWrite-Log \"PaperCut MF Client uninstallation script execution finished.\"\n</code></pre>","tags":["intune","powershell","automation"]},{"location":"blog/uninstalling-papercut-mf-client-via-intune--a-step-by-step-guide/#step-2-testing-the-script-locally","title":"\ud83e\uddea Step 2: Testing the Script Locally","text":"<p>Before deploying via Intune, it's best to test locally:</p> <ol> <li>Open PowerShell as Administrator</li> <li>Run the script manually:</li> </ol> <pre><code>powershell.exe -ExecutionPolicy Bypass -File \"C:\\Path\\To\\Script.ps1\"\n</code></pre> <ol> <li>Verify:</li> <li>Check <code>C:\\Program Files (x86)\\PaperCut MF Client</code> to confirm deletion</li> <li>Check <code>C:\\ProgramData\\AXA-Custom-Intune-Scripts\\Papercut-Uninstall.log</code> for success logs</li> </ol>","tags":["intune","powershell","automation"]},{"location":"blog/uninstalling-papercut-mf-client-via-intune--a-step-by-step-guide/#step-3-running-the-script-on-a-remote-pc","title":"\ud83c\udf0d Step 3: Running the Script on a Remote PC","text":"<p>If you need to test the script remotely before deploying via Intune:</p> <pre><code>$RemotePC = \"COMPUTER-NAME\"  # Change this to the target PC name\nInvoke-Command -ComputerName $RemotePC -FilePath \"C:\\Path\\To\\Script.ps1\" -Credential (Get-Credential)\n</code></pre>","tags":["intune","powershell","automation"]},{"location":"blog/uninstalling-papercut-mf-client-via-intune--a-step-by-step-guide/#step-4-deploying-via-intune","title":"\ud83d\udce1 Step 4: Deploying via Intune","text":"<p>Instead of packaging the script as a <code>.intunewin</code> file, we will deploy it as a PowerShell script in Intune.</p>","tags":["intune","powershell","automation"]},{"location":"blog/uninstalling-papercut-mf-client-via-intune--a-step-by-step-guide/#steps-to-deploy-in-intune","title":"\ud83c\udfaf Steps to Deploy in Intune","text":"<ol> <li>Go to Microsoft Endpoint Manager admin center (endpoint.microsoft.com)</li> <li>Navigate to Devices &gt; Scripts</li> <li>Click Add &gt; Windows 10 and later</li> <li>Upload the PowerShell script (<code>Papercut-Uninstall.ps1</code>)</li> <li>Configure settings:</li> <li>Run script using the logged-on credentials? \u2192 No (runs as SYSTEM)</li> <li>Enforce script signature check? \u2192 No</li> <li>Run script in 64-bit PowerShell Host? \u2192 Yes</li> <li>Assign the script to device groups (not users)</li> <li>Monitor deployment logs in Intune</li> </ol>","tags":["intune","powershell","automation"]},{"location":"blog/uninstalling-papercut-mf-client-via-intune--a-step-by-step-guide/#final-thoughts","title":"\ud83d\udccc Final Thoughts","text":"<p>By using Intune and PowerShell, we successfully automated the silent uninstallation of PaperCut MF Client. This approach ensures a zero-touch removal with no residual files, keeping endpoints clean and manageable. \ud83d\ude80</p> <p>Got questions or need enhancements? Drop them in the comments! \ud83d\ude0a</p> <p>Share on  Share on </p>","tags":["intune","powershell","automation"]},{"location":"blog/life-update-projects-learning-and-the-usual-balancing-act/","title":"\ud83d\udccc Life Update: Projects, Learning, and the Usual Balancing Act","text":"<p>\ud83d\udcf7 </p> <p>Life has been ticking along \u2014 work, learning, personal projects. Nothing dramatic, but I thought I\u2019d put down some thoughts on what I\u2019ve been up to. Mostly for my own benefit, but if anyone else finds it useful, all the better.  </p>","tags":["technology","life"]},{"location":"blog/life-update-projects-learning-and-the-usual-balancing-act/#documenting-projects-properly-finally","title":"\u270d\ufe0f Documenting Projects Properly (Finally)","text":"<p>For a while now, I\u2019ve been meaning to get better at documenting my work. Not in an overly polished or performative way, just something structured enough to be useful. I finally set up this MkDocs blog to keep track of things.  </p> <p>It\u2019s already proving helpful. Writing things down forces me to clarify my thinking, and it\u2019s nice to have a reference point when revisiting old work. If someone else stumbles across it and finds it useful, that\u2019s fine too.  </p>","tags":["technology","life"]},{"location":"blog/life-update-projects-learning-and-the-usual-balancing-act/#work-learning-and-trying-to-keep-up","title":"\u2699\ufe0f Work, Learning, and Trying to Keep Up","text":"<p>Keeping up with work and continuous learning at the same time is an ongoing challenge. There\u2019s always more to read, more to test, more to refine. Lately, I\u2019ve been focused on:  </p> <p>\ud83d\udd39 Automation \u2013 making things run themselves where possible. \ud83d\udd39 Security \u2013 because the landscape never stops shifting. \ud83d\udd39 Cloud optimization \u2013 getting the most out of what\u2019s already in place.  </p> <p>I also need to push myself to do more hands-on work in areas I haven\u2019t explored as much. It\u2019s easy to keep circling around familiar topics, but I want to force some variety into my learning.  </p>","tags":["technology","life"]},{"location":"blog/life-update-projects-learning-and-the-usual-balancing-act/#getting-outside-because-screens-arent-everything","title":"\ud83d\udeb4\u200d\u2642\ufe0f Getting Outside (Because Screens Aren\u2019t Everything)","text":"<p>\ud83d\udcf7  </p> <p>I spend enough time at a desk as it is, so I\u2019ve been continuing to prioritise getting outside. Cycling, running, gardening and birdwatching have been good ways to clear my head.  </p> <p>It\u2019s a reminder that productivity isn\u2019t just about doing more\u2014it\u2019s about doing things well, which is harder when you\u2019re constantly in work mode. Some distance helps.</p>","tags":["technology","life"]},{"location":"blog/life-update-projects-learning-and-the-usual-balancing-act/#next-steps","title":"\ud83d\udd2e Next Steps","text":"<p>No major changes on the horizon, but things I want to focus on:  </p> <p>\u2705 Expanding this blog with more useful content. \u2705 Continuing to refine workflows and knowledge-sharing. \u2705 Dedicating time to learning and experimenting with new tech.  </p> <p>That\u2019s about it. Nothing groundbreaking, just a steady progression of projects and ideas. If you\u2019re also juggling work, learning, and trying to maintain some kind of balance, I\u2019d be interested to hear how you manage it.  </p> <p>Share on  Share on </p>","tags":["technology","life"]},{"location":"blog/enforcing-time-zone-and-dst-compliance-on-windows-servers-using-gpo-and-scheduled-tasks/","title":"Enforcing Time Zone and DST Compliance on Windows Servers Using GPO and Scheduled Tasks","text":"","tags":["dst","timezone","gpo","windows server","azure vm","scheduled task"]},{"location":"blog/enforcing-time-zone-and-dst-compliance-on-windows-servers-using-gpo-and-scheduled-tasks/#why-this-matters","title":"\ud83d\udee0\ufe0f Why This Matters","text":"<p>Time zone misconfigurations \u2014 especially those affecting Daylight Saving Time (DST) \u2014 can cause:</p> <ul> <li>Scheduled tasks to run early or late</li> <li>Timestamp mismatches in logs</li> <li>Errors in time-sensitive integrations</li> </ul> <p>Windows doesn\u2019t always honour DST automatically, particularly in Azure VMs, automated deployments, or custom images.</p>","tags":["dst","timezone","gpo","windows server","azure vm","scheduled task"]},{"location":"blog/enforcing-time-zone-and-dst-compliance-on-windows-servers-using-gpo-and-scheduled-tasks/#whats-changed-in-2025","title":"\ud83d\udd01 What\u2019s Changed in 2025?","text":"<p>As of April 2025, we revised our approach to enforce time zone compliance in a cleaner, more manageable way:</p> <ul> <li>\ud83e\uddf9 Removed all registry-based enforcement from existing GPOs</li> <li>\u2699\ufe0f Executed a one-time PowerShell script to correct servers incorrectly set to <code>UTC</code> (excluding domain controllers)</li> <li>\u23f2\ufe0f Updated the GPO to use a Scheduled Task that sets the correct time zone at startup (<code>GMT Standard Time</code>)</li> </ul>","tags":["dst","timezone","gpo","windows server","azure vm","scheduled task"]},{"location":"blog/enforcing-time-zone-and-dst-compliance-on-windows-servers-using-gpo-and-scheduled-tasks/#audit-process-time-zone-and-ntp-source-check","title":"\ud83d\udccb Audit Process: Time Zone and NTP Source Check","text":"<p>Before remediation, an audit was performed across the server estate to confirm the current time zone and time sync source for each host.</p>","tags":["dst","timezone","gpo","windows server","azure vm","scheduled task"]},{"location":"blog/enforcing-time-zone-and-dst-compliance-on-windows-servers-using-gpo-and-scheduled-tasks/#time-zone-audit-script","title":"\ud83d\udd0e Time Zone Audit Script","text":"<pre><code># Set your target OU\n$OU = \"OU=Servers,DC=yourdomain,DC=local\"\n\n# Prompt for credentials once\n$cred = Get-Credential\n\n# Optional: output to file\n$OutputCsv = \"C:\\Temp\\TimeZoneAudit.csv\"\n$results = @()\n\n# Get all enabled computer objects in the OU\n$servers = Get-ADComputer -Filter {Enabled -eq $true} -SearchBase $OU -Properties Name | Select-Object -ExpandProperty Name\n\nforeach ($server in $servers) {\n    Write-Host \"`nConnecting to $server...\" -ForegroundColor Cyan\n    try {\n        $tzInfo = Invoke-Command -ComputerName $server -Credential $cred -ScriptBlock {\n            $tz = Get-TimeZone\n            $source = (w32tm /query /source) -join ''\n            $status = (w32tm /query /status | Out-String).Trim()\n            [PSCustomObject]@{\n                ComputerName     = $env:COMPUTERNAME\n                TimeZoneId       = $tz.Id\n                TimeZoneDisplay  = $tz.DisplayName\n                CurrentTime      = (Get-Date).ToString(\"yyyy-MM-dd HH:mm:ss\")\n                TimeSource       = $source\n                SyncStatus       = $status\n            }\n        } -ErrorAction Stop\n\n        $results += $tzInfo\n    }\n    catch {\n        Write-Warning \"Failed to connect to ${server}: $_\"\n        $results += [PSCustomObject]@{\n            ComputerName     = $server\n            TimeZoneId       = \"ERROR\"\n            TimeZoneDisplay  = \"ERROR\"\n            CurrentTime      = \"N/A\"\n            TimeSource       = \"N/A\"\n            SyncStatus       = \"N/A\"\n        }\n    }\n}\n\n# Output results\n$results | Format-Table -AutoSize\n\n# Save to CSV\n$results | Export-Csv -NoTypeInformation -Path $OutputCsv\nWrite-Host \"`nAudit complete. Results saved to $OutputCsv\" -ForegroundColor Green\n</code></pre>","tags":["dst","timezone","gpo","windows server","azure vm","scheduled task"]},{"location":"blog/enforcing-time-zone-and-dst-compliance-on-windows-servers-using-gpo-and-scheduled-tasks/#gpo-based-scheduled-task-preferred-solution","title":"\ud83e\uddf0 GPO-Based Scheduled Task (Preferred Solution)","text":"<p>Instead of relying on registry modifications, we now use a Scheduled Task deployed via Group Policy.</p>","tags":["dst","timezone","gpo","windows server","azure vm","scheduled task"]},{"location":"blog/enforcing-time-zone-and-dst-compliance-on-windows-servers-using-gpo-and-scheduled-tasks/#task-overview","title":"\u2705 Task Overview","text":"<ul> <li>Trigger: At Startup</li> <li>Action: Run <code>powershell.exe</code></li> <li>Arguments:</li> </ul> <pre><code>-Command \"Set-TimeZone -Id 'GMT Standard Time'\"\n</code></pre> <p>\ud83d\udca1 The GPO targets all domain-joined servers. Servers in isolated environments (e.g. DMZ) or not joined to the domain are excluded.</p>","tags":["dst","timezone","gpo","windows server","azure vm","scheduled task"]},{"location":"blog/enforcing-time-zone-and-dst-compliance-on-windows-servers-using-gpo-and-scheduled-tasks/#scheduled-task-screenshots","title":"\ud83d\udcf8 Scheduled Task Screenshots","text":"<p> Fig 1: Scheduled Task created via GPO Preferences</p> <p> Fig 2: PowerShell command configuring the time zone</p>","tags":["dst","timezone","gpo","windows server","azure vm","scheduled task"]},{"location":"blog/enforcing-time-zone-and-dst-compliance-on-windows-servers-using-gpo-and-scheduled-tasks/#one-off-remediation-script-setting-the-time-zone","title":"\ud83d\udee0\ufe0f One-Off Remediation Script: Setting the Time Zone","text":"<p>For servers identified as incorrect in the audit, the following script was used to apply the fix:</p> <pre><code># List of servers to correct (e.g., from your audit results)\n$servers = @(\n    \"server1\",\n    \"server2\",\n    \"server3\"\n)\n\n# Prompt for credentials if needed\n$cred = Get-Credential\n\nforeach ($server in $servers) {\n    Write-Host \"Setting time zone on $server...\" -ForegroundColor Cyan\n    try {\n        Invoke-Command -ComputerName $server -Credential $cred -ScriptBlock {\n            Set-TimeZone -Id \"GMT Standard Time\"\n        } -ErrorAction Stop\n\n        Write-Host \"\u2714 $server: Time zone set to GMT Standard Time\" -ForegroundColor Green\n    }\n    catch {\n        Write-Warning \"\u2716 Failed to set time zone on ${server}: $_\"\n    }\n}\n</code></pre>","tags":["dst","timezone","gpo","windows server","azure vm","scheduled task"]},{"location":"blog/enforcing-time-zone-and-dst-compliance-on-windows-servers-using-gpo-and-scheduled-tasks/#how-to-verify-time-zone-dst-compliance","title":"\ud83d\udd0d How to Verify Time Zone + DST Compliance","text":"<p>Use these PowerShell commands to verify:</p> <pre><code>Get-TimeZone\n(Get-TimeZone).SupportsDaylightSavingTime\n</code></pre> <p>And for registry inspection (read-only):</p> <pre><code>Get-ItemProperty \"HKLM:\\SYSTEM\\CurrentControlSet\\Control\\TimeZoneInformation\" |\n  Select-Object TimeZoneKeyName, DisableAutoDaylightTimeSet, DynamicDaylightTimeDisabled\n</code></pre> <p>Expected values:</p> <ul> <li><code>TimeZoneKeyName</code>: <code>\"GMT Standard Time\"</code></li> <li><code>DisableAutoDaylightTimeSet</code>: <code>0</code></li> <li><code>DynamicDaylightTimeDisabled</code>: <code>0</code></li> </ul>","tags":["dst","timezone","gpo","windows server","azure vm","scheduled task"]},{"location":"blog/enforcing-time-zone-and-dst-compliance-on-windows-servers-using-gpo-and-scheduled-tasks/#summary","title":"\ud83e\uddfc Summary","text":"<p>To ensure consistent time zone configuration and DST compliance:</p> <ul> <li>Use a GPO-based Scheduled Task to set <code>GMT Standard Time</code> at startup</li> <li>Run a one-time audit and remediation script to fix legacy misconfigurations</li> <li>Avoid registry edits \u2014 they\u2019re no longer required</li> <li>Validate using <code>Get-TimeZone</code> and confirm time sync via <code>w32tm</code></li> </ul>","tags":["dst","timezone","gpo","windows server","azure vm","scheduled task"]},{"location":"blog/enforcing-time-zone-and-dst-compliance-on-windows-servers-using-gpo-and-scheduled-tasks/#next-steps","title":"\ud83d\udcd8 Next Steps","text":"<ul> <li>[ ] Extend to Azure Arc or Intune-managed servers  </li> <li>[ ] Monitor for changes in Windows DST behaviour in future builds  </li> <li>[ ] Automate reporting to maintain compliance across environments</li> </ul>","tags":["dst","timezone","gpo","windows server","azure vm","scheduled task"]},{"location":"blog/enforcing-time-zone-and-dst-compliance-on-windows-servers-using-gpo-and-scheduled-tasks/#final-thoughts","title":"\ud83e\udde0 Final Thoughts","text":"<p>This GPO+script approach delivers a clean, scalable way to enforce time zone standards and DST logic \u2014 without relying on brittle registry changes.</p> <p>Let me know if you'd like help adapting this for cloud-native or hybrid environments!</p> <p>Share on  Share on </p>","tags":["dst","timezone","gpo","windows server","azure vm","scheduled task"]},{"location":"blog/configuring-uk-regional-settings-on-windows-servers-with-powershell/","title":"Configuring UK Regional Settings on Windows Servers with PowerShell","text":"<p>When building out cloud-hosted or automated deployments of Windows Servers, especially for UK-based organisations, it\u2019s easy to overlook regional settings. But these seemingly small configurations \u2014 like date/time formats, currency symbols, or keyboard layouts \u2014 can have a big impact on usability, application compatibility, and user experience.</p> <p>In this post, I\u2019ll show how I automate this using a simple PowerShell script that sets all relevant UK regional settings in one go.</p>","tags":["powershell","windows server","locale","automation","regional settings","uk"]},{"location":"blog/configuring-uk-regional-settings-on-windows-servers-with-powershell/#why-regional-settings-matter","title":"\ud83d\udd0d Why Regional Settings Matter","text":"<p>Out-of-the-box, Windows often defaults to en-US settings:</p> <ul> <li>Date format becomes <code>MM/DD/YYYY</code></li> <li>Decimal separators switch to <code>.</code> instead of <code>,</code></li> <li>Currency symbols use <code>$</code></li> <li>Time zones default to US-based settings</li> <li>Keyboard layout defaults to US (which can be infuriating!)</li> </ul> <p>For UK-based organisations, this can:</p> <ul> <li>Cause confusion in logs or spreadsheets</li> <li>Break date parsing in scripts or apps expecting <code>DD/MM/YYYY</code></li> <li>Result in the wrong characters being typed (e.g., <code>@</code> vs <code>\"</code>)</li> <li>Require manual fixing after deployment</li> </ul> <p>Automating this ensures consistency across environments, saves time, and avoids annoying regional mismatches.</p>","tags":["powershell","windows server","locale","automation","regional settings","uk"]},{"location":"blog/configuring-uk-regional-settings-on-windows-servers-with-powershell/#script-overview","title":"\ud83d\udd27 Script Overview","text":"<p>I created a PowerShell script that:</p> <ul> <li>Sets the system locale and input methods</li> <li>Configures UK date/time formats</li> <li>Applies the British English language pack (if needed)</li> <li>Sets the time zone to GMT Standard Time (London)</li> </ul> <p>The script can be run manually, included in provisioning pipelines, or dropped into automation tools like Task Scheduler or cloud-init processes.</p>","tags":["powershell","windows server","locale","automation","regional settings","uk"]},{"location":"blog/configuring-uk-regional-settings-on-windows-servers-with-powershell/#prerequisites","title":"\u2705 Prerequisites","text":"<p>To run this script, you should have:</p> <ul> <li>Administrator privileges</li> <li>PowerShell 5.1+ (default on most supported Windows Server versions)</li> <li>Optional: Internet access (if language pack needs to be added)</li> </ul>","tags":["powershell","windows server","locale","automation","regional settings","uk"]},{"location":"blog/configuring-uk-regional-settings-on-windows-servers-with-powershell/#the-script-set-ukregionalsettingsps1","title":"\ud83d\udd39 The Script: <code>Set-UKRegionalSettings.ps1</code>","text":"<pre><code># Set system locale and formats to English (United Kingdom)\nSet-WinSystemLocale -SystemLocale en-GB\nSet-WinUserLanguageList -LanguageList en-GB -Force\nSet-Culture en-GB\nSet-WinHomeLocation -GeoId 242\nSet-TimeZone -Id \"GMT Standard Time\"\n\n# Optional reboot prompt\nWrite-Host \"UK regional settings applied. A reboot is recommended for all changes to take effect.\"\n</code></pre>","tags":["powershell","windows server","locale","automation","regional settings","uk"]},{"location":"blog/configuring-uk-regional-settings-on-windows-servers-with-powershell/#how-to-use-it","title":"\ud83d\ude80 How to Use It","text":"","tags":["powershell","windows server","locale","automation","regional settings","uk"]},{"location":"blog/configuring-uk-regional-settings-on-windows-servers-with-powershell/#option-1-manual-execution","title":"\u2708\ufe0f Option 1: Manual Execution","text":"<ol> <li>Open PowerShell as Administrator</li> <li>Run the script:</li> </ol> <pre><code>.\\Set-UKRegionalSettings.ps1\n</code></pre>","tags":["powershell","windows server","locale","automation","regional settings","uk"]},{"location":"blog/configuring-uk-regional-settings-on-windows-servers-with-powershell/#option-2-include-in-build-pipeline-or-image","title":"\ud83d\udd22 Option 2: Include in Build Pipeline or Image","text":"<p>For Azure VMs or cloud images, consider running this as part of your deployment process via:</p> <ul> <li>Custom Script Extension in ARM/Bicep</li> <li>cloud-init or Terraform provisioners</li> <li>Group Policy Startup Script</li> </ul>","tags":["powershell","windows server","locale","automation","regional settings","uk"]},{"location":"blog/configuring-uk-regional-settings-on-windows-servers-with-powershell/#quick-tips","title":"\u26a1 Quick Tips","text":"<ul> <li>Reboot after running to ensure all settings apply across UI and system processes.</li> <li>For non-UK keyboards (like US physical hardware), you may also want to explicitly set <code>InputLocale</code>.</li> <li>Want to validate the settings? Use:</li> </ul> <pre><code>Get-WinSystemLocale\nGet-Culture\nGet-WinUserLanguageList\nGet-TimeZone\n</code></pre>","tags":["powershell","windows server","locale","automation","regional settings","uk"]},{"location":"blog/configuring-uk-regional-settings-on-windows-servers-with-powershell/#registry-verification-per-user-and-default-settings","title":"\ud83d\udcc2 Registry Verification: Per-User and Default Settings","text":"<p>If you're troubleshooting or validating the configuration for specific users, regional settings are stored in the Windows Registry under:</p>","tags":["powershell","windows server","locale","automation","regional settings","uk"]},{"location":"blog/configuring-uk-regional-settings-on-windows-servers-with-powershell/#for-each-user-profile","title":"\ud83d\udc64 For Each User Profile","text":"<pre><code>HKEY_USERS\\&lt;SID&gt;\\Control Panel\\International\n</code></pre> <p>You can find the user SIDs by looking under <code>HKEY_USERS</code> or using:</p> <pre><code>Get-ChildItem Registry::HKEY_USERS\n</code></pre>","tags":["powershell","windows server","locale","automation","regional settings","uk"]},{"location":"blog/configuring-uk-regional-settings-on-windows-servers-with-powershell/#for-new-users-default-profile","title":"\ud83e\uddf5 For New Users (Default Profile)","text":"<pre><code>HKEY_USERS\\.DEFAULT\\Control Panel\\International\n</code></pre> <p>This determines what settings new user profiles inherit on first logon.</p> <p>You can script changes here if needed, but always test carefully to avoid corrupting profile defaults.</p>","tags":["powershell","windows server","locale","automation","regional settings","uk"]},{"location":"blog/configuring-uk-regional-settings-on-windows-servers-with-powershell/#final-thoughts","title":"\ud83c\udf1f Final Thoughts","text":"<p>Small tweaks like regional settings might seem minor, but they go a long way in making your Windows Server environments feel localised and ready for your users.</p> <p>Automating them early in your build pipeline means one less thing to worry about during post-deployment configuration.</p> <p>Let me know if you want a version of this that handles multi-user scenarios or works across multiple OS versions!</p> <p>Share on  Share on </p>","tags":["powershell","windows server","locale","automation","regional settings","uk"]},{"location":"blog/how-i-used-chatgpt-to-create-az-400-exam-prep-notes-from-mslearn/","title":"How I Used ChatGPT to Create AZ-400 Exam Prep Notes from MSLearn","text":"","tags":["azure","certification","ai-study-tools","exam-prep"]},{"location":"blog/how-i-used-chatgpt-to-create-az-400-exam-prep-notes-from-mslearn/#tldr-results-first","title":"\ud83d\ude80 TL;DR - Results First","text":"<p>Using the method detailed in this post, I successfully passed the AZ-400 exam while creating a reusable study system. This approach helped me transform 34+ hours of MSLearn content into structured, searchable revision notes that I could quickly reference during my exam preparation.</p> <p>Let me walk you through how I developed this system and how you can apply it to your own certification journey.</p>","tags":["azure","certification","ai-study-tools","exam-prep"]},{"location":"blog/how-i-used-chatgpt-to-create-az-400-exam-prep-notes-from-mslearn/#the-challenge","title":"The Challenge","text":"<p>Studying for Microsoft certification exams like AZ-400 can be overwhelming due to the vast amount of content available. Microsoft Learn alone provides over 34 hours of recommended reading, making it difficult to retain everything effectively.</p> <p>To tackle this challenge, I developed a structured method using MSLearn, third-party exam questions, and ChatGPT to create a comprehensive revision guide. This method helped me organize knowledge into concise notes, cheat sheets, glossaries, and knowledge checks, ultimately leading to a successful exam pass!</p> <p>This guide documents my step-by-step process so that you can replicate or adapt it for your own Microsoft exam preparation.</p>","tags":["azure","certification","ai-study-tools","exam-prep"]},{"location":"blog/how-i-used-chatgpt-to-create-az-400-exam-prep-notes-from-mslearn/#goals-of-this-study-approach","title":"\ud83c\udfc6 Goals of This Study Approach","text":"<p>My main objectives were:</p> <ul> <li>\ud83d\udccc Summarize each MSLearn module into easily digestible revision notes.</li> <li>\ud83d\udccc Create a structured, searchable reference for later review.</li> <li>\ud83d\udccc Store my notes in GitHub using Markdown for easy access.</li> <li>\ud83d\udccc Use AI (ChatGPT) to extract and summarize key information.</li> <li>\ud83d\udccc Supplement with third-party practice exams to test my knowledge.</li> </ul> <p>This method is not a quick win but provides an efficient, structured, and reusable way to prepare for any MSLearn-based exam.</p>","tags":["azure","certification","ai-study-tools","exam-prep"]},{"location":"blog/how-i-used-chatgpt-to-create-az-400-exam-prep-notes-from-mslearn/#mslearn-collections-used","title":"\ud83d\udd39 MSLearn Collections Used","text":"<p>To ensure comprehensive coverage of the exam syllabus, I structured my studies around the official Microsoft Learn learning paths. Each path covers a key topic required for AZ-400 certification, including DevOps principles, CI/CD, infrastructure as code, and security best practices. I systematically worked through these collections, summarizing important concepts, capturing key insights, and using ChatGPT to refine the content into structured notes.</p> <p>Below are the learning paths I followed, each linking directly to its respective Microsoft Learn module:</p> <ul> <li>AZ-400 Development for Enterprise DevOps \u2013 Covers Git workflows, branch strategies, and repository management.</li> <li>AZ-400 Implement CI with Azure Pipelines and GitHub Actions \u2013 Focuses on automating build and release processes with Azure Pipelines and GitHub Actions.</li> <li>AZ-400 Design and Implement a Release Strategy \u2013 Covers feature flagging, blue-green deployments, and release management.</li> <li>AZ-400 Implement Secure Continuous Deployment \u2013 Discusses security measures for CI/CD pipelines.</li> <li>AZ-400 Manage Infrastructure as Code Using Azure \u2013 Focuses on managing cloud resources with Terraform, Bicep, and ARM templates.</li> <li>AZ-400 Design and Implement a Dependency Management Strategy \u2013 Covers artifact management and dependency security.</li> <li>AZ-400 Implement Continuous Feedback \u2013 Discusses monitoring and feedback loops for DevOps processes.</li> <li>AZ-400 Implement Security and Validate Code Bases for Compliance \u2013 Focuses on DevSecOps principles and compliance validation.</li> </ul> <p>These resources formed the foundation of my study plan, ensuring alignment with the official exam objectives. I used these collections as the basis for my revision notes, AI-generated summaries, and knowledge checks.</p>","tags":["azure","certification","ai-study-tools","exam-prep"]},{"location":"blog/how-i-used-chatgpt-to-create-az-400-exam-prep-notes-from-mslearn/#process-overview","title":"\ud83d\udcca Process Overview","text":"<p>Before diving into the detailed steps, here's an overview of the complete workflow:</p> <pre><code>MSLearn Content \u2192 Link Collection \u2192 ChatGPT Summarization \u2192 GitHub Storage \u2192 Practice Testing \u2192 Final Review\n</code></pre> <p>Estimated time investment per module:</p> <ul> <li>Manual link collection: ~15 minutes</li> <li>AI summarization and refinement: ~30-60 minutes</li> <li>Review and validation: ~30 minutes</li> <li>Total per module: ~1-1.75 hours (compared to 3-4 hours of traditional study)</li> </ul> <p>These estimates are based on my experience after processing several modules. As you'll see in the learning curve section below, your first few modules might take longer as you refine your workflow.</p> <p>Let's dive into each step of the process in detail.</p>","tags":["azure","certification","ai-study-tools","exam-prep"]},{"location":"blog/how-i-used-chatgpt-to-create-az-400-exam-prep-notes-from-mslearn/#step-1-collecting-and-organizing-mslearn-content","title":"\ud83d\udccc Step 1: Collecting and Organizing MSLearn Content","text":"<p>MSLearn provides structured learning paths, but I needed a way to track important links and content. Here's how I manually compiled everything:</p>","tags":["azure","certification","ai-study-tools","exam-prep"]},{"location":"blog/how-i-used-chatgpt-to-create-az-400-exam-prep-notes-from-mslearn/#manual-link-collection-process","title":"\ud83d\udd39 Manual Link Collection Process","text":"<ol> <li> <p>Initial Setup: I created a dedicated folder structure on my computer with sub-folders for each learning path, mirroring the eventual GitHub repository structure.</p> </li> <li> <p>After each lesson: Captured all relevant hyperlinks and stored them in a <code>.txt</code> file within the appropriate folder. This was as simple as copy-pasting links while reading.</p> </li> <li> <p>At the end of each module: Consolidated all links into the text file and organized them by topic.</p> </li> <li> <p>Mapped content to official exam objectives: Fed the exam study guide into ChatGPT to check alignment, ensuring I wasn't missing critical areas.</p> </li> </ol> <p>\ud83d\udcf8 </p>","tags":["azure","certification","ai-study-tools","exam-prep"]},{"location":"blog/how-i-used-chatgpt-to-create-az-400-exam-prep-notes-from-mslearn/#before-after-example","title":"Before &amp; After Example","text":"<p>Raw MSLearn Content:</p> <pre><code>Unit Title: Implement branch policies in Azure Repos\n- Configure branch policies\n- Implement pull request approval processes\n- Manage branch policy bypasses\n- Configure auto-complete pull requests\n- Configure branch policy permissions\n- Configure build validation\n</code></pre> <p>Transformed into Structured Notes:</p> <pre><code>## Branch Policies in Azure Repos\n\nBranch policies help teams protect important branches by:\n- Requiring code reviews before merging\n- Setting minimum number of reviewers (typically 2+)\n- Enforcing build validation to prevent broken code\n- Restricting direct pushes to protected branches\n\n### Key Configuration Options:\n| Policy | Purpose | Real-world Usage |\n|--------|---------|------------------|\n| Minimum reviewers | Ensures code quality | Set to 2+ for production code |\n| Build validation | Prevents broken builds | Configure with main CI pipeline |\n| Comment resolution | Tracks issue fixes | Require resolution before merge |\n</code></pre> <p>Lesson Learned: Consistent link collection during the learning process is much more efficient than trying to gather everything after completing a module. I developed a habit of copying links as I encountered them, which saved significant time later.</p> <p>\ud83d\udca1 Future Improvement: Automating this process via a script could save time. A PowerShell or Python script could potentially scrape MSLearn modules for relevant links.</p>","tags":["azure","certification","ai-study-tools","exam-prep"]},{"location":"blog/how-i-used-chatgpt-to-create-az-400-exam-prep-notes-from-mslearn/#step-2-using-ai-chatgpt-to-summarize-content","title":"\ud83d\udccc Step 2: Using AI (ChatGPT) to Summarize Content","text":"<p>To turn raw MSLearn material into usable study notes, I fed collected links into ChatGPT and asked it to scrape and summarize key points.</p> <p>I used ChatGPT 4 for this process, as it provided better context handling and more accurate summaries than earlier versions.</p> <p>The summarization workflow consisted of the following steps:</p> <p>1\ufe0f\u20e3 Collected MSLearn Links \u2013 Compiled all module-related links into a text file. 2\ufe0f\u20e3 Fed the Links into ChatGPT \u2013 Asked ChatGPT to analyze and summarize key information. 3\ufe0f\u20e3 Refined the Output Iteratively \u2013 Adjusted prompts to enhance clarity and completeness.</p>","tags":["azure","certification","ai-study-tools","exam-prep"]},{"location":"blog/how-i-used-chatgpt-to-create-az-400-exam-prep-notes-from-mslearn/#crafting-effective-chatgpt-prompts","title":"\ud83d\udd39 Crafting Effective ChatGPT Prompts","text":"<p>Well-structured prompts were essential for generating clear and accurate summaries. Below is an example of my initial prompt:</p> prompt - ChatGPT first iteration<pre><code>Please create a .md file in the same format as the previous ones and include the following:\n\nSummarize key information within each unit, including diagrams, tables, and exercises and labs.\nList steps performed and order of steps/workflow, where applicable.\nUse tables primarily for comparing differences between items.\nInclude:\nKey exam points.\nPoints to remember.\nPrerequisite information.\nInclude any service limits - maximum minutes per month for a particular tier, difference services available in varying tiers/services/SKUs for example\nPermissions required for activities.\nProvide real-world applications, troubleshooting scenarios, and advanced tips.\nHighlight common pitfalls or mistakes to avoid.\nReview the canvas and add/remove any relevant information.\nUse the web to search for supplementary material where necessary, and summarize this information within the notes.\nAvoid external links\u2014include all relevant information directly in the notes.\nEnsure all \"Learning objectives\" in Unit 1 are met by the material included in the output .md file(s)\nEnsure no content is included that doesn't have a real-world example or exam related reference included\nReview the output you have created at the end, and make any further improvements automatically be manually revising the file or implementing your comments.\n\nHere is a list of the links contained in this module.\nUsing the parameters outlined above create a comprehensive exam cram resource cheat sheet, that can be used for my AZ-400 exam prep.\nThe resulting output needs contain material relevant to the AZ-400 study guide:\n\nhttps://learn.microsoft.com/en-us/credentials/certifications/resources/study-guides/az-400\n\nLet me know when you are ready for the module links?\n</code></pre> <p>While this prompt worked initially, I found it had several limitations:</p> <ul> <li>It was too lengthy and complex for ChatGPT to consistently follow all instructions</li> <li>It lacked specific formatting guidance</li> <li>It didn't emphasize accuracy and relevance enough</li> </ul>","tags":["azure","certification","ai-study-tools","exam-prep"]},{"location":"blog/how-i-used-chatgpt-to-create-az-400-exam-prep-notes-from-mslearn/#prompt-evolution","title":"\ud83d\udd39 Prompt Evolution","text":"<p>I refined the prompt through several iterations to address these issues. The key improvements included:</p> <ol> <li>Adding a clear word count limit (750 words per file) to prevent overly lengthy outputs</li> <li>Explicitly prohibiting generic statements like \"understand X\" and \"know how to...\"</li> <li>Requiring concrete examples for each concept rather than theoretical explanations</li> <li>Creating a more structured format with clear section headings and organization</li> <li>Adding a final validation step to ensure content aligned with learning objectives</li> </ol>","tags":["azure","certification","ai-study-tools","exam-prep"]},{"location":"blog/how-i-used-chatgpt-to-create-az-400-exam-prep-notes-from-mslearn/#initial-results-vs-refined-results","title":"Initial Results vs. Refined Results","text":"<p>First Iteration Output (Excerpt):</p> <pre><code>## Key Exam Points\n- Understand the different branch policies in Azure Repos\n- Know how to configure pull request approvals\n- Understand branch policy permissions\n</code></pre> <p>Problems:</p> <ul> <li>Too generic with \"understand\" and \"know how\" statements</li> <li>Lacks specific examples and actionable information</li> <li>No clear formatting structure</li> </ul> <p>Refined Output (After Improved Prompt):</p> <pre><code>## Branch Policies in Azure DevOps\n\n### Key Exam Points\n- Branch policies in Azure Repos protect branches by enforcing code review and build validation\n- Required reviewers policy must be configured with minimum count (2+ recommended for production)\n- Build validation policy links CI pipeline to PR process, ensuring code builds successfully\n- Policy bypasses can be granted to specific users or groups (Project Administrators have bypass by default)\n- Branch policies are set at repository level under Branches \u2192 [...] \u2192 Branch Policies\n\n### Common Scenarios\nWhen setting up branch policies for a large team:\n1. Configure minimum 2 reviewers for main branch\n2. Enable \"Comment resolution\" to require addressing feedback\n3. Link build validation to prevent broken builds\n4. Set reset votes when new changes are pushed\n</code></pre>","tags":["azure","certification","ai-study-tools","exam-prep"]},{"location":"blog/how-i-used-chatgpt-to-create-az-400-exam-prep-notes-from-mslearn/#challenges-solutions","title":"\ud83d\udd39 Challenges &amp; Solutions","text":"<p>Using AI to generate structured content wasn't always seamless. Here are some key challenges and how I addressed them:</p> Challenge Solution Example ChatGPT lost context in long sessions Processed module-by-module instead of bulk inputs Split \"Azure Pipelines\" module into 3 separate prompts Overwrote useful content in iterations Manually saved outputs before requesting refinements Created checkpoint files labeled v1, v2, etc. Large data inputs led to incomplete summaries Used multiple iterations, focusing on key areas of each module First pass: core concepts; Second pass: examples and scenarios Hallucinations on technical details Cross-validated against official documentation Corrected service limits and permission details Generic \"understand X\" statements Explicitly requested specific actionable information Replaced \"Understand CI/CD\" with actual pipeline YAML examples <p>Breaking down content into smaller chunks and applying manual validation helped ensure better results.</p> <p>Learning Curve: My first module took nearly 2 hours to process completely, as I was still figuring out the optimal prompt structure and workflow. By my fifth module, I had reduced this to about 45 minutes through improved prompting and a more streamlined approach.</p>","tags":["azure","certification","ai-study-tools","exam-prep"]},{"location":"blog/how-i-used-chatgpt-to-create-az-400-exam-prep-notes-from-mslearn/#refining-the-output","title":"\ud83d\udd39 Refining the Output","text":"<p>To improve content accuracy, I introduced an additional review prompt:</p> ChatGPT prompt - second iteration<pre><code>Objective:\nCreate a .md file that acts as a comprehensive AZ-400 exam cram resource cheat sheet.\n\nInstructions:\nAct as my Azure DevOps training expert with a focus on preparing me for the AZ-400 exam.\n\nThe output must adhere to the structure and content requirements outlined below:\n\nContent Requirements:\nEach file should contain no more than 750 words (excluding text that make up hyperlinks)\n\nSummarize Key Information:\nInclude summaries for each unit, diagram, table, exercise, and lab where applicable.\nUse clear and concise explanations.\n\nList Steps/Workflows:\nSummarize steps performed in labs/exercises and the order of steps/workflows where applicable.\n\nUse Tables:\nCreate tables primarily for comparing differences between items (examples, but not limited to - features, tiers, SKUs etc ).\n\nKey Exam Points: Highlight crucial information likely to appear in the exam and provide actual examples. \nDo not use generic phrases like \"Understand....\" and  \"Know how to....\". \nI need you to provide the information I need to know for each exam tip.\n\nPoints to Remember: Provide concise, high-priority notes for studying.\n\nPrerequisite Information: Mention anything needed to understand or implement concepts.\n\nService Limits: Include tier limitations (e.g., maximum minutes per month), service availability by SKU, etc.\n\nPermissions Required: Specify roles/permissions necessary for activities.\n\nPractical Applications:\nProvide real-world applications, troubleshooting scenarios, and advanced tips.\nHighlight common pitfalls or mistakes to avoid.\n\nRelevance:\nEnsure the output aligns with the Microsoft AZ-400 study guide \n(https://learn.microsoft.com/en-us/credentials/certifications/resources/study-guides/az-400)\nExclude any content that lacks real-world examples or exam-related references.\n\nFinal Review:\nEvaluate the output to ensure all \"Learning Objectives\" in Unit 1 are met.\nAutomatically revise the file manually if needed to enhance clarity and completeness.\n\n\nPrompt me for a list of URL's or an existing .md file when you have understood the instructions.\n</code></pre>","tags":["azure","certification","ai-study-tools","exam-prep"]},{"location":"blog/how-i-used-chatgpt-to-create-az-400-exam-prep-notes-from-mslearn/#iterative-refinement-technique","title":"\ud83d\udd39 Iterative Refinement Technique","text":"<p>Depending on the results, I would often break the prompt up further, and just use a specific part. For example, once I was happy with the results of a certain output I would re-enter the \"Final Review: Evaluate the output to ensure all \"Learning Objectives\" in Unit 1 are met.\" Automatically revise the file manually if needed to enhance clarity and completeness.\" prompt, once or maybe several times until was happy with the finished output.</p> <p>A typical module would go through 2-3 iterations:</p> <ol> <li>Initial generation - Creates the basic structure and content</li> <li>Content enhancement - Adds real-world examples and specifics</li> <li>Final validation - Checks against learning objectives and improves clarity</li> </ol> <p>For complex topics like Azure Pipelines, I might need 4-5 iterations to fully refine the content.</p>","tags":["azure","certification","ai-study-tools","exam-prep"]},{"location":"blog/how-i-used-chatgpt-to-create-az-400-exam-prep-notes-from-mslearn/#workflow-integration","title":"Workflow Integration","text":"<p>I integrated this process into my daily study routine by:</p> <ol> <li>Reading a module in the morning</li> <li>Collecting links as I went</li> <li>Processing with ChatGPT during lunch break or after work</li> <li>Reviewing and committing to GitHub in the evening</li> </ol> <p>This approach allowed me to maintain consistent progress without feeling overwhelmed.</p>","tags":["azure","certification","ai-study-tools","exam-prep"]},{"location":"blog/how-i-used-chatgpt-to-create-az-400-exam-prep-notes-from-mslearn/#time-saving-tip","title":"Time-Saving Tip","text":"<p>A full module processing cycle typically took about 30-45 minutes, compared to 2-3 hours of traditional study and note-taking. The time investment was front-loaded, but paid dividends during revision.</p>","tags":["azure","certification","ai-study-tools","exam-prep"]},{"location":"blog/how-i-used-chatgpt-to-create-az-400-exam-prep-notes-from-mslearn/#step-3-integrating-third-party-exam-resources","title":"\ud83d\udccc Step 3: Integrating Third-Party Exam Resources","text":"<p>While MSLearn is great for structured content, real-world practice questions are crucial for exam success. I incorporated:</p> <ul> <li>\u2705 MSLearn Official Practice Questions</li> <li>\u2705 Third-Party Providers: Tutorials Dojo, MeasureUp</li> <li>\u2705 Exam Explanation Links (to validate answers)</li> </ul>","tags":["azure","certification","ai-study-tools","exam-prep"]},{"location":"blog/how-i-used-chatgpt-to-create-az-400-exam-prep-notes-from-mslearn/#how-i-validated-third-party-questions","title":"\ud83d\udd39 How I Validated Third-Party Questions","text":"<ul> <li>Checked explanations for links to MSLearn documentation.</li> <li>Manually cross-referenced answers with official study material.</li> <li>Used my GitHub repo's search function to quickly verify concepts.</li> </ul>","tags":["azure","certification","ai-study-tools","exam-prep"]},{"location":"blog/how-i-used-chatgpt-to-create-az-400-exam-prep-notes-from-mslearn/#identifying-valuable-practice-questions","title":"\ud83d\udd39 Identifying Valuable Practice Questions","text":"<p>Not all practice questions are created equal. I prioritized questions that:</p> <ol> <li>Included detailed explanations with documentation links</li> <li>Covered scenarios rather than simple definition recall</li> <li>Tested practical knowledge rather than obscure facts</li> <li>Referenced multiple concepts in a single question</li> </ol> <p>\ud83d\udcf8 </p>","tags":["azure","certification","ai-study-tools","exam-prep"]},{"location":"blog/how-i-used-chatgpt-to-create-az-400-exam-prep-notes-from-mslearn/#supplementary-resources-worth-the-investment","title":"\ud83d\udd39 Supplementary Resources Worth The Investment","text":"<p>Based on my experience, these additional resources provided the best value:</p> <ol> <li>Tutorials Dojo Practice Exams - Excellent explanations and documentation links</li> <li>MeasureUp Official Practice Tests - Most similar to actual exam format</li> <li>WhizLabs Labs - Hands-on practice for key scenarios</li> </ol> <p>The combination of AI-summarized MSLearn content and targeted practice questions created a comprehensive exam preparation strategy.</p> <p>Real-World Application Example: During a practice exam, I encountered a question about configuring branch policies with required reviewers. Using my GitHub repository's search function, I quickly found the related notes I had created, which included the exact setting location and recommended configuration values. This allowed me to answer correctly and understand the underlying concept, rather than just memorizing an answer.</p>","tags":["azure","certification","ai-study-tools","exam-prep"]},{"location":"blog/how-i-used-chatgpt-to-create-az-400-exam-prep-notes-from-mslearn/#step-4-storing-notes-in-github-for-easy-reference","title":"\ud83d\udccc Step 4: Storing Notes in GitHub for Easy Reference","text":"<p>One key advantage of this method was storing all notes in a GitHub repository, allowing easy searchability.</p>","tags":["azure","certification","ai-study-tools","exam-prep"]},{"location":"blog/how-i-used-chatgpt-to-create-az-400-exam-prep-notes-from-mslearn/#initial-github-repository-setup","title":"\ud83d\udd39 Initial GitHub Repository Setup","text":"<ol> <li>Created a new GitHub repository specifically for my AZ-400 exam notes</li> <li>Established a folder structure that mirrored the MSLearn learning paths</li> <li>Set up a README with quick navigation links to major sections</li> <li>Created a consistent file naming convention (numbered by sequence in the learning path)</li> </ol>","tags":["azure","certification","ai-study-tools","exam-prep"]},{"location":"blog/how-i-used-chatgpt-to-create-az-400-exam-prep-notes-from-mslearn/#how-i-structured-my-notes","title":"\ud83d\udd39 How I Structured My Notes","text":"<p>(VS Code folder structure represented in Markdown)</p> <pre><code>\ud83d\udcc2 AZ-400 - MS LEARN - EXAM NOTES/\n \u251c\u2500\u2500 \ud83d\udcc1 .github/\n \u2502\n \u251c\u2500\u2500 \ud83d\udcc1 1. AZ-400 Development for Enterprise DevOps/\n \u2502    \u251c\u2500\u2500 1. Introduction to DevOps.md\n \u2502    \u251c\u2500\u2500 2. Plan Agile with GitHub Projects and Azure Boards.md\n \u2502    \u251c\u2500\u2500 3. Design and implement branch strategies and workflows.md\n \u2502    \u251c\u2500\u2500 4. Collaborate with pull requests in Azure Repos.md\n \u2502    \u251c\u2500\u2500 5. Explore Git hooks.md\n \u2502    \u251c\u2500\u2500 6. Plan foster inner source.md\n \u2502    \u251c\u2500\u2500 7. Manage and configure repositories.md\n \u2502    \u251c\u2500\u2500 8. Identify technical debt.md\n \u2502\n \u251c\u2500\u2500 \ud83d\udcc1 2. AZ-400 Implement CI with Azure Pipelines and GitHub Actions/\n \u2502    \u251c\u2500\u2500 1. Explore Azure Pipelines.md\n \u2502    \u251c\u2500\u2500 2. Manage Azure Pipeline agents and pools.md\n \u2502    \u251c\u2500\u2500 3. Describe pipelines and concurrency.md\n \u2502    \u251c\u2500\u2500 4. Design and implement a pipeline strategy.md\n \u2502    \u251c\u2500\u2500 5. Integrate with Azure Pipelines.md\n \u2502    \u251c\u2500\u2500 6. Introduction to GitHub Actions.md\n \u2502    \u251c\u2500\u2500 7. Learn continuous integration with GitHub Actions.md\n \u2502    \u251c\u2500\u2500 8. Design Container Build Strategy.md\n \u251c\u2500\u2500 \ud83d\udcc1 3. AZ-400 Design and Implement a Release Strategy/\n \u251c\u2500\u2500 ...\n \u251c\u2500\u2500 \ud83d\udcc1 4. AZ-400 Implement a Secure Continuous Deployment using Azure Pipelines/\n \u251c\u2500\u2500 ...\n \u251c\u2500\u2500 \ud83d\udcc1 5. AZ-400 Manage Infrastructure as Code using Azure and DSC/\n \u251c\u2500\u2500 ...\n \u251c\u2500\u2500 \ud83d\udcc1 6. AZ-400 Design and Implement a Dependency Management Strategy/\n \u251c\u2500\u2500 ...\n \u251c\u2500\u2500 \ud83d\udcc1 7. AZ-400 Implement Continuous Feedback\n \u251c\u2500\u2500 ...\n \u251c\u2500\u2500 \ud83d\udcc1 8. AZ-400 Implement Security and Validate Code Bases for Compliance/\n \u251c\u2500\u2500 ...\n \u251c\u2500\u2500 \ud83d\udcc1 AZ-400 Tutorials Dojo - Exam Notes/\n \u2502    \u251c\u2500\u2500 Azure DevOps - Terms and Definitions - Pt 1.md\n \u2502    \u251c\u2500\u2500 Azure DevOps - Terms and Definitions - Pt 2.md\n \u2502    \u251c\u2500\u2500 Azure DevOps - Terms and Definitions - Pt 3.md\n</code></pre> <p>\ud83d\udcf8 </p>","tags":["azure","certification","ai-study-tools","exam-prep"]},{"location":"blog/how-i-used-chatgpt-to-create-az-400-exam-prep-notes-from-mslearn/#workflow-for-adding-new-notes","title":"\ud83d\udd39 Workflow for Adding New Notes","text":"<ol> <li>Create new markdown file in the appropriate folder</li> <li>Paste ChatGPT-generated content and review for accuracy</li> <li>Commit changes with descriptive commit messages (e.g., \"Add Azure Pipelines agents notes\")</li> <li>Push to GitHub to make available for searching during study sessions</li> </ol> <p>\ud83d\udd0d Searching Notes Efficiently</p> <ul> <li>Used GitHub's search function to quickly find terms (e.g., <code>MendBolt</code>).</li> <li>This allowed me to cross-check answers during study sessions.</li> </ul>","tags":["azure","certification","ai-study-tools","exam-prep"]},{"location":"blog/how-i-used-chatgpt-to-create-az-400-exam-prep-notes-from-mslearn/#example-search-workflow","title":"Example Search Workflow","text":"<ol> <li>Practice question mentions \"Which Azure DevOps deployment strategy minimizes downtime during releases?\"</li> <li>One of the answers mnetions \"Blue/Green\" deployment</li> <li>Search repository for \"Blue/Green\"</li> <li>Results show multiple matching files.</li> <li>Quickly identify that \"Blue/Green deployment\" is the correct answer based on my notes.</li> <li>Verify with documentation reference that Blue/Green deployments maintain two identical production environments, allowing for instant switching between versions.</li> </ol> <p>\ud83d\udcf8 </p>","tags":["azure","certification","ai-study-tools","exam-prep"]},{"location":"blog/how-i-used-chatgpt-to-create-az-400-exam-prep-notes-from-mslearn/#real-world-performance-impact","title":"\ud83d\udd39 Real-World Performance Impact","text":"<p>During practice exams, I could typically locate key information in under 30 seconds using this method, compared to several minutes when using traditional notes or searching documentation directly.</p>","tags":["azure","certification","ai-study-tools","exam-prep"]},{"location":"blog/how-i-used-chatgpt-to-create-az-400-exam-prep-notes-from-mslearn/#results-key-takeaways","title":"\ud83d\udccc Results &amp; Key Takeaways","text":"<p>\ud83c\udfaf Outcome:</p> <ul> <li>\u2705 Passed the AZ-400 exam! \ud83c\udf89</li> <li>\u2705 Created a structured, reusable study guide.</li> <li>\u2705 Used AI efficiently to save time and condense information.</li> <li>\u2705 Reduced overall study time by approximately 40% compared to traditional methods.</li> </ul>","tags":["azure","certification","ai-study-tools","exam-prep"]},{"location":"blog/how-i-used-chatgpt-to-create-az-400-exam-prep-notes-from-mslearn/#time-investment-analysis","title":"\ud83d\udd39 Time Investment Analysis","text":"Activity Traditional Approach AI-Enhanced Approach Time Savings Reading MSLearn 34+ hours 8-10 hours ~70% Note-taking 10-15 hours 5-7 hours ~50% Organization 3-5 hours 2-3 hours ~40% Practice &amp; Review 15-20 hours 15-20 hours 0% Total 62-74 hours 30-40 hours ~45% <p>These figures are based on my own experience and tracking of study time. Your results may vary depending on your familiarity with the subject matter and the tools involved. The key insight is that the most significant time savings came from condensing the initial reading phase while maintaining or even improving knowledge retention through structured notes.</p>","tags":["azure","certification","ai-study-tools","exam-prep"]},{"location":"blog/how-i-used-chatgpt-to-create-az-400-exam-prep-notes-from-mslearn/#what-id-do-differently-next-time","title":"\ud83d\udd39 What I'd Do Differently Next Time","text":"<p>\ud83d\udd39 Further break down the ChatGPT input process into smaller steps \ud83d\udd39 Explore alternative AI tools like Claude or Bard to compare summary quality \ud83d\udd39 Consider automating link collection from MSLearn using a simple web scraper \ud83d\udd39 Create a standardized template for each module type from the beginning \ud83d\udd39 Add more visual elements like diagrams to represent complex relationships  </p>","tags":["azure","certification","ai-study-tools","exam-prep"]},{"location":"blog/how-i-used-chatgpt-to-create-az-400-exam-prep-notes-from-mslearn/#would-i-recommend-this-approach","title":"\ud83d\udd39 Would I Recommend This Approach?","text":"<p>\u2705 Yes\u2014but only if you're prepared for an iterative, hands-on study process.</p> <p>The greatest benefits were:</p> <ol> <li>Structured organization of complex information</li> <li>Quick searchability during practice tests</li> <li>Forced engagement with the material (rather than passive reading)</li> <li>Creation of a reusable resource for future reference</li> </ol>","tags":["azure","certification","ai-study-tools","exam-prep"]},{"location":"blog/how-i-used-chatgpt-to-create-az-400-exam-prep-notes-from-mslearn/#how-to-get-started-yourself","title":"\ud83d\ude80 How to Get Started Yourself","text":"","tags":["azure","certification","ai-study-tools","exam-prep"]},{"location":"blog/how-i-used-chatgpt-to-create-az-400-exam-prep-notes-from-mslearn/#initial-setup-steps","title":"Initial Setup Steps","text":"<p>Before diving into the content summarization, take these important setup steps:</p> <ol> <li>Create your repository structure:</li> <li>Set up a GitHub repository with appropriate folder structure</li> <li>Create a README.md with navigation links</li> <li> <p>Establish a consistent file naming convention</p> </li> <li> <p>Set up a local workflow:</p> </li> <li>Create a folder structure on your computer mirroring your GitHub repo</li> <li>Establish a system for collecting and storing links as you study</li> <li> <p>Set up a template for your ChatGPT prompts</p> </li> <li> <p>Gather your resources:</p> </li> <li>Bookmark all relevant MSLearn collections</li> <li>Organize them according to the exam objectives</li> <li>Create a schedule for working through them systematically</li> </ol>","tags":["azure","certification","ai-study-tools","exam-prep"]},{"location":"blog/how-i-used-chatgpt-to-create-az-400-exam-prep-notes-from-mslearn/#quick-start-template","title":"Quick Start Template","text":"<p>Here's a simplified prompt template to get you started:</p> <pre><code>I'm studying for the [EXAM CODE] certification. Please help me create concise, exam-focused notes for the following module: [MODULE NAME]\n\nFor each key concept, please:\n1. Explain it in 1-2 sentences\n2. Provide a real-world example or scenario\n3. Note any configuration options or limitations\n4. Mention if it's likely to appear on the exam\n\nPlease format your response in Markdown with clear headings and avoid generic \"understand X\" statements.\n\nHere are the links to the module content:\n[PASTE LINKS HERE]\n</code></pre>","tags":["azure","certification","ai-study-tools","exam-prep"]},{"location":"blog/how-i-used-chatgpt-to-create-az-400-exam-prep-notes-from-mslearn/#complete-process","title":"Complete Process","text":"<p>1\ufe0f\u20e3 Set up a GitHub repo for your notes. 2\ufe0f\u20e3 Manually collect MSLearn hyperlinks as you study. 3\ufe0f\u20e3 Use ChatGPT to summarize module-by-module. 4\ufe0f\u20e3 Validate third-party questions with official docs. 5\ufe0f\u20e3 Store and search your notes in GitHub for quick reference.  </p>","tags":["azure","certification","ai-study-tools","exam-prep"]},{"location":"blog/how-i-used-chatgpt-to-create-az-400-exam-prep-notes-from-mslearn/#frequently-asked-questions","title":"\u2753 Frequently Asked Questions","text":"<p>Q: Is this approach considered cheating? A: No. This method enhances learning by actively engaging with the material rather than replacing study. You're creating custom notes by directing AI to extract and organize information you need to know.</p> <p>Q: How much technical knowledge do I need to implement this approach? A: Basic GitHub knowledge and familiarity with markdown formatting are helpful but not essential. The core process can be adapted to use any note-taking system.</p> <p>Q: Does this work for all Microsoft certification exams? A: Yes, this approach works well for any exam with structured MSLearn paths.</p> <p>Q: How do you handle inaccurate information from AI? A: Always verify key technical details against official documentation. When in doubt, trust Microsoft's documentation over AI-generated content.</p> <p>Q: How long did it take you to become proficient with this workflow? A: After completing about 3-4 modules, I had established an efficient workflow. The learning curve is relatively quick if you're already familiar with GitHub and ChatGPT.</p>","tags":["azure","certification","ai-study-tools","exam-prep"]},{"location":"blog/how-i-used-chatgpt-to-create-az-400-exam-prep-notes-from-mslearn/#final-thoughts","title":"\ud83d\udcac Final Thoughts","text":"<p>This method made my exam prep structured and efficient, though it required significant manual effort. If you're preparing for a Microsoft certification, consider trying this approach!</p> <p>The combination of AI-powered summarization, structured GitHub storage, and focused practice testing created a powerful study system that both saved time and improved retention.</p> <p>The most valuable aspect wasn't just passing the exam, but creating a reusable knowledge base that continues to serve as a reference in my professional work. While traditional study methods might help you pass an exam, this approach helps build a lasting resource.</p> <p>\ud83d\udca1 Have you used AI tools for exam prep? Share your thoughts in the comments!</p>","tags":["azure","certification","ai-study-tools","exam-prep"]},{"location":"blog/how-i-used-chatgpt-to-create-az-400-exam-prep-notes-from-mslearn/#additional-resources","title":"\ud83d\udd17 Additional Resources","text":"<ul> <li>My GitHub Repository Link</li> <li>Microsoft Learn AZ-400 Path</li> <li>Official AZ-400 Exam Page</li> <li>GitHub Documentation</li> <li>ChatGPT Documentation</li> </ul> <p>Share on  Share on </p>","tags":["azure","certification","ai-study-tools","exam-prep"]},{"location":"blog/archive/2025/","title":"2025","text":""},{"location":"blog/category/infrastructure/","title":"Infrastructure","text":""},{"location":"blog/category/ai-integration/","title":"AI Integration","text":""},{"location":"blog/category/monitoring/","title":"Monitoring","text":""},{"location":"blog/category/learning-projects/","title":"Learning Projects","text":""},{"location":"blog/category/ai/","title":"AI","text":""},{"location":"blog/category/automation/","title":"Automation","text":""},{"location":"blog/category/self-hosting/","title":"Self-Hosting","text":""},{"location":"blog/category/azure/","title":"Azure","text":""},{"location":"blog/category/patch-management/","title":"Patch Management","text":""},{"location":"blog/category/bcdr/","title":"BCDR","text":""},{"location":"blog/category/cloud-architecture/","title":"Cloud Architecture","text":""},{"location":"blog/category/finops/","title":"FinOps","text":""},{"location":"blog/category/powershell/","title":"PowerShell","text":""},{"location":"blog/category/windows/","title":"Windows","text":""},{"location":"blog/category/group-policy/","title":"Group Policy","text":""},{"location":"blog/category/time-settings/","title":"Time Settings","text":""},{"location":"blog/category/regional-settings/","title":"Regional Settings","text":""},{"location":"blog/category/storage/","title":"Storage","text":""},{"location":"blog/category/identity/","title":"Identity","text":""},{"location":"blog/category/sql/","title":"SQL","text":""},{"location":"blog/category/security/","title":"Security","text":""},{"location":"blog/category/aws/","title":"AWS","text":""},{"location":"blog/category/terraform/","title":"Terraform","text":""},{"location":"blog/category/devops/","title":"DevOps","text":""},{"location":"blog/category/career-development/","title":"Career Development","text":""},{"location":"blog/category/cloud-resume-challenge/","title":"Cloud Resume Challenge","text":""},{"location":"blog/category/cicd/","title":"CI/CD","text":""},{"location":"blog/category/serverless/","title":"Serverless","text":""},{"location":"blog/category/web-hosting/","title":"Web Hosting","text":""},{"location":"blog/category/iis/","title":"IIS","text":""},{"location":"blog/category/application-insights/","title":"Application Insights","text":""},{"location":"blog/category/intune/","title":"Intune","text":""},{"location":"blog/category/scripting/","title":"Scripting","text":""},{"location":"blog/category/microsoft-certifications/","title":"Microsoft Certifications","text":""},{"location":"blog/category/ai--study-techniques/","title":"AI &amp; Study Techniques","text":""},{"location":"blog/category/iac/","title":"IaC","text":""},{"location":"blog/category/gitops/","title":"GitOps","text":""},{"location":"blog/category/mkdocs/","title":"MkDocs","text":""},{"location":"blog/category/customization/","title":"Customization","text":""},{"location":"blog/category/blogging/","title":"Blogging","text":""},{"location":"blog/category/tech/","title":"Tech","text":""},{"location":"blog/category/documentation/","title":"Documentation","text":""},{"location":"blog/category/life-update/","title":"Life Update","text":""},{"location":"blog/page/2/","title":"Index","text":""},{"location":"blog/page/3/","title":"Index","text":""},{"location":"blog/archive/2025/page/2/","title":"2025","text":""},{"location":"blog/archive/2025/page/3/","title":"2025","text":""},{"location":"blog/tags/","title":"Tags","text":""},{"location":"blog/tags/#ai","title":"AI","text":"<ul> <li>Building AI-Powered Infrastructure Monitoring: From Home Lab to Cloud Production</li> </ul>"},{"location":"blog/tags/#azure-openai","title":"Azure OpenAI","text":"<ul> <li>First Steps into AI Automation: My Journey from Trial to Self-Hosted Chaos</li> <li>Building AI-Powered Infrastructure Monitoring: From Home Lab to Cloud Production</li> </ul>"},{"location":"blog/tags/#azure-policy","title":"Azure Policy","text":"<ul> <li>Bringing Patch Management In-House: Migrating from MSP to Azure Update Manager</li> </ul>"},{"location":"blog/tags/#azure-update-manager","title":"Azure Update Manager","text":"<ul> <li>Bringing Patch Management In-House: Migrating from MSP to Azure Update Manager</li> </ul>"},{"location":"blog/tags/#cloud-infrastructure","title":"Cloud Infrastructure","text":"<ul> <li>Building AI-Powered Infrastructure Monitoring: From Home Lab to Cloud Production</li> </ul>"},{"location":"blog/tags/#devops","title":"DevOps","text":"<ul> <li>Building AI-Powered Infrastructure Monitoring: From Home Lab to Cloud Production</li> </ul>"},{"location":"blog/tags/#docker","title":"Docker","text":"<ul> <li>First Steps into AI Automation: My Journey from Trial to Self-Hosted Chaos</li> </ul>"},{"location":"blog/tags/#email-automation","title":"Email Automation","text":"<ul> <li>First Steps into AI Automation: My Journey from Trial to Self-Hosted Chaos</li> </ul>"},{"location":"blog/tags/#finops","title":"FinOps","text":"<ul> <li>Bringing Patch Management In-House: Migrating from MSP to Azure Update Manager</li> </ul>"},{"location":"blog/tags/#grafana","title":"Grafana","text":"<ul> <li>Building AI-Powered Infrastructure Monitoring: From Home Lab to Cloud Production</li> </ul>"},{"location":"blog/tags/#home-lab","title":"Home Lab","text":"<ul> <li>Building AI-Powered Infrastructure Monitoring: From Home Lab to Cloud Production</li> </ul>"},{"location":"blog/tags/#infrastructure-monitoring","title":"Infrastructure Monitoring","text":"<ul> <li>Building AI-Powered Infrastructure Monitoring: From Home Lab to Cloud Production</li> </ul>"},{"location":"blog/tags/#llm","title":"LLM","text":"<ul> <li>First Steps into AI Automation: My Journey from Trial to Self-Hosted Chaos</li> </ul>"},{"location":"blog/tags/#ollama","title":"Ollama","text":"<ul> <li>First Steps into AI Automation: My Journey from Trial to Self-Hosted Chaos</li> </ul>"},{"location":"blog/tags/#openwebui","title":"OpenWebUI","text":"<ul> <li>First Steps into AI Automation: My Journey from Trial to Self-Hosted Chaos</li> </ul>"},{"location":"blog/tags/#powershell","title":"PowerShell","text":"<ul> <li>Bringing Patch Management In-House: Migrating from MSP to Azure Update Manager</li> </ul>"},{"location":"blog/tags/#prometheus","title":"Prometheus","text":"<ul> <li>Building AI-Powered Infrastructure Monitoring: From Home Lab to Cloud Production</li> </ul>"},{"location":"blog/tags/#self-hosting","title":"Self-Hosting","text":"<ul> <li>First Steps into AI Automation: My Journey from Trial to Self-Hosted Chaos</li> </ul>"},{"location":"blog/tags/#vm-patching","title":"VM Patching","text":"<ul> <li>Bringing Patch Management In-House: Migrating from MSP to Azure Update Manager</li> </ul>"},{"location":"blog/tags/#workflow-automation","title":"Workflow Automation","text":"<ul> <li>Building AI-Powered Infrastructure Monitoring: From Home Lab to Cloud Production</li> </ul>"},{"location":"blog/tags/#ai-study-tools","title":"ai-study-tools","text":"<ul> <li>How I used ChatGPT to create AZ-400 exam prep notes from MSLearn</li> </ul>"},{"location":"blog/tags/#api-gateway","title":"api-gateway","text":"<ul> <li>Cloud Resume Challenge with Terraform: Building the Backend API</li> </ul>"},{"location":"blog/tags/#appinsights","title":"appinsights","text":"<ul> <li>Monitoring an IIS-Based Web Farm with Azure Application Insights</li> </ul>"},{"location":"blog/tags/#automation","title":"automation","text":"<ul> <li>Cloud Resume Challenge with Terraform: Automating Deployments with GitHub Actions</li> <li>Uninstalling PaperCut MF Client via Intune \u2013 A Step-by-Step Guide</li> <li>Configuring UK Regional Settings on Windows Servers with PowerShell</li> </ul>"},{"location":"blog/tags/#aws","title":"aws","text":"<ul> <li>Cloud Resume Challenge with Terraform: Final Reflections &amp; Future Directions</li> <li>Cloud Resume Challenge with Terraform: Introduction &amp; Setup</li> </ul>"},{"location":"blog/tags/#azcopy","title":"azcopy","text":"<ul> <li>Replacing SAS Tokens with User Assigned Managed Identity (UAMI) in AzCopy for Blob Uploads</li> </ul>"},{"location":"blog/tags/#azure","title":"azure","text":"<ul> <li>Monitoring an IIS-Based Web Farm with Azure Application Insights</li> <li>Azure BCDR Review \u2013 Turning Inherited Cloud Infrastructure into a Resilient Recovery Strategy</li> <li>Azure BCDR \u2013 How I Turned a DR Review into a Strategic Recovery Plan</li> <li>Replacing SQL Credentials with User Assigned Managed Identity (UAMI) in Azure SQL Managed Instance</li> <li>Replacing SAS Tokens with User Assigned Managed Identity (UAMI) in AzCopy for Blob Uploads</li> <li>How I used ChatGPT to create AZ-400 exam prep notes from MSLearn</li> </ul>"},{"location":"blog/tags/#azure-automation","title":"azure automation","text":"<ul> <li>Saving Azure Costs with Scheduled VM Start/Stop using Custom Azure Automation Runbooks</li> </ul>"},{"location":"blog/tags/#azure-vm","title":"azure vm","text":"<ul> <li>Saving Azure Costs with Scheduled VM Start/Stop using Custom Azure Automation Runbooks</li> <li>Enforcing Time Zone and DST Compliance on Windows Servers Using GPO and Scheduled Tasks</li> </ul>"},{"location":"blog/tags/#bcdr","title":"bcdr","text":"<ul> <li>Azure BCDR Review \u2013 Turning Inherited Cloud Infrastructure into a Resilient Recovery Strategy</li> <li>Azure BCDR \u2013 How I Turned a DR Review into a Strategic Recovery Plan</li> </ul>"},{"location":"blog/tags/#blob-storage","title":"blob storage","text":"<ul> <li>Replacing SAS Tokens with User Assigned Managed Identity (UAMI) in AzCopy for Blob Uploads</li> </ul>"},{"location":"blog/tags/#career","title":"career","text":"<ul> <li>Cloud Resume Challenge with Terraform: Final Reflections &amp; Future Directions</li> </ul>"},{"location":"blog/tags/#certification","title":"certification","text":"<ul> <li>How I used ChatGPT to create AZ-400 exam prep notes from MSLearn</li> </ul>"},{"location":"blog/tags/#cloud","title":"cloud","text":"<ul> <li>Cloud Resume Challenge with Terraform: Introduction &amp; Setup</li> <li>Cloud Resume Challenge with Terraform: Automating Deployments with GitHub Actions</li> </ul>"},{"location":"blog/tags/#cloud-strategy","title":"cloud strategy","text":"<ul> <li>Azure BCDR Review \u2013 Turning Inherited Cloud Infrastructure into a Resilient Recovery Strategy</li> </ul>"},{"location":"blog/tags/#cloud-resume-challenge","title":"cloud-resume-challenge","text":"<ul> <li>Cloud Resume Challenge with Terraform: Final Reflections &amp; Future Directions</li> </ul>"},{"location":"blog/tags/#cloudfront","title":"cloudfront","text":"<ul> <li>Cloud Resume Challenge with Terraform: Deploying the Static Website</li> </ul>"},{"location":"blog/tags/#cost-optimisation","title":"cost optimisation","text":"<ul> <li>Saving Azure Costs with Scheduled VM Start/Stop using Custom Azure Automation Runbooks</li> </ul>"},{"location":"blog/tags/#customization","title":"customization","text":"<ul> <li>Enhancing My MkDocs Blog with Custom Features</li> </ul>"},{"location":"blog/tags/#devops_1","title":"devops","text":"<ul> <li>Cloud Resume Challenge with Terraform: Final Reflections &amp; Future Directions</li> <li>Cloud Resume Challenge with Terraform: Automating Deployments with GitHub Actions</li> </ul>"},{"location":"blog/tags/#disaster-recovery","title":"disaster recovery","text":"<ul> <li>Azure BCDR Review \u2013 Turning Inherited Cloud Infrastructure into a Resilient Recovery Strategy</li> <li>Azure BCDR \u2013 How I Turned a DR Review into a Strategic Recovery Plan</li> </ul>"},{"location":"blog/tags/#dst","title":"dst","text":"<ul> <li>Enforcing Time Zone and DST Compliance on Windows Servers Using GPO and Scheduled Tasks</li> </ul>"},{"location":"blog/tags/#dynamodb","title":"dynamodb","text":"<ul> <li>Cloud Resume Challenge with Terraform: Building the Backend API</li> </ul>"},{"location":"blog/tags/#exam-prep","title":"exam-prep","text":"<ul> <li>How I used ChatGPT to create AZ-400 exam prep notes from MSLearn</li> </ul>"},{"location":"blog/tags/#finops_1","title":"finops","text":"<ul> <li>Saving Azure Costs with Scheduled VM Start/Stop using Custom Azure Automation Runbooks</li> </ul>"},{"location":"blog/tags/#github","title":"github","text":"<ul> <li>GitOps Project: Automating AWS infrastructure deployment with Terraform and GitHub Actions</li> </ul>"},{"location":"blog/tags/#github-actions","title":"github-actions","text":"<ul> <li>Cloud Resume Challenge with Terraform: Automating Deployments with GitHub Actions</li> </ul>"},{"location":"blog/tags/#gpo","title":"gpo","text":"<ul> <li>Enforcing Time Zone and DST Compliance on Windows Servers Using GPO and Scheduled Tasks</li> </ul>"},{"location":"blog/tags/#iac","title":"iac","text":"<ul> <li>GitOps Project: Automating AWS infrastructure deployment with Terraform and GitHub Actions</li> </ul>"},{"location":"blog/tags/#identity","title":"identity","text":"<ul> <li>Replacing SQL Credentials with User Assigned Managed Identity (UAMI) in Azure SQL Managed Instance</li> <li>Replacing SAS Tokens with User Assigned Managed Identity (UAMI) in AzCopy for Blob Uploads</li> </ul>"},{"location":"blog/tags/#iis","title":"iis","text":"<ul> <li>Monitoring an IIS-Based Web Farm with Azure Application Insights</li> </ul>"},{"location":"blog/tags/#infrastructure-as-code","title":"infrastructure-as-code","text":"<ul> <li>Cloud Resume Challenge with Terraform: Final Reflections &amp; Future Directions</li> <li>Cloud Resume Challenge with Terraform: Introduction &amp; Setup</li> </ul>"},{"location":"blog/tags/#intune","title":"intune","text":"<ul> <li>Uninstalling PaperCut MF Client via Intune \u2013 A Step-by-Step Guide</li> </ul>"},{"location":"blog/tags/#lambda","title":"lambda","text":"<ul> <li>Cloud Resume Challenge with Terraform: Building the Backend API</li> </ul>"},{"location":"blog/tags/#learning","title":"learning","text":"<ul> <li>Welcome to My Blog!</li> <li>Customizing Your MkDocs Blog</li> <li>Setting Up MkDocs for Your Blog</li> </ul>"},{"location":"blog/tags/#life","title":"life","text":"<ul> <li>Life Update: Projects, Learning, and the Usual Balancing Act</li> </ul>"},{"location":"blog/tags/#locale","title":"locale","text":"<ul> <li>Configuring UK Regional Settings on Windows Servers with PowerShell</li> </ul>"},{"location":"blog/tags/#managed-identity","title":"managed identity","text":"<ul> <li>Replacing SQL Credentials with User Assigned Managed Identity (UAMI) in Azure SQL Managed Instance</li> </ul>"},{"location":"blog/tags/#material-theme","title":"material-theme","text":"<ul> <li>Enhancing My MkDocs Blog with Custom Features</li> </ul>"},{"location":"blog/tags/#mkdocs","title":"mkdocs","text":"<ul> <li>Enhancing My MkDocs Blog with Custom Features</li> </ul>"},{"location":"blog/tags/#monitoring","title":"monitoring","text":"<ul> <li>Monitoring an IIS-Based Web Farm with Azure Application Insights</li> </ul>"},{"location":"blog/tags/#n8n","title":"n8n","text":"<ul> <li>First Steps into AI Automation: My Journey from Trial to Self-Hosted Chaos</li> <li>Building AI-Powered Infrastructure Monitoring: From Home Lab to Cloud Production</li> </ul>"},{"location":"blog/tags/#powershell_1","title":"powershell","text":"<ul> <li>Saving Azure Costs with Scheduled VM Start/Stop using Custom Azure Automation Runbooks</li> <li>Replacing SAS Tokens with User Assigned Managed Identity (UAMI) in AzCopy for Blob Uploads</li> <li>Uninstalling PaperCut MF Client via Intune \u2013 A Step-by-Step Guide</li> <li>Configuring UK Regional Settings on Windows Servers with PowerShell</li> </ul>"},{"location":"blog/tags/#regional-settings","title":"regional settings","text":"<ul> <li>Configuring UK Regional Settings on Windows Servers with PowerShell</li> </ul>"},{"location":"blog/tags/#resume-challenge","title":"resume-challenge","text":"<ul> <li>Cloud Resume Challenge with Terraform: Introduction &amp; Setup</li> </ul>"},{"location":"blog/tags/#route53","title":"route53","text":"<ul> <li>Cloud Resume Challenge with Terraform: Deploying the Static Website</li> </ul>"},{"location":"blog/tags/#s3","title":"s3","text":"<ul> <li>Cloud Resume Challenge with Terraform: Deploying the Static Website</li> </ul>"},{"location":"blog/tags/#sas","title":"sas","text":"<ul> <li>Replacing SAS Tokens with User Assigned Managed Identity (UAMI) in AzCopy for Blob Uploads</li> </ul>"},{"location":"blog/tags/#scheduled-task","title":"scheduled task","text":"<ul> <li>Enforcing Time Zone and DST Compliance on Windows Servers Using GPO and Scheduled Tasks</li> </ul>"},{"location":"blog/tags/#security","title":"security","text":"<ul> <li>Replacing SQL Credentials with User Assigned Managed Identity (UAMI) in Azure SQL Managed Instance</li> </ul>"},{"location":"blog/tags/#serverless","title":"serverless","text":"<ul> <li>Cloud Resume Challenge with Terraform: Building the Backend API</li> </ul>"},{"location":"blog/tags/#site-recovery","title":"site recovery","text":"<ul> <li>Azure BCDR Review \u2013 Turning Inherited Cloud Infrastructure into a Resilient Recovery Strategy</li> <li>Azure BCDR \u2013 How I Turned a DR Review into a Strategic Recovery Plan</li> </ul>"},{"location":"blog/tags/#sql","title":"sql","text":"<ul> <li>Replacing SQL Credentials with User Assigned Managed Identity (UAMI) in Azure SQL Managed Instance</li> </ul>"},{"location":"blog/tags/#strategy","title":"strategy","text":"<ul> <li>Azure BCDR \u2013 How I Turned a DR Review into a Strategic Recovery Plan</li> </ul>"},{"location":"blog/tags/#technology","title":"technology","text":"<ul> <li>Welcome to My Blog!</li> <li>Customizing Your MkDocs Blog</li> <li>Setting Up MkDocs for Your Blog</li> <li>GitOps Project: Automating AWS infrastructure deployment with Terraform and GitHub Actions</li> <li>Life Update: Projects, Learning, and the Usual Balancing Act</li> </ul>"},{"location":"blog/tags/#terraform","title":"terraform","text":"<ul> <li>Cloud Resume Challenge with Terraform: Final Reflections &amp; Future Directions</li> <li>Cloud Resume Challenge with Terraform: Introduction &amp; Setup</li> <li>Cloud Resume Challenge with Terraform: Deploying the Static Website</li> <li>Cloud Resume Challenge with Terraform: Building the Backend API</li> <li>Cloud Resume Challenge with Terraform: Automating Deployments with GitHub Actions</li> <li>GitOps Project: Automating AWS infrastructure deployment with Terraform and GitHub Actions</li> </ul>"},{"location":"blog/tags/#timezone","title":"timezone","text":"<ul> <li>Enforcing Time Zone and DST Compliance on Windows Servers Using GPO and Scheduled Tasks</li> </ul>"},{"location":"blog/tags/#tls","title":"tls","text":"<ul> <li>Cloud Resume Challenge with Terraform: Deploying the Static Website</li> </ul>"},{"location":"blog/tags/#uami","title":"uami","text":"<ul> <li>Replacing SQL Credentials with User Assigned Managed Identity (UAMI) in Azure SQL Managed Instance</li> <li>Replacing SAS Tokens with User Assigned Managed Identity (UAMI) in AzCopy for Blob Uploads</li> </ul>"},{"location":"blog/tags/#uk","title":"uk","text":"<ul> <li>Configuring UK Regional Settings on Windows Servers with PowerShell</li> </ul>"},{"location":"blog/tags/#vm-schedule","title":"vm schedule","text":"<ul> <li>Saving Azure Costs with Scheduled VM Start/Stop using Custom Azure Automation Runbooks</li> </ul>"},{"location":"blog/tags/#web-development","title":"web-development","text":"<ul> <li>Enhancing My MkDocs Blog with Custom Features</li> </ul>"},{"location":"blog/tags/#website-hosting","title":"website-hosting","text":"<ul> <li>Cloud Resume Challenge with Terraform: Deploying the Static Website</li> </ul>"},{"location":"blog/tags/#windows-server","title":"windows server","text":"<ul> <li>Enforcing Time Zone and DST Compliance on Windows Servers Using GPO and Scheduled Tasks</li> <li>Configuring UK Regional Settings on Windows Servers with PowerShell</li> </ul>"}]}