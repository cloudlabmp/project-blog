---
title: "Vibe Coding My Way to AI Infra"
description: "Terraform, Claude & Cloud-Native Monitoring"
author: "Matt Pollock"
date:
  created: 2025-07-18
  updated: 2025-07-21
tags:
  - AI
  - Infrastructure as Code
  - Terraform
  - Azure OpenAI
  - GenAI
  - Cloud Infrastructure
  - DevOps
  - Vibe Coding
  - AI Integration
  - Container Apps
  - Azure Functions
  - AI Slop-Ops
  - Slop-Ops
categories:
  - Infrastructure
  - AI Integration
  - Cloud Engineering
  - Learning Projects
reading_time: "20 min"
---

# ðŸ”¥ Vibe Coding My Way to AI Connected Infra: Claude, Terraform & Cloud-Native Monitoring

## ðŸ“– TL;DR â€“ What This Post Covers

- How I used AI tools to build an Azure-based monitoring solution from scratch  
- Lessons learned from developing two full versions (manual vs. Terraform)  
- The good, bad, and wandering of GenAI for infrastructure engineers  
- A working, cost-effective, and fully redeployable AI monitoring stack

## Introduction

This project began, as many of mine do, with a career planning conversation. During a discussion with ChatGPT about professional development and emerging skill areas for 2025, one suggestion stuck with me:

> "You should become an Infrastructure AI Integration Engineer."

Itâ€™s a role that doesnâ€™t really exist yet â€” but probably should.

What followed was a journey to explore whether such a role could be real. I set out to build an AI-powered infrastructure monitoring solution in Azure, without any formal development background and using nothing but conversations with Claude. This wasnâ€™t just about building something cool â€” it was about testing whether a seasoned infra engineer could:

- Use GenAI to design and deploy a full solution
- Embrace the unknown and lean into the chaos of LLM-based workflows
- Create something reusable, repeatable, and useful

The first phase of the journey was a local prototype using my Pi5 and n8n for AI workflow automation (see my previous post for that). It worked â€” but it was local, limited, and not exactly enterprise-ready.

So began the cloud migration.

### Why this project mattered

I had two goals:

- âœ… Prove that â€œvibe codingâ€ â€” using GenAI with limited pre-planning â€” could produce something deployable
- âœ… Build a portfolio project for the emerging intersection of AI and infrastructure engineering

This isnâ€™t a tutorial on AI monitoring. Instead, itâ€™s a behind-the-scenes look at what happens when you try to:

- Build something real using AI chat alone
- Translate a messy, manual deployment into clean Infrastructure as Code
- Learn *with* the AI, not just from it

The Terraform modules prove it works.  
The chat logs show how we got there.  
The dashboard screenshots demonstrate the outcome.

> *The next sections cover the journey in two parts: first, the vibe-coded v1; then the Terraform-powered refactor in v2.*

---

## ðŸ“š Table of Contents

- [Introduction](#introduction)
- [ðŸ“š Table of Contents](#table-of-contents)
- [Part 1: The Prototype](#part-1-the-prototype)
- [Version 1: The Manual Deployment Marathon](#version-1-the-manual-deployment-marathon)
- [Platform and GenAI Choices](#platform-and-genai-choices)
- [ðŸ§± Phase 1: Foundation](#phase-1-foundation)
- [ðŸ§  Phase 2: Intelligence Layer](#phase-2-intelligence-layer)
- [ðŸŽ¨ Phase 3: The User Experience](#phase-3-the-user-experience)
- [ðŸŒ Part 2: Why Terraform? Why Now?](#part-2-why-terraform-why-now)
- [ðŸ§  Part 3: Working with GenAI â€“ The Good, the Bad, and the Wandering](#part-3-working-with-genai-the-good-the-bad-and-the-wandering)
- [ðŸ—ï¸ Part 4: Building the Stack â€“ What Got Built](#part-4-building-the-stack-what-got-built)
- [ðŸ§¾ Part 5: The Result - A Portable, Reusable AI Monitoring Stack](#part-5-the-result-a-portable-reusable-ai-monitoring-stack)
- [ðŸ§  Part 6: Reflections and Lessons Learned](#part-6-reflections-and-lessons-learned)
- [âœ… Conclusion](#conclusion)

## Part 1: The Prototype

*(Stage 1 â€“ Manual AI-Assisted Deployment)* The Birth of a Vibe-Coded Project

The project didnâ€™t start with a business requirement â€” it started with curiosity. One evening, mid-career reflection turned into a late-night conversation with ChatGPT:

> "You should become an Infrastructure AI Integration Engineer."

Iâ€™d never heard the term, but it sparked something. With 20+ years in IT infrastructure and the growing presence of AI in tooling, it felt like a direction worth exploring.

### The Thought Experiment

Could I â€” an infrastructure engineer, not a dev â€” build an AI-driven cloud monitoring solution:

- End-to-end, using only AI assistance
- Without dictating the architecture
- With minimal manual planning

The rules were simple:

- âŒ No specifying what resources to use
- âŒ No formal design documents
- âœ… Just tell the AI the *outcome* I wanted, and let it choose the path

The result: pure "vibe coding." Or as I now call it: **AI Slop-Ops**.

### What is Vibe Coding (a.k.a. Slop-Ops)?

For this project, "vibe coding" meant:

- ðŸ¤– Generating all infrastructure and app code using natural language prompts
- ðŸ§  Letting Claude decide how to structure everything
- ðŸªµ Learning through experimentation and iteration

> My starting prompt was something like:  
> *"I want to build an AI monitoring solution in Azure that uses Azure OpenAI to analyze infrastructure metrics."*

Claude replied:
> "Letâ€™s start with a simple architecture: Azure Container Apps for the frontend, Azure Functions for the AI processing, and Azure OpenAI for the intelligence. We'll build it in phases."

That one sentence kicked off a 4â€“5 week journey involving:

- ~40â€“50 hours of evening and weekend effort ðŸ§µ
- Dozens of chats, scripts, and browser tabs
- An unpredictable mix of brilliance and bafflement

And the whole thing started to work.

---

## Version 1: The Manual Deployment Marathon

The first build was fully manual â€” a mix of PowerShell scripts, Azure portal clicks, and Claude-prompting marathons. Claude suggested a phased approach, which turned out to be the only way to keep it manageable.

ðŸ’¬ *Claude liked PowerShell. I honestly canâ€™t remember if that was my idea or if I just went along with it.* ðŸ¤·â€â™‚ï¸

---

## Platform and GenAI Choices

### ðŸŒ Why Azure?

The platform decision was pragmatic:

- I already had a [Visual Studio Developer Subscription](https://learn.microsoft.com/en-us/visualstudio/subscriptions/) with Â£120/month of Azure credits.
- Azure is the cloud provider I work with day-to-day, so it made sense to double down.
- Using Azure OpenAI gave me hands-on experience with **Azure AI Foundry** â€“ increasingly relevant in modern infrastructure roles.

In short: low cost, high familiarity, and useful upskilling.

---

### ðŸ§  Why Claude?

This project was built almost entirely through chat with **Claude**, Anthropicâ€™s conversational AI. Iâ€™ve found:

âœ… **Claude is better at structured technical responses**, particularly with IaC and shell scripting.  
âŒ **ChatGPT tends to hallucinate more often** in my experience when writing infrastructure code.

But Claude had its own quirks too:

- No memory between chats â€” every session required reloading context.
- Occasional focus issues â€” drifting from task or overcomplicating simple requests.
- Tendency to suggest hardcoded values when debugging â€” needing constant vigilance to maintain DRY principles.

> âš ï¸ Reality check: Claude isn't a Terraform expert. It's a language model that *guesses well* based on input. The human still needs to guide architecture, validate outputs, and ensure everything actually works.

---

### ðŸ¤– Prompt Engineering Principles

I used a consistent framework to keep Claude focused and productive:

- **ROLE**: Define Claude's purpose (e.g., â€œYou are a Terraform expertâ€)
- **INPUT**: What files or context is provided
- **OUTPUT**: What should Claude return (e.g., a module, refactored block, explanation)
- **CONSTRAINTS**: e.g., â€œNo hardcoded valuesâ€, â€œUse locals not repeated variablesâ€
- **TASK**: Specific action or generation requested
- **REMINDERS**: Extra nudges â€” â€œUse commentsâ€, â€œOutput in markdownâ€, â€œUse Azure CLI not PowerShellâ€

> This approach reduced misunderstandings and helped prevent â€œsolution driftâ€ during long iterative sessions.

---

## ðŸ§± Phase 1: Foundation

This first phase set up the core infrastructure that everything else would build upon.

### ðŸ”§ What Got Built

- **Resource Groups** â€“ Logical container for resources  
- **Storage Accounts** â€“ Persistent storage for logs, state, and AI interaction data  
- **Log Analytics Workspace** â€“ Centralized logging for observability  
- **Application Insights** â€“ Telemetry and performance monitoring for apps

These services created the backbone of the environment, enabling both operational and analytical insight.

---

### ðŸ–¥ï¸ PowerShell Verification Script

This example script was used during v1 to manually verify deployment success:

```powershell
# Verify everything is working
Write-Host "ðŸ” Verifying Step 1.1 completion..." -ForegroundColor Yellow

# Check resource group
$rg = Get-AzResourceGroup -Name "rg-ai-monitoring-dev" -ErrorAction SilentlyContinue
if ($rg) {
    Write-Host "âœ… Resource Group exists" -ForegroundColor Green
} else {
    Write-Host "âŒ Resource Group not found" -ForegroundColor Red
}

# Check workspace
$ws = Get-AzOperationalInsightsWorkspace -ResourceGroupName "rg-ai-monitoring-dev" -Name "law-ai-monitoring-dev" -ErrorAction SilentlyContinue
if ($ws -and $ws.ProvisioningState -eq "Succeeded") {
    Write-Host "âœ… Log Analytics Workspace is ready" -ForegroundColor Green
} else {
    Write-Host "âŒ Log Analytics Workspace not ready. State: $($ws.ProvisioningState)" -ForegroundColor Red
}

# Check config file
if (Test-Path ".\phase1-step1-config.json") {
    Write-Host "âœ… Configuration file created" -ForegroundColor Green
} else {
    Write-Host "âŒ Configuration file missing" -ForegroundColor Red
}
```

## ðŸ§  Phase 2: Intelligence Layer

With the foundation in place, the next step was to add the brainpower â€” the AI and automation components that turn infrastructure data into actionable insights.

### ðŸ§© Key Components

- **Azure OpenAI Service**
  - Deployed with `gpt-4o-mini` to balance cost and performance
  - Powers the natural language analysis and recommendation engine

- **Azure Function App**
  - Hosts the core AI processing logic
  - Parses data from monitoring tools and feeds it to OpenAI
  - Returns interpreted insights in a format suitable for dashboards and alerts

- **Logic Apps**
  - Automates data ingestion and flow between services
  - Orchestrates the processing of logs, telemetry, and alert conditions
  - Acts as glue between Function Apps, OpenAI, and supporting services

---

### ðŸ—£ï¸ AI Integration Philosophy

This stage wasnâ€™t about building complex AI logic â€” it was about using OpenAI to interpret patterns in infrastructure data and return intelligent summaries or recommendations in natural language.

> Example prompt fed to OpenAI from within a Function App:
>
> â€œBased on this log stream, are there any signs of service degradation or performance issues in the last 15 minutes?â€

The response would be embedded in a monitoring dashboard or sent via alert workflows, giving human-readable insights without manual interpretation.

---

### âš™ï¸ Why This Setup?

Each component in this layer was chosen for a specific reason:

- **OpenAI** for flexible, contextual intelligence  
- **Function Apps** for scalable, event-driven execution  
- **Logic Apps** for orchestration without writing custom backend code  

This approach removed the need for always-on VMs or bespoke integrations â€” and kept things lean.

---

ðŸ“Œ *By the end of Phase 2, the system had a functioning AI backend that could interpret infrastructure metrics in plain English and respond in near real-time.*

## ðŸŽ¨ Phase 3: The User Experience

With the core infrastructure and AI processing in place, it was time to build the frontend â€” the visible interface for users to interact with the AI-powered monitoring system.

This phase focused on deploying a set of containerized applications, each responsible for a specific role in the monitoring workflow.

---

### ðŸ§± Components Deployed

The solution was built around **Azure Container Apps**, with a four-container ecosystem designed to work in harmony:

- **FastAPI Backend**  
  Handles API requests, routes data to the correct services, and acts as the core orchestrator behind the scenes.

- **React Dashboard**  
  A clean, responsive frontend displaying infrastructure metrics, system health, and AI-generated insights.

- **Background Processor**  
  Continuously monitors incoming data and triggers AI evaluations when certain thresholds or patterns are detected.

- **Load Generator**  
  Provides synthetic traffic and test metrics to simulate real usage patterns and help validate system behavior.

---

### ðŸ”„ Why This Architecture?

Each container serves a focused purpose, allowing for:

- **Isolation of concerns** â€” easier debugging and development
- **Scalable deployment** â€” each component scales independently
- **Separation of UI and logic** â€” keeping the AI and logic layers decoupled from the frontend

> â€œClaude recommended this separation early on â€” the decision to use Container Apps instead of AKS or App Services kept costs down and complexity low, while still providing a modern cloud-native experience.â€

---

### âš™ï¸ Deployment Highlights

Container Apps were provisioned via CLI in the manual version, and later through Terraform in v2. The deployment process involved:

- Registering a **Container Apps Environment**
- Creating the four separate app containers
- Passing environment variables for API endpoints, keys, and settings
- Enabling diagnostics and logging via Application Insights

```bash
az containerapp create \
  --name react-dashboard \
  --image myregistry.azurecr.io/dashboard:latest \
  --env-vars REACT_APP_API_URL=https://api.example.com
```

### ðŸ“Š Final Result

Once deployed, the user-facing layer provided:

- ðŸ” **Real-time visual metrics**
- ðŸ’¡ **AI-generated recommendations**
- ðŸ§  **Interactive analysis via chat**
- ðŸ“ˆ **Infrastructure performance summaries**
- ðŸ’¬ **Stakeholder-friendly reporting**

This phase brought the system to life â€” from backend AI logic to a polished, interactive dashboard.

### ðŸ¤– The Reality of AI-Assisted Development

Here's what the success story doesnâ€™t capture: the relentless battles with Claudeâ€™s limitations.

Despite its capabilities, working with GenAI in a complex, multi-phase project revealed real friction points â€” especially when continuity and context were critical.

#### ðŸ˜« Daily Frustrations Included

- ðŸ§± **Hitting chat length limits daily** â€” even with Claude Pro
- ðŸ§­ **AI meandering off-topic**, despite carefully structured prompts
- ðŸ“š **Over-analysis** â€” asking for one thing and receiving a detailed architectural breakdown
- âš™ï¸ **Token burn during troubleshooting** â€” Claude often provided five-step fixes when a one-liner was needed
- âŒ **No persistent memory or project history**
  - This meant manually copy/pasting prior chats into a `.txt` file just to refeed them back in
- ðŸ” **Starting new chats daily** â€” and re-establishing context from scratch every time
- ðŸ“ **Scope creep** â€” Claude regularly expanded simple requests into full architectural reviews without being asked

Despite these pain points, the experience was still a net positive â€” but only because I was prepared to steer the conversation firmly and frequently.
![Chat length limit warning](./Claude_ChatLengthLImit.png)

#### ðŸ§ª From Real-World Troubleshooting

Sometimes, working with Claude felt like pair programming with a colleague who had perfect recall â€” until they completely wiped their memory overnight.

> ðŸ§µ **From an actual troubleshooting session:**
>
> â€œThe dashboard is calling the wrong function URL again.  
> Itâ€™s trying to reach `func-tf-ai-monitoring-dev-ai`,  
> but the actual function is at `func-ai-monitoring-dev-ask6868-ai`.â€

It was a recurring theme: great memory during a session, zero continuity the next day.

> **Me:** â€œRight, shall we pick up where we left off yesterday then?â€  
> **Claude:** â€œI literally have no idea what you're talking about, mate.â€  
> **Claude:** â€œWait, who are you again?â€

Every failure taught *both* me and Claude something â€” but the learning curve was steep, and the iteration cycles could be genuinely exhausting.

### Version 1 - Deployed & Working

![AI Monitoring Dashboard V1](./AIMonitoringDashboardV1.png)

### ðŸ§  What I Learned from Part 1

Reflecting on the first phase of this project â€” the manual, vibe-coded deployment â€” several key takeaways emerged.

#### âœ… What Worked Well

- âš¡ **Rapid prototyping** â€” quickly turned ideas into functioning infrastructure
- ðŸ’¬ **Natural language problem-solving** â€” great for tackling Azureâ€™s complex service interactions
- ðŸ§¾ **Syntactically sound code generation** â€” most outputs worked with minimal tweaks
- â±ï¸ **Massive time savings** â€” tasks that might take days manually were completed in hours

#### ðŸ” What Needed Constant Oversight

- ðŸ§  **Keeping the AI focused** â€” drift and distraction were constant threats
- ðŸ”— **Managing dependencies and naming** â€” conflicts and collisions needed manual intervention
- ðŸ› **Debugging runtime issues** â€” particularly frustrating when errors only manifested in Azure
- ðŸ§­ **Architectural decisions** â€” strategic direction still had to come from me
- âš ï¸ **Knowing when â€œit worksâ€ wasnâ€™t â€œproduction-readyâ€** â€” validation remained a human job

#### ðŸ› ï¸ Language & Tooling Choices

Interestingly, **Claude dictated the stack** more than I did.

- **Version 1** leaned heavily on **PowerShell**
- **Version 2** shifted to **Azure CLI and Bash**

Despite years of experience with PowerShell, I found Claude was significantly more confident (and accurate) when generating Azure CLI or Bash-based commands. This influenced the eventual choice to move away from PowerShell in the second iteration.

---

By the end of Part 1, I had a functional AI monitoring solution â€” but it was fragile, inconsistent, and impossible to redeploy without repeating all the manual steps.

That realisation led directly to **Version 2** â€” a full rebuild using Infrastructure as Code.

---

## ðŸŒ Part 2: Why Terraform? Why Now?

After several weeks of manual deployments, the **limitations of version 1** became unmissable.

Yes â€” the system *worked* â€” but only just:

- Scripts were fragmented and inconsistent  
- Fixes required custom, ad-hoc scripts created on the fly  
- Dependencies werenâ€™t tracked, and naming conflicts crept in  
- Reproducibility? Practically zero  

ðŸš¨ The deployment process had become **unwieldy** â€” a sprawl of folders, partial fixes, and manual interventions. Functional? Sure. Maintainable? Absolutely not.

---

Thatâ€™s when the **Infrastructure as Code (IaC)** mindset kicked in.

> *â€œAnything worth building once is worth building repeatably.â€*

The question was simple:  
ðŸ’¡ *Could I rebuild everything from scratch â€” but this time, using AI assistance to create clean, modular, production-ready Terraform code?*

---

### ðŸ§± The Terraform Challenge

Rebuilding in Terraform wasnâ€™t just a choice of tooling â€” it was a **challenge to see how far AI-assisted development could go** when held to production-level standards.

#### ðŸŽ¯ Goals of the Terraform Rewrite

- **Modularity**  
  Break down the monolithic structure into **reusable, isolated modules**  
- **Portability**  
  Enable consistent deployment **across environments and subscriptions**
- **DRY Principles**  
  Absolutely **no hardcoded values** or duplicate code
- **Documentation**  
  Ensure the code was **clear, self-documenting**, and reusable by others

---

Terraform wasnâ€™t just a tech choice â€” it became the **refinement phase**.  
A chance to take what Iâ€™d learned from the vibe-coded version and bake that insight into clean, structured infrastructure-as-code.

Next: how AI and I tackled that rebuild, and the (sometimes surprising) choices we made.

### ðŸ§  The Structured Prompt Approach

The **prompt engineering** approach became absolutely crucial during the Terraform refactoring phase.

Rather than relying on vague questions or â€œdo what I meanâ€ instructions, I adopted a structured briefing style â€” the kind you might use when assigning work to a consultant:

- Define the **role**
- Set the **goals**
- Describe the **inputs**
- Outline the **method**
- Impose **constraints**

---

Hereâ€™s the **actual instruction prompt** I used to initiate the Terraform rebuild ðŸ‘‡

```bash
ðŸ”§ Enhanced Prompt: AI Monitoring Solution IaC Refactoring Project

ðŸ‘¤ Role Definition
You are acting as:
â€¢ An Infrastructure as Code (IaC) specialist with deep expertise in Terraform
â€¢ An AI integration engineer, experienced in deploying Azure-based AI workloads

Your responsibilities are:
â€¢ To refactor an existing AI Monitoring solution from a manually built prototype 
  into a modular, efficient, and portable Terraform project
â€¢ To minimize bloat, ensure code reusability, and produce clear documentation 
  to allow redeployment with minimal changes

ðŸŽ¯ Project Goals
â€¢ Rebuild the existing AI Monitoring solution as a fully modular, DRY-compliant 
  Terraform deployment
â€¢ Modularize resources (OpenAI, Function Apps, Logic Apps, Container Apps) 
  into reusable components
â€¢ Provide clear, concise README.md files for each module describing usage, 
  input/output variables, and deployment steps

ðŸ“ Project Artifacts (Input)
The following components are part of the original Azure-hosted AI Monitoring solution:
â€¢ Azure OpenAI service
â€¢ Azure Function App
â€¢ Logic App
â€¢ Web Dashboard
â€¢ Container Apps Environment
â€¢ Supporting components (Key Vaults, App Insights, Storage, etc.)

ðŸ› ï¸ Approach / Methodology
For each module:
â€¢ Use minimal but complete resource blocks
â€¢ Include only essential variables with sensible defaults
â€¢ Use output values to export key resource properties
â€¢ Follow DRY principles using locals or reusable variables where possible

ðŸ“Œ Additional Guidelines
â€¢ Efficiency first: Avoid code repetition; prefer reusability, locals, and input variables
â€¢ Practical defaults: Pre-fill variables with production-safe, but general-purpose values
â€¢ Keep it modular: No monolithic deployment blocksâ€”use modules for all core resources
â€¢ Strict adherence: Do not expand scope unless confirmed
```

This structured approach helped maintain focus and provided clear boundaries for the AI to work within â€” though, as you'll see, **constant reinforcement was still required** throughout the process.

---

### ðŸ”„ The Refactoring Process

The Terraform rebuild became a **different kind of AI collaboration**.

Where version 1 was about vibing ideas into reality, version 2 was about **methodically translating a messy prototype into clean, modular, production-friendly code**.

---

#### ðŸ§© Key Modules Created

- `foundation`  
  Core infrastructure â€” resource groups, storage accounts, logging, etc.
  
- `openai`  
  Azure OpenAI resource and model deployment â€” central to the intelligent analysis pipeline

- `function-app`  
  Azure Functions for AI processing â€” connecting telemetry with insights

- `container-apps`  
  Four-container ecosystem â€” the user-facing UI and visualization layers

- `monitoring`  
  Application Insights + alerting â€” keeping the system observable and maintainable

---

#### ðŸ“ Modular Structure Overview

```bash
terraform-ai-monitoring/
â”œâ”€â”€ modules/
â”‚   â”œâ”€â”€ foundation/
â”‚   â”œâ”€â”€ openai/
â”‚   â”œâ”€â”€ function-app/
â”‚   â””â”€â”€ container-apps/
â”œâ”€â”€ main.tf
â””â”€â”€ terraform.tfvars
```

Each module went through **multiple refinement cycles**. The goal wasnâ€™t just to get it working â€” it was to ensure:

- Clean, reusable Terraform code
- Explicit configuration
- DRY principles throughout
- Reproducible, idempotent deployments

---

#### ðŸ”§ Iterative Refinement in Practice

A typical troubleshooting session went something like this:

- Iâ€™d run the code or attempt a terraform plan or apply.
- If there were no errors, Iâ€™d verify the outcome and move on.
- If there were errors, Iâ€™d copy the output into Claude and weâ€™d go back and forth trying to fix the problem.

This is where things often got tricky. Claude would sometimes suggest hardcoded values despite earlier instructions to avoid them, or propose overly complex fixes instead of the simple, obvious one. Even with clear guidance in the prompt, it was a constant effort to keep the AI focused and within scope.

The process wasnâ€™t just code generation â€” it was troubleshooting, adjusting, and rechecking until things finally worked as expected.

![Terraform schema correction](./Claude_Terraform_Chat_Error_Example.png)

The process revealed both the strengths and limitations of AI-assisted Infrastructure as Code development.

---

## ðŸ§  Part 3: Working with GenAI â€“ The Good, the Bad, and the Wandering

Building two versions of the same project entirely through AI conversations provided unique insights into the **practical realities of AI-assisted development**.

This wasnâ€™t the utopian "AI will do everything" fantasy â€” nor was it the cynical "AI canâ€™t do anything useful" view.  
It was somewhere in between: **messy, human, instructive**.

---

### âœ… The Good: Where AI Excelled

**âš¡ Rapid prototyping and iteration**  
Claude could produce working infrastructure code faster than I could even open the Azure documentation.  
Need a Container App with specific environment variables? âœ… Done.  
Modify the OpenAI integration logic? âœ… Updated in seconds.

**ðŸ§© Pattern recognition and consistency**  
Once Claude grasped the structure of the project, it stuck with it.  
Variable names, tagging conventions, module layout â€” it stayed consistent without me needing to babysit every decision.

**ðŸ› ï¸ Boilerplate generation**  
Claude churned out huge volumes of code across Terraform, PowerShell, React, and Python â€” all **syntactically correct and logically structured**, freeing me from repetitive coding.

---

### âŒ The Bad: Where AI Struggled

**ðŸ§  Context drift and prompt guardrails**  
Even with structured, detailed instructions, Claude would sometimes go rogue:  

- Proposing solutions for problems I hadnâ€™t asked about  
- Rewriting things that didnâ€™t need fixing  
- Suggesting complete redesigns for simple tweaks  

**ðŸŽ‰ Over-enthusiasm**  
Claude would often blurt out things like:  
> *â€œCONGRATULATIONS!! ðŸŽ‰ You now have a production-ready AI Monitoring platform!â€*  
To which Iâ€™d reply:  
> *â€œEr, no bro. We're nowhere near done here. Still Cuz.â€*

(Okay, I donâ€™t *really* talk to Claude like a GenZ wannabe Roadman â€” but you get the idea ðŸ˜‚)

**ðŸ› Runtime debugging limitations**  
Claude could *write* the code. But fixing things like:  

- Azure authentication issues  
- Misconfigured private endpoints  
- Resource naming collisions  
â€¦was always on me. These werenâ€™t things AI could reliably troubleshoot on its own.

**ðŸ” Project continuity fail**  
Thereâ€™s no persistent memory.  
Every new session meant reloading context from scratch â€” usually by copy-pasting yesterdayâ€™s chat into a new one.  
Tedious, error-prone, and inefficient.

---

### ðŸŒ€ The Wandering: Managing AI Attention

**âš ï¸ Fundamental challenge: No memory**  
Claude has no memory beyond the current chat. Even structured prompts didnâ€™t prevent â€œchat driftâ€ unless I constantly reinforced boundaries.
This is where ChatGPT has an edge in my opiion.
If I ask about previous chats, ChatGPT can give me examples and context about chats we had previously if prompted.

**ðŸŽ¯ The specificity requirement**  
Vague:  
> "Fix the container deployment"  
Resulted in:  
> "Letâ€™s rebuild the entire architecture from scratch" ðŸ˜¬  

Precise:  
> "Update the environment variable `REACT_APP_API_URL` in `container-apps` module"  
Got the job done.

**ðŸš« The hardcoded value trap**  
Claude loved quick fixes â€” often hardcoding values just to â€œmake it workâ€.  
I had to go back and **de-hardcode** everything to stay true to the DRY principles I set from day one.

**â³ Time impact for non-devs
Both stages of the project took longer than they probably should have â€” not because of any one flaw, but because of the nature of working with AI-generated infrastructure code.

A seasoned DevOps engineer might have moved faster by spotting bugs earlier and validating logic more confidently. But a pure developer? Probably not. Theyâ€™d likely struggle with the Azure-specific infrastructure decisions, access policies, and platform configuration that were second nature to me.

This kind of work sits in a grey area â€” it needs both engineering fluency and platform experience. The real takeaway? GenAI can bridge that gap in either direction, but whichever way youâ€™re coming from, thereâ€™s a learning curve.

The cost: higher validation effort.  
The reward: greater independence and accelerated learning.

---

## ðŸ—ï¸ Part 4: Building The Stack - What Got Built

The final Terraform solution creates a fully integrated AI monitoring ecosystem in Azure â€” one thatâ€™s modular, intelligent, and *almost* production-ready.  
Hereâ€™s what was actually built â€” and why.

---

### ðŸ”§ Core Architecture

**ðŸ§  Azure OpenAI Integration**  
At the heart of the system is GPT-4o-mini, providing infrastructure analysis and recommendations at a significantly lower cost than GPT-4 â€” without compromising on quality for this use case.

**ðŸ“¦ Container Apps Environment**  
Four lightweight, purpose-driven containers manage the monitoring workflow:

- âš™ï¸ **FastAPI backend** â€“ Data ingestion and processing
- ðŸ“Š **React dashboard** â€“ Front-end UI and live telemetry
- ðŸ”„ **Background processor** â€“ Continuously monitors resource health
- ðŸš€ **Load generator** â€“ Simulates traffic for stress testing and metrics

**âš¡ Azure Function Apps for AI Processing**  
Serverless compute bridges raw telemetry with OpenAI for analysis.  
Functions scale on demand, keeping costs low and architecture lean.

> âš ï¸ The only part of the project not handled in Terraform was the custom dashboard container build.
> That's by design â€” Terraform isnâ€™t meant for image building or pushing.
> Instead, I handled that manually (or via CI pipeline), which aligns with Hashicorps ![best practices](https://developer.hashicorp.com/terraform/docs/language/resources/externally-managed).

---

### ðŸ§° Supporting Infrastructure

- **Application Insights** â€“ Real-time telemetry for diagnostics
- **Log Analytics** â€“ Centralised logging and query aggregation
- **Azure Container Registry (ACR)** â€“ Stores and serves custom container images
- **Key Vault** â€“ Secrets management for safe credential handling

---

### ðŸ¤” Key Technical Decisions

**ðŸ†š Why Container Apps instead of AKS?**  
Honestly? Claude made the call.  
When I described what I needed (multi-container orchestration without complex ops), Claude recommended **Container Apps over AKS**, citing:  

- Lower cost  
- Simpler deployment  
- Sufficient capability for this workload  

Andâ€¦ Claude was right. AKS would have been overkill.

**ðŸ’¸ Why GPT-4o-mini over GPT-4?**  
This was a no-brainer. GPT-4o-mini gave near-identical results for our monitoring analysis â€” at a **fraction of the cost**.  
Perfect balance of performance and budget.

**ðŸ“¦ Why modular Terraform over monolithic deployment?**  
Because chaos is not a deployment strategy.  
Modular code = clean boundaries, reusable components, and simple environment customization.  
Itâ€™s easier to debug, update, and scale.

---

### ðŸ§® Visual Reference

Below are visuals captured during project development and testing:

**ðŸ”¹ VS Code project structure**  
![VS Code project structure](./VSCode_Terraform_Project_Structure.png)

**ðŸ”¹ Claude Projects interface**  
![Claude Projects interface](./Claude_Project_Screenshot.png)

---

### ðŸ“Š What the Dashboard Shows

The final React-based dashboard delivers:

- âœ… **Real-time API health checks**
- ðŸ§  **AI-generated infrastructure insights**
- ðŸ“ˆ **Performance metrics + trend analysis**
- ðŸ’¬ **Interactive chat with OpenAI**
- ðŸ“¤ **Exportable chats for analysis**

**ðŸ”¹ Dashboard â€“ Full view**  
![Dashboard Full View](./AIMonitoringDashboard_Final.png)

**ðŸ”¹ AI analysis in progress**  
![Dashboard AI analysis 2](./AIMonitoringDashboard_2.png)

**ðŸ”¹ OpenAI response card**  
![OpenAI response](./AIMonitoringDashboard.png)

---

## ðŸ§¾ Part 5: The Result - A Portable, Reusable AI Monitoring Stack

The final Terraform deployment delivers a **complete, modular, and production-friendly AI monitoring solution** â€” fully reproducible across environments. More importantly, it **demonstrates that AI-assisted infrastructure creation is not just viable, but effective** when paired with good practices and human oversight.

---

### ðŸš€ Deployment Experience

**From zero to running dashboard:**  
~ **15 minutes (give or take 30-40 hours ðŸ˜‚)**

```bash
terraform init
terraform plan
terraform apply
```

Minimal configuration required:

- âœ… Azure subscription credentials
- ðŸ“„ Terraform variables (project name, region, container image names, etc.)
- ðŸ³ Container image references (can use defaults or custom builds)

---

### ðŸ—ºï¸ Infrastructure Overview

The final deployment provisions a complete, AI-driven monitoring stack â€” built entirely with Infrastructure as Code and connected through modular Terraform components.

**ðŸ”¹ Azure Resource Visualizer**  
![Azure Resource Visualizer](./Azure_Resources.png)

---

### ðŸ’° Cost Optimization

This solution costs **~Â£15 per month** for a dev/test deployment (even cheaper if you remember to turn the container apps off!ðŸ˜²) â€” vastly cheaper than typical enterprise-grade monitoring tools (which can range Â£50â€“Â£200+ per month).

**Key savings come from:**

- âš¡ Serverless Functions instead of always-on compute
- ðŸ“¦ Container Apps that scale to zero during idle time
- ðŸ¤– GPT-4o-mini instead of GPT-4 (with negligible accuracy trade-off)

---

### ðŸ” Portability Validation

The real benefit of this solution is in its **repeatability**:

âœ… **Dev environment**  
UK South, full-feature stack

âœ… **Test deployment**  
New resource group, same subscription â€” **identical results**

âœ… **Clean subscription test**  
Fresh environment, zero config drift

**Conclusion:**  
No matter where or how it's deployed, the **stack just works**.

---

## ðŸ§  Part 6: Reflections and Lessons Learned

Building the same solution twice â€” once manually, once using Infrastructure as Code â€” offered a unique lens through which to view both **AI-assisted development** and **modern infrastructure practices**.

---

### ðŸ¤– On AI-Assisted Development

**ðŸ”Ž The reality check**  
AI-assisted development is **powerful but not autonomous**. It still relies on:

- Human oversight
- Strategic decisions
- Recognizing when the AI is confidently wrong

**âš¡ Speed vs. Quality**  
AI can produce working code fast â€” sometimes scarily fast â€” but:

- The **validation/debugging** can take *longer* than traditional coding
- The real power lies in **architectural iteration**, not production-readiness

**ðŸ“š The learning curve**  
Truthfully, both v1 and v2 took **much longer than they should have**.  
A seasoned developer with better validation skills could likely complete either project in **half the time** â€” by catching subtle issues earlier.

---

### ðŸ› ï¸ On Infrastructure as Code

**ðŸ“ The transformation**  
Switching to Terraform wasnâ€™t just about reusability:

- It encouraged **cleaner design**, **logical resource grouping**, and **explicit dependencies**
- It *forced* better decisions

**ðŸ§© The hidden complexity**  
What looked simple in Terraform:

- Revealed just how **messy** the manual deployment had been
- Every implicit assumption, naming decision, and â€œjust click hereâ€ moment had to become **codified and reproducible**

---

### ðŸŽ­ On Vibe Coding as a Methodology

**âœ… What worked:**

- Rapid architectural exploration
- Solving problems in plain English
- Iterative builds based on feedback
- AI-assisted speed gains (things built in hours, not days)

**âŒ What didnâ€™t:**

- Continuity across chat sessions
- Preserving project context
- Runtime debugging in Azure
- Keeping the agent focused on scoped tasks

---

### ðŸ” Things Iâ€™d Do Differently

**ðŸ§¾ Better structured prompting from the outset**  
While I used a defined structure for the AI prompt, I learned:

- Even good prompts **require ongoing reinforcement**
- Claude needed regular reminders to stay on track during long sessions

**âœ… Regular resource validation**  
A recurring challenge:

- Claude often **over-provisioned** services
- Periodic reviews of what we were building helped cut waste and simplify architecture

**ðŸ§  The reality of AI memory limitations**  
No, the AI does not â€œrememberâ€ anything meaningful between sessions:

- Every day required **rebuilding the conversation context**
- Guardrails had to be **restated** often

**ðŸŽ¯ The extreme specificity requirement**  
Vague asks = vague solutions  
But:

- **Precise requests** like â€œupdate `REACT_APP_API_URL` in `container-apps` moduleâ€ yielded laser-targeted results

---

## âœ… Conclusion

This project started as a career thought experiment â€” *â€œWhat if there was a role focused on AI-integrated infrastructure?â€* â€” and ended with a fully functional AI monitoring solution deployed in Azure.

What began as a prototype on a local Pi5 evolved into a robust, modular Terraform deployment. Over 4â€“5 weeks, it generated **thousands of lines of infrastructure code**, countless iterations, and a treasure trove of insights into AI-assisted development.

---

### ðŸš€ The Technical Outcome

The result is a **portable, cost-effective, AI-powered monitoring system** that doesnâ€™t just work â€” it *proves a point*. It's not quite enterprise-ready, but itâ€™s a solid proof-of-concept and a foundation for learning, experimentation, and future iteration.

---

### ðŸ§  Key Takeaways

1. **AI-assisted development is powerful â€” but not autonomous.**  
   It requires constant direction, critical oversight, and the ability to spot when the AI is confidently wrong.

2. **Infrastructure as Code changes how you architect.**  
   Writing Terraform forces discipline: clean structure, explicit dependencies, and reproducible builds.

3. **Vibe coding has a learning curve.**  
   Both versions took longer than expected. A seasoned developer could likely move faster â€” but for infra pros, this is how we learn.

4. **Context management is still a major limitation.**  
   The inability to persist AI session memory made long-term projects harder than they should have been.

5. **The role of â€œInfrastructure AI Integration Engineerâ€ is real â€” and emerging.**  
   This project sketches out what that future job might involve: blending IaC, AI, automation, and architecture.

---

### ðŸ§­ Whatâ€™s Next?

Version 3 is already brewing â˜• â€” ideas include:

- Monitoring more Azure services
- Improving the dashboardâ€™s AI output formatting
- Experimenting with newer tools like **Claude Code** and **ChatGPT Codex**
- Trying AI-native IDEs and inline assistants to streamline the workflow

And letâ€™s not forget the rise of **â€œSlop-Opsâ€** â€” that beautiful mess where AI, infrastructure, and vibe-based engineering collide ðŸ˜Ž

---

### ðŸ’¡ Final Thoughts

If you're an infrastructure engineer looking to explore AI integration, hereâ€™s the reality:

- The tools are ready.
- The method works.
- But itâ€™s not magic â€” **it takes effort, patience, and curiosity**.

The future of infrastructure might be conversational â€” but itâ€™s not (yet) automatic.

---

> If youâ€™ve read this far â€” thanks. ðŸ™
> Iâ€™d love feedback from anyone experimenting with AI-assisted IaC or Terraform refactors.
> Find me on [LinkedIn] or leave a comment.
